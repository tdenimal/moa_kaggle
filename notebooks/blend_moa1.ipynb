{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1606040782192",
   "display_name": "Python 3.7.9 64-bit ('moa_kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "#from pytorch_tabnet import tab_network\n",
    "from pytorch_tabnet.multiclass_utils import unique_labels\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from pytorch_tabnet.utils import (PredictDataset,\n",
    "                                  create_explain_matrix)\n",
    "from sklearn.base import BaseEstimator\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import io\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['train_targets_scored.csv',\n 'sample_submission.csv',\n '.gitkeep',\n 'train_drug.csv',\n 'train_features.csv',\n 'test_features.csv',\n 'train_targets_nonscored.csv']"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data_dir = '../data/01_raw'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "no_ctl = True\n",
    "scale = \"rankgauss\"\n",
    "decompo = \"PCA\"\n",
    "#ncompo_genes = 80\n",
    "#ncompo_cells = 10\n",
    "ncompo_genes = 300\n",
    "ncompo_cells = 25\n",
    "encoding = \"dummy\"\n",
    "variance_threshold = .7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(data_dir+'/train_features.csv')\n",
    "train_targets_scored = pd.read_csv(data_dir+'/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(data_dir+'/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv(data_dir+'/test_features.csv')\n",
    "sample_submission = pd.read_csv(data_dir+'/sample_submission.csv')\n",
    "drug = pd.read_csv(data_dir+'/train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_scored = train_targets_scored.columns[1:]\n",
    "targets_nscored = train_targets_nonscored.columns[1:]\n",
    "scored = train_targets_scored.merge(drug, on='sig_id', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "not_ctl\n"
    }
   ],
   "source": [
    "if no_ctl:\n",
    "    # cp_type == ctl_vehicle\n",
    "    print(\"not_ctl\")\n",
    "    train_features = train_features[train_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "    test_features = test_features[test_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "    train_targets_scored = train_targets_scored.iloc[train_features.index]\n",
    "    train_targets_nonscored = train_targets_nonscored.iloc[train_features.index]\n",
    "    train_features.reset_index(drop = True, inplace = True)\n",
    "    test_features.reset_index(drop = True, inplace = True)\n",
    "    train_targets_scored.reset_index(drop = True, inplace = True)\n",
    "    train_targets_nonscored.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([train_features, test_features], ignore_index = True)\n",
    "\n",
    "cols_numeric = [feat for feat in list(data_all.columns) if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"] +\\\n",
    "                                                                       list(train_targets_nonscored.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True gauss rank\n",
    "import cupy as cp\n",
    "from cupyx.scipy.special import erfinv\n",
    "epsilon = 1e-6\n",
    "\n",
    "for k in (cols_numeric):\n",
    "    r_gpu = cp.array(data_all.loc[:,k])\n",
    "    r_gpu = r_gpu.argsort().argsort()\n",
    "    r_gpu = (r_gpu/r_gpu.max()-0.5)*2 \n",
    "    r_gpu = cp.clip(r_gpu,-1+epsilon,1-epsilon)\n",
    "    r_gpu = erfinv(r_gpu) \n",
    "    data_all.loc[:,k] = cp.asnumpy( r_gpu * np.sqrt(2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Quantiletransformer\n",
    "# for col in (cols_numeric):\n",
    "\n",
    "#     transformer = QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n",
    "#     vec_len = len(data_all[col].values)\n",
    "#     raw_vec = data_all[col].values.reshape(vec_len, 1)\n",
    "#     transformer.fit(raw_vec)\n",
    "\n",
    "#     data_all[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PCA\n"
    }
   ],
   "source": [
    "#PCA\n",
    "if decompo == \"PCA\":\n",
    "    print(\"PCA\")\n",
    "    GENES = [col for col in data_all.columns if col.startswith(\"g-\")]\n",
    "    CELLS = [col for col in data_all.columns if col.startswith(\"c-\")]\n",
    "    \n",
    "    pca_genes = PCA(n_components = ncompo_genes,\n",
    "                    random_state = seed).fit_transform(data_all[GENES])\n",
    "    pca_cells = PCA(n_components = ncompo_cells,\n",
    "                    random_state = seed).fit_transform(data_all[CELLS])\n",
    "    \n",
    "    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n",
    "    pca_cells = pd.DataFrame(pca_cells, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n",
    "    data_all = pd.concat([data_all, pca_genes, pca_cells], axis = 1)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "One-Hot\n"
    }
   ],
   "source": [
    "# Encoding\n",
    "if encoding == \"lb\":\n",
    "    print(\"Label Encoding\")\n",
    "    for feat in [\"cp_time\", \"cp_dose\"]:\n",
    "        data_all[feat] = LabelEncoder().fit_transform(data_all[feat])\n",
    "elif encoding == \"dummy\":\n",
    "    print(\"One-Hot\")\n",
    "    data_all = pd.get_dummies(data_all, columns = [\"cp_time\", \"cp_dose\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in data_all.columns if col.startswith(\"g-\")]\n",
    "CELLS = [col for col in data_all.columns if col.startswith(\"c-\")]\n",
    "\n",
    "for stats in [\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]:\n",
    "#for stats in [\"sum\",  \"std\"]:\n",
    "    data_all[\"g_\" + stats] = getattr(data_all[GENES], stats)(axis = 1)\n",
    "    data_all[\"c_\" + stats] = getattr(data_all[CELLS], stats)(axis = 1)\n",
    "\n",
    "for stats in [\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]:\n",
    "#for stats in [\"sum\"]:\n",
    "    data_all[\"gc_\" + stats] = getattr(data_all[GENES + CELLS], stats)(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "feat_cols = [c for c in data_all.columns if c not in [\"sig_id\", \"cp_type\"]]\n",
    "var_thresh = VarianceThreshold(variance_threshold)  #<-- Update\n",
    "data_feats = pd.DataFrame(var_thresh.fit_transform(data_all[feat_cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all = pd.concat([data_all[\"sig_id\"],data_all[targets_nscored], data_feats],axis=1)\n",
    "data_all = pd.concat([data_all[\"sig_id\"], data_feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(25572, 1079)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df and test_df\n",
    "# try:\n",
    "#     train_targets_scored.drop(\"sig_id\", axis = 1, inplace = True)\n",
    "#     train_targets_nonscored.drop(\"sig_id\", axis = 1, inplace = True)\n",
    "# except:\n",
    "#     pass\n",
    "train_df = data_all[: train_features.shape[0]]\n",
    "train_df.reset_index(drop = True, inplace = True)\n",
    "# The following line it's a bad practice in my opinion, targets on train set\n",
    "#train_df = pd.concat([train_df, targets], axis = 1)\n",
    "test_df = data_all[train_df.shape[0]: ]\n",
    "test_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train_df.shape: (21948, 1079)\ntest_df.shape: (3624, 1079)\n"
    }
   ],
   "source": [
    "print(f\"train_df.shape: {train_df.shape}\")\n",
    "print(f\"test_df.shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "NFOLDS = 5\n",
    "\n",
    "\n",
    "# LOCATE DRUGS\n",
    "vc = scored.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc<=18].index.sort_values()\n",
    "vc2 = vc.loc[vc>18].index.sort_values()\n",
    "\n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}; dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = scored.groupby('drug_id')[targets_scored].mean().loc[vc1]\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_scored])):\n",
    "    dd = {k:fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "\n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_scored])):\n",
    "    dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "# ASSIGN FOLDS\n",
    "train_df = train_df.merge(drug,on=\"sig_id\")\n",
    "train_df['fold'] = train_df.drug_id.map(dct1)\n",
    "train_df.loc[train_df.fold.isna(),'fold'] =\\\n",
    "    train_df.loc[train_df.fold.isna(),'sig_id'].map(dct2)\n",
    "train_df.fold = train_df.fold.astype('int8')\n",
    "\n",
    "\n",
    "feat_cols = [c for c in train_df.columns if c not in [\"sig_id\",\"cp_type\",\"drug_id\",\"fold\"]]\n",
    "num_features=len(feat_cols)\n",
    "num_targets_scored=len(targets_scored)\n",
    "num_targets_nscored=len(targets_nscored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "def log_loss_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n",
    "    return results.mean()\n",
    "\n",
    "def log_loss_score(actual, predicted,  eps=1e-15):\n",
    "\n",
    "        \"\"\"\n",
    "        :param predicted:   The predicted probabilities as floats between 0-1\n",
    "        :param actual:      The binary labels. Either 0 or 1.\n",
    "        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n",
    "        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        p1 = actual * np.log(predicted+eps)\n",
    "        p0 = (1-actual) * np.log(1-predicted+eps)\n",
    "        loss = p0 + p1\n",
    "\n",
    "        return -loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Format for numpy array\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        The input matrix\n",
    "    y : 2D array\n",
    "        The one-hot encoded target\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.x[index], self.y[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    X_train, y_scored_train, X_valid,y_valid, weights, batch_size, num_workers, drop_last, pin_memory=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Create dataloaders with or wihtout subsampling depending on weights and balanced.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.ndarray\n",
    "        Training data\n",
    "    y_train : np.array\n",
    "        Mapped Training targets\n",
    "    eval_set : list of tuple\n",
    "        List of eval tuple set (X, y)\n",
    "    weights : either 0, 1, dict or iterable\n",
    "        if 0 (default) : no weights will be applied\n",
    "        if 1 : classification only, will balanced class with inverse frequency\n",
    "        if dict : keys are corresponding class values are sample weights\n",
    "        if iterable : list or np array must be of length equal to nb elements\n",
    "                      in the training set\n",
    "    batch_size : int\n",
    "        how many samples per batch to load\n",
    "    num_workers : int\n",
    "        how many subprocesses to use for data loading. 0 means that the data\n",
    "        will be loaded in the main process\n",
    "    drop_last : bool\n",
    "        set to True to drop the last incomplete batch, if the dataset size is not\n",
    "        divisible by the batch size. If False and the size of dataset is not\n",
    "        divisible by the batch size, then the last batch will be smaller\n",
    "    pin_memory : bool\n",
    "        Whether to pin GPU memory during training\n",
    "    Returns\n",
    "    -------\n",
    "    train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "        Training and validation dataloaders\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(weights, int):\n",
    "        if weights == 0:\n",
    "            need_shuffle = True\n",
    "            sampler = None\n",
    "        elif weights == 1:\n",
    "            need_shuffle = False\n",
    "            class_sample_count = np.array(\n",
    "                [len(np.where(y_train == t)[0]) for t in np.unique(y_scored_train)]\n",
    "            )\n",
    "\n",
    "            weights = 1.0 / class_sample_count\n",
    "\n",
    "            samples_weight = np.array([weights[t] for t in y_scored_train])\n",
    "\n",
    "            samples_weight = torch.from_numpy(samples_weight)\n",
    "            samples_weight = samples_weight.double()\n",
    "            sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        else:\n",
    "            raise ValueError(\"Weights should be either 0, 1, dictionnary or list.\")\n",
    "    elif isinstance(weights, dict):\n",
    "        # custom weights per class\n",
    "        need_shuffle = False\n",
    "        samples_weight = np.array([weights[t] for t in y_scored_train])\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    else:\n",
    "        # custom weights\n",
    "        if len(weights) != len(y_scored_train):\n",
    "            raise ValueError(\"Custom weights should match number of train samples.\")\n",
    "        need_shuffle = False\n",
    "        samples_weight = np.array(weights)\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        MoADataset(X_train.astype(np.float32), y_scored_train),\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        shuffle=need_shuffle,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=drop_last,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    # valid_dataloaders = []\n",
    "    # for X, y in [(X_valid,y_valid)]:\n",
    "    #     valid_dataloaders.append(\n",
    "    #         DataLoader(\n",
    "    #             ValidDataset(X.astype(np.float32), y),\n",
    "    #             batch_size=batch_size,\n",
    "    #             shuffle=False,\n",
    "    #             num_workers=num_workers,\n",
    "    #             pin_memory=pin_memory\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    valid_dataloaders = DataLoader(\n",
    "        MoADataset(X_valid.astype(np.float32), y_valid),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "\n",
    "    return train_dataloader, valid_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU, PReLU,LeakyReLU\n",
    "import numpy as np\n",
    "from pytorch_tabnet import sparsemax\n",
    "\n",
    "\n",
    "def initialize_non_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(4*input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "def initialize_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "class GBN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Ghost Batch Normalization\n",
    "        https://arxiv.org/abs/1705.08741\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, virtual_batch_size=128, momentum=0.01):\n",
    "        super(GBN, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.bn = BatchNorm1d(self.input_dim, momentum=momentum)\n",
    "        # self.dropout = nn.Dropout(.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
    "        res = [self.bn(x_) for x_ in chunks]\n",
    "\n",
    "        #return self.dropout(torch.cat(res, dim=0))\n",
    "        return torch.cat(res, dim=0)\n",
    "\n",
    "\n",
    "class TabNetNoEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 n_d=8, n_a=8,\n",
    "                 n_steps=3, gamma=1.3,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,\n",
    "                 virtual_batch_size=128, momentum=0.02,\n",
    "                 mask_type=\"sparsemax\"):\n",
    "        \"\"\"\n",
    "        Defines main part of the TabNet network without the embedding layers.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Number of features\n",
    "        output_dim : int or list of int for multi task classification\n",
    "            Dimension of network output\n",
    "            examples : one for regression, 2 for binary classification etc...\n",
    "        n_d : int\n",
    "            Dimension of the prediction  layer (usually between 4 and 64)\n",
    "        n_a : int\n",
    "            Dimension of the attention  layer (usually between 4 and 64)\n",
    "        n_steps : int\n",
    "            Number of sucessive steps in the newtork (usually betwenn 3 and 10)\n",
    "        gamma : float\n",
    "            Float above 1, scaling factor for attention updates (usually betwenn 1.0 to 2.0)\n",
    "        n_independent : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        n_shared : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        epsilon : float\n",
    "            Avoid log(0), this should be kept very low\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in all batch norm\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(TabNetNoEmbeddings, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.is_multi_task = isinstance(output_dim, list)\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.mask_type = mask_type\n",
    "        self.initial_bn = BatchNorm1d(self.input_dim, momentum=0.01)\n",
    "\n",
    "        if self.n_shared > 0:\n",
    "            shared_feat_transform = torch.nn.ModuleList()\n",
    "\n",
    "            for i in range(self.n_shared):\n",
    "                if i == 0:\n",
    "                    shared_feat_transform.append(Linear(self.input_dim,\n",
    "                                                        2*(n_d + n_a),\n",
    "                                                        bias=False))\n",
    "                else:\n",
    "                    shared_feat_transform.append(Linear(n_d + n_a, 2*(n_d + n_a), bias=False))\n",
    "\n",
    "        else:\n",
    "            shared_feat_transform = None\n",
    "\n",
    "        self.initial_splitter = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
    "                                                n_glu_independent=self.n_independent,\n",
    "                                                virtual_batch_size=self.virtual_batch_size,\n",
    "                                                momentum=momentum)\n",
    "\n",
    "        self.feat_transformers = torch.nn.ModuleList()\n",
    "        self.att_transformers = torch.nn.ModuleList()\n",
    "\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            transformer = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
    "                                          n_glu_independent=self.n_independent,\n",
    "                                          virtual_batch_size=self.virtual_batch_size,\n",
    "                                          momentum=momentum)\n",
    "\n",
    "            attention = AttentiveTransformer(n_a, self.input_dim,\n",
    "                                             virtual_batch_size=self.virtual_batch_size,\n",
    "                                             momentum=momentum,\n",
    "                                             mask_type=self.mask_type)\n",
    "\n",
    "            self.feat_transformers.append(transformer)\n",
    "            self.att_transformers.append(attention)\n",
    "\n",
    "        if self.is_multi_task:\n",
    "            self.multi_task_mappings = torch.nn.ModuleList()\n",
    "            for task_dim in output_dim:\n",
    "                task_mapping = Linear(n_d, task_dim, bias=False)\n",
    "                initialize_non_glu(task_mapping, n_d, task_dim)\n",
    "                self.multi_task_mappings.append(task_mapping)\n",
    "\n",
    "            # self.multi_task_mappings2 = torch.nn.ModuleList()\n",
    "            # for task_dim in output_dim2:\n",
    "            #     task_mapping = Linear(n_d2, task_dim, bias=False)\n",
    "            #     initialize_non_glu(task_mapping, n_d2, task_dim)\n",
    "            #     self.multi_task_mappings2.append(task_mapping)\n",
    "        else:\n",
    "            self.final_mapping = Linear(n_d, output_dim, bias=False)\n",
    "            initialize_non_glu(self.final_mapping, n_d, output_dim)\n",
    "            # self.final_mapping2 = Linear(n_d2, output_dim2, bias=False)\n",
    "            # initialize_non_glu(self.final_mapping2, n_d2, output_dim2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = 0\n",
    "        x = self.initial_bn(x)\n",
    "\n",
    "        prior = torch.ones(x.shape).to(x.device)\n",
    "\n",
    "        M_loss = 0\n",
    "        att = self.initial_splitter(x)[:, self.n_d:]\n",
    "        \n",
    "        for step in range(self.n_steps):\n",
    "            M = self.att_transformers[step](prior, att)\n",
    "\n",
    "            M_loss += torch.mean(torch.sum(torch.mul(M, torch.log(M+self.epsilon)),\n",
    "                                           dim=1))\n",
    "\n",
    "            # update prior\n",
    "            prior = torch.mul(self.gamma - M, prior)\n",
    "            # output\n",
    "            masked_x = torch.mul(M, x)\n",
    "            out = self.feat_transformers[step](masked_x)\n",
    "            # d = ReLU()(out[:, :self.n_d])\n",
    "            d = PReLU().to(x.device)(out[:, :self.n_d])\n",
    "            res = torch.add(res, d)\n",
    "            # update attention\n",
    "            att = out[:, self.n_d:]\n",
    "\n",
    "        M_loss /= self.n_steps\n",
    "\n",
    "        if self.is_multi_task:\n",
    "            # Result will be in list format\n",
    "            out = []\n",
    "            for task_mapping in self.multi_task_mappings:\n",
    "                out.append(task_mapping(res))\n",
    "\n",
    "        else:\n",
    "            out = self.final_mapping(res)\n",
    "        return out, M_loss\n",
    "        # return out1, M_loss\n",
    "\n",
    "    def forward_masks(self, x):\n",
    "        x = self.initial_bn(x)\n",
    "\n",
    "        prior = torch.ones(x.shape).to(x.device)\n",
    "        M_explain = torch.zeros(x.shape).to(x.device)\n",
    "        att = self.initial_splitter(x)[:, self.n_d:]\n",
    "        masks = {}\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            M = self.att_transformers[step](prior, att)\n",
    "            masks[step] = M\n",
    "            # update prior\n",
    "            prior = torch.mul(self.gamma - M, prior)\n",
    "            # output\n",
    "            masked_x = torch.mul(M, x)\n",
    "            out = self.feat_transformers[step](masked_x)\n",
    "            # d = ReLU()(out[:, :self.n_d])\n",
    "            d = PReLU().to(x.device)(out[:, :self.n_d])\n",
    "            # explain\n",
    "            step_importance = torch.sum(d, dim=1)\n",
    "            M_explain += torch.mul(M, step_importance.unsqueeze(dim=1))\n",
    "            # update attention\n",
    "            att = out[:, self.n_d:]\n",
    "\n",
    "        return M_explain, masks\n",
    "\n",
    "\n",
    "class TabNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, n_d=8, n_a=8,\n",
    "                 n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,\n",
    "                 virtual_batch_size=128, momentum=0.02, device_name='auto',\n",
    "                 mask_type=\"sparsemax\"):\n",
    "        \"\"\"\n",
    "        Defines TabNet network\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Initial number of features\n",
    "        output_dim : int\n",
    "            Dimension of network output\n",
    "            examples : one for regression, 2 for binary classification etc...\n",
    "        n_d : int\n",
    "            Dimension of the prediction  layer (usually between 4 and 64)\n",
    "        n_a : int\n",
    "            Dimension of the attention  layer (usually between 4 and 64)\n",
    "        n_steps : int\n",
    "            Number of sucessive steps in the newtork (usually betwenn 3 and 10)\n",
    "        gamma : float\n",
    "            Float above 1, scaling factor for attention updates (usually betwenn 1.0 to 2.0)\n",
    "        cat_idxs : list of int\n",
    "            Index of each categorical column in the dataset\n",
    "        cat_dims : list of int\n",
    "            Number of categories in each categorical column\n",
    "        cat_emb_dim : int or list of int\n",
    "            Size of the embedding of categorical features\n",
    "            if int, all categorical features will have same embedding size\n",
    "            if list of int, every corresponding feature will have specific size\n",
    "        n_independent : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        n_shared : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        epsilon : float\n",
    "            Avoid log(0), this should be kept very low\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in all batch norm\n",
    "        device_name : {'auto', 'cuda', 'cpu'}\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(TabNet, self).__init__()\n",
    "        self.cat_idxs = cat_idxs or []\n",
    "        self.cat_dims = cat_dims or []\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.mask_type = mask_type\n",
    "\n",
    "        if self.n_steps <= 0:\n",
    "            raise ValueError(\"n_steps should be a positive integer.\")\n",
    "        if self.n_independent == 0 and self.n_shared == 0:\n",
    "            raise ValueError(\"n_shared and n_independant can't be both zero.\")\n",
    "\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.embedder = EmbeddingGenerator(input_dim, cat_dims, cat_idxs, cat_emb_dim)\n",
    "        self.post_embed_dim = self.embedder.post_embed_dim\n",
    "        self.tabnet = TabNetNoEmbeddings(self.post_embed_dim, output_dim,n_d, n_a, n_steps,\n",
    "                                         gamma, n_independent, n_shared, epsilon,\n",
    "                                         virtual_batch_size, momentum, mask_type)\n",
    "\n",
    "        # Defining device\n",
    "        if device_name == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device_name = 'cuda'\n",
    "            else:\n",
    "                device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedder(x)\n",
    "        return self.tabnet(x)\n",
    "\n",
    "    def forward_masks(self, x):\n",
    "        x = self.embedder(x)\n",
    "        return self.tabnet.forward_masks(x)\n",
    "\n",
    "\n",
    "class AttentiveTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 virtual_batch_size=128,\n",
    "                 momentum=0.02,\n",
    "                 mask_type=\"sparsemax\"):\n",
    "        \"\"\"\n",
    "        Initialize an attention transformer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Input size\n",
    "        output_dim : int\n",
    "            Outpu_size\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.fc = Linear(input_dim, output_dim, bias=False)\n",
    "        initialize_non_glu(self.fc, input_dim, output_dim)\n",
    "        self.bn = GBN(output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "        if mask_type == \"sparsemax\":\n",
    "            # Sparsemax\n",
    "            self.selector = sparsemax.Sparsemax(dim=-1)\n",
    "        elif mask_type == \"entmax\":\n",
    "            # Entmax\n",
    "            self.selector = sparsemax.Entmax15(dim=-1)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Please choose either sparsemax\" +\n",
    "                                      \"or entmax as masktype\")\n",
    "\n",
    "    def forward(self, priors, processed_feat):\n",
    "        x = self.fc(processed_feat)\n",
    "        x = self.bn(x)\n",
    "        x = torch.mul(x, priors)\n",
    "        x = self.selector(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, shared_layers, n_glu_independent,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(FeatTransformer, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize a feature transformer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Input size\n",
    "        output_dim : int\n",
    "            Outpu_size\n",
    "        shared_layers : torch.nn.ModuleList\n",
    "            The shared block that should be common to every step\n",
    "        n_glu_independant : int\n",
    "            Number of independent GLU layers\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization within GLU block(s)\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            'n_glu': n_glu_independent,\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        if shared_layers is None:\n",
    "            # no shared layers\n",
    "            self.shared = torch.nn.Identity()\n",
    "            is_first = True\n",
    "        else:\n",
    "            self.shared = GLU_Block(input_dim, output_dim,\n",
    "                                    first=True,\n",
    "                                    shared_layers=shared_layers,\n",
    "                                    n_glu=len(shared_layers),\n",
    "                                    virtual_batch_size=virtual_batch_size,\n",
    "                                    momentum=momentum)\n",
    "            is_first = False\n",
    "\n",
    "        if n_glu_independent == 0:\n",
    "            # no independent layers\n",
    "            self.specifics = torch.nn.Identity()\n",
    "        else:\n",
    "            spec_input_dim = input_dim if is_first else output_dim\n",
    "            self.specifics = GLU_Block(spec_input_dim, output_dim,\n",
    "                                       first=is_first,\n",
    "                                       **params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        x = self.specifics(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Block(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Independant GLU block, specific to each step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, n_glu=2, first=False, shared_layers=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Block, self).__init__()\n",
    "        self.first = first\n",
    "        self.shared_layers = shared_layers\n",
    "        self.n_glu = n_glu\n",
    "        self.glu_layers = torch.nn.ModuleList()\n",
    "\n",
    "        params = {\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        fc = shared_layers[0] if shared_layers else None\n",
    "        self.glu_layers.append(GLU_Layer(input_dim, output_dim,\n",
    "                                         fc=fc,\n",
    "                                         **params))\n",
    "        for glu_id in range(1, self.n_glu):\n",
    "            fc = shared_layers[glu_id] if shared_layers else None\n",
    "            self.glu_layers.append(GLU_Layer(output_dim, output_dim,\n",
    "                                             fc=fc,\n",
    "                                             **params))\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = torch.sqrt(torch.FloatTensor([0.5]).to(x.device))\n",
    "        if self.first:  # the first layer of the block has no scale multiplication\n",
    "            x = self.glu_layers[0](x)\n",
    "            layers_left = range(1, self.n_glu)\n",
    "        else:\n",
    "            layers_left = range(self.n_glu)\n",
    "\n",
    "        for glu_id in layers_left:\n",
    "            x = torch.add(x, self.glu_layers[glu_id](x))\n",
    "            x = x*scale\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, fc=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Layer, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        if fc:\n",
    "            self.fc = fc\n",
    "        else:\n",
    "            self.fc = Linear(input_dim, 2*output_dim, bias=False)\n",
    "        initialize_glu(self.fc, input_dim, 2*output_dim)\n",
    "\n",
    "        self.bn = GBN(2*output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "\n",
    "        #self.dropout = nn.Dropout(.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        #x = self.dropout(x)\n",
    "        out = torch.mul(x[:, :self.output_dim], torch.sigmoid(x[:, self.output_dim:]))\n",
    "        return out\n",
    "\n",
    "\n",
    "class EmbeddingGenerator(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Classical embeddings generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, cat_dims, cat_idxs, cat_emb_dim):\n",
    "        \"\"\" This is an embedding module for an entier set of features\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Number of features coming as input (number of columns)\n",
    "        cat_dims : list of int\n",
    "            Number of modalities for each categorial features\n",
    "            If the list is empty, no embeddings will be done\n",
    "        cat_idxs : list of int\n",
    "            Positional index for each categorical features in inputs\n",
    "        cat_emb_dim : int or list of int\n",
    "            Embedding dimension for each categorical features\n",
    "            If int, the same embdeding dimension will be used for all categorical features\n",
    "        \"\"\"\n",
    "        super(EmbeddingGenerator, self).__init__()\n",
    "        if cat_dims == [] or cat_idxs == []:\n",
    "            self.skip_embedding = True\n",
    "            self.post_embed_dim = input_dim\n",
    "            return\n",
    "\n",
    "        self.skip_embedding = False\n",
    "        if isinstance(cat_emb_dim, int):\n",
    "            self.cat_emb_dims = [cat_emb_dim]*len(cat_idxs)\n",
    "        else:\n",
    "            self.cat_emb_dims = cat_emb_dim\n",
    "\n",
    "        # check that all embeddings are provided\n",
    "        if len(self.cat_emb_dims) != len(cat_dims):\n",
    "            msg = \"\"\" cat_emb_dim and cat_dims must be lists of same length, got {len(self.cat_emb_dims)}\n",
    "                      and {len(cat_dims)}\"\"\"\n",
    "            raise ValueError(msg)\n",
    "        self.post_embed_dim = int(input_dim + np.sum(self.cat_emb_dims) - len(self.cat_emb_dims))\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleList()\n",
    "\n",
    "        # Sort dims by cat_idx\n",
    "        sorted_idxs = np.argsort(cat_idxs)\n",
    "        cat_dims = [cat_dims[i] for i in sorted_idxs]\n",
    "        self.cat_emb_dims = [self.cat_emb_dims[i] for i in sorted_idxs]\n",
    "\n",
    "        for cat_dim, emb_dim in zip(cat_dims, self.cat_emb_dims):\n",
    "            self.embeddings.append(torch.nn.Embedding(cat_dim, emb_dim))\n",
    "\n",
    "        # record continuous indices\n",
    "        self.continuous_idx = torch.ones(input_dim, dtype=torch.bool)\n",
    "        self.continuous_idx[cat_idxs] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply embdeddings to inputs\n",
    "        Inputs should be (batch_size, input_dim)\n",
    "        Outputs will be of size (batch_size, self.post_embed_dim)\n",
    "        \"\"\"\n",
    "        if self.skip_embedding:\n",
    "            # no embeddings required\n",
    "            return x\n",
    "\n",
    "        cols = []\n",
    "        cat_feat_counter = 0\n",
    "        for feat_init_idx, is_continuous in enumerate(self.continuous_idx):\n",
    "            # Enumerate through continuous idx boolean mask to apply embeddings\n",
    "            if is_continuous:\n",
    "                cols.append(x[:, feat_init_idx].float().view(-1, 1))\n",
    "            else:\n",
    "                cols.append(self.embeddings[cat_feat_counter](x[:, feat_init_idx].long()))\n",
    "                cat_feat_counter += 1\n",
    "        # concat\n",
    "        post_embeddings = torch.cat(cols, dim=1)\n",
    "        return post_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabModel(BaseEstimator):\n",
    "    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n",
    "                 lambda_sparse=1e-3, seed=0,\n",
    "                 clip_value=1, verbose=1,\n",
    "                 optimizer_fn=torch.optim.Adam,\n",
    "                 optimizer_params=dict(lr=2e-2),\n",
    "                 scheduler_params=None, scheduler_fn=None,\n",
    "                 mask_type=\"sparsemax\",\n",
    "                 input_dim=None, output_dim=None,\n",
    "                 device_name='auto'):\n",
    "        \"\"\" Class for TabNet model\n",
    "        Parameters\n",
    "        ----------\n",
    "            device_name: str\n",
    "                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.cat_idxs = cat_idxs\n",
    "        self.cat_dims = cat_dims\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.clip_value = clip_value\n",
    "        self.verbose = verbose\n",
    "        self.optimizer_fn = optimizer_fn\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.device_name = device_name\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "        self.mask_type = mask_type\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        #self.batch_size = 1024\n",
    "        self.batch_size = 1024\n",
    "\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        # Defining device\n",
    "        if device_name == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device_name = 'cuda'\n",
    "            else:\n",
    "                device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        print(f\"Device used : {self.device}\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def construct_loaders(self, X_train, y_scored_train, X_valid, y_valid,\n",
    "                          weights, batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define construct_loaders to use this base class')\n",
    "\n",
    "    def init_network(\n",
    "                     self,\n",
    "                     input_dim,\n",
    "                     output_dim,\n",
    "                     n_d,\n",
    "                     n_a,\n",
    "                     n_steps,\n",
    "                     gamma,\n",
    "                     cat_idxs,\n",
    "                     cat_dims,\n",
    "                     cat_emb_dim,\n",
    "                     n_independent,\n",
    "                     n_shared,\n",
    "                     epsilon,\n",
    "                     virtual_batch_size,\n",
    "                     momentum,\n",
    "                     device_name,\n",
    "                     mask_type,\n",
    "                     ):\n",
    "        self.network = TabNet(\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            n_d=n_d,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=cat_emb_dim,\n",
    "            n_independent=n_independent,\n",
    "            n_shared=n_shared,\n",
    "            epsilon=epsilon,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            momentum=momentum,\n",
    "            device_name=device_name,\n",
    "            mask_type=mask_type).to(self.device)\n",
    "\n",
    "        self.reducing_matrix = create_explain_matrix(\n",
    "            self.network.input_dim,\n",
    "            self.network.cat_emb_dim,\n",
    "            self.network.cat_idxs,\n",
    "            self.network.post_embed_dim)\n",
    "\n",
    "    def fit(self, X_train, y_scored_train, X_valid=None, y_valid=None, loss_fn=None,loss_tr=None,\n",
    "            weights=0, max_epochs=100, patience=10, batch_size=1024,\n",
    "            virtual_batch_size=128, num_workers=0, drop_last=False):\n",
    "        \"\"\"Train a neural network stored in self.network\n",
    "        Using train_dataloader for training data and\n",
    "        valid_dataloader for validation.\n",
    "        Parameters\n",
    "        ----------\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            weights : bool or dictionnary\n",
    "                0 for no balancing\n",
    "                1 for automated balancing\n",
    "                dict for custom weights per class\n",
    "            max_epochs : int\n",
    "                Maximum number of epochs during training\n",
    "            patience : int\n",
    "                Number of consecutive non improving epoch before early stopping\n",
    "            batch_size : int\n",
    "                Training batch size\n",
    "            virtual_batch_size : int\n",
    "                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n",
    "            num_workers : int\n",
    "                Number of workers used in torch.utils.data.DataLoader\n",
    "            drop_last : bool\n",
    "                Whether to drop last batch during training\n",
    "        \"\"\"\n",
    "        # update model name\n",
    "\n",
    "        self.update_fit_params(X_train, y_scored_train, X_valid, y_valid, loss_fn,loss_tr,\n",
    "                               weights, max_epochs, patience, batch_size,virtual_batch_size, num_workers, drop_last)\n",
    "\n",
    "\n",
    "        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n",
    "                                                                    y_scored_train,\n",
    "                                                                    X_valid,\n",
    "                                                                    y_valid,\n",
    "                                                                    self.updated_weights,\n",
    "                                                                    self.batch_size,\n",
    "                                                                    self.num_workers,\n",
    "                                                                    self.drop_last)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=self.virtual_batch_size,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "\n",
    "        self.optimizer = self.optimizer_fn(self.network.parameters(),\n",
    "                                           **self.optimizer_params)\n",
    "\n",
    "        if self.scheduler_fn:\n",
    "            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "        self.losses_train = []\n",
    "        self.losses_valid = []\n",
    "        self.learning_rates = []\n",
    "        self.metrics_train = []\n",
    "        self.metrics_valid = []\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(\"Will train until validation stopping metric\",\n",
    "                  f\"hasn't improved in {self.patience} rounds.\")\n",
    "            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n",
    "            print('---------------------------------------')\n",
    "            print(msg_epoch)\n",
    "\n",
    "        total_time = 0\n",
    "        while (self.epoch < self.max_epochs and self.patience_counter < self.patience):\n",
    "            starting_time = time.time()\n",
    "            # updates learning rate history\n",
    "            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n",
    "\n",
    "            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n",
    "\n",
    "            # leaving it here, may be used for callbacks later\n",
    "            self.losses_train.append(fit_metrics['train']['loss_avg'])\n",
    "            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n",
    "            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n",
    "            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n",
    "\n",
    "            stopping_loss = fit_metrics['valid']['stopping_loss']\n",
    "            if stopping_loss < self.best_cost:\n",
    "                self.best_cost = stopping_loss\n",
    "                self.patience_counter = 0\n",
    "                # Saving model\n",
    "                self.best_network = deepcopy(self.network)\n",
    "                # torch.save(self.network.state_dict(), \"best_network.pth\")\n",
    "                has_improved = True\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                has_improved=False\n",
    "            self.epoch += 1\n",
    "            total_time += time.time() - starting_time\n",
    "\n",
    "\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                if self.epoch % self.verbose == 0:\n",
    "                    separator = \"|\"\n",
    "                    msg_epoch = f\"| {self.epoch:<5} | \"\n",
    "                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n",
    "                    msg_epoch += f\" {has_improved}\"\n",
    "                    print(msg_epoch)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            if self.patience_counter == self.patience:\n",
    "                print(f\"Early stopping occured at epoch {self.epoch}\")\n",
    "            print(f\"Training done in {total_time:.3f} seconds. Best loss : {self.best_cost:.5f}\")\n",
    "            print('---------------------------------------')\n",
    "\n",
    "        self.history = {\"train\": {\"loss\": self.losses_train,\n",
    "                                  \"metric\": self.metrics_train,\n",
    "                                  \"lr\": self.learning_rates},\n",
    "                        \"valid\": {\"loss\": self.losses_valid,\n",
    "                                  \"metric\": self.metrics_valid}}\n",
    "        # load best models post training\n",
    "        self.load_best_model()\n",
    "\n",
    "        # compute feature importance once the best model is defined\n",
    "        # self._compute_feature_importances(train_dataloader)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saving model with two distinct files.\n",
    "        \"\"\"\n",
    "        saved_params = {}\n",
    "        for key, val in self.get_params().items():\n",
    "            if isinstance(val, type):\n",
    "                # Don't save torch specific params\n",
    "                continue\n",
    "            else:\n",
    "                saved_params[key] = val\n",
    "\n",
    "        # Create folder\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save models params\n",
    "        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(saved_params, f)\n",
    "\n",
    "        # Save state_dict\n",
    "        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n",
    "        shutil.make_archive(path, 'zip', path)\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Successfully saved model at {path}.zip\")\n",
    "        return f\"{path}.zip\"\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                with zipfile.ZipFile(filepath) as z:\n",
    "                    with z.open(\"model_params.json\") as f:\n",
    "                        loaded_params = json.load(f)\n",
    "                    with z.open(\"network.pt\") as f:\n",
    "                        try:\n",
    "                            saved_state_dict = torch.load(f)\n",
    "                        except io.UnsupportedOperation:\n",
    "                            # In Python <3.7, the returned file object is not seekable (which at least\n",
    "                            # some versions of PyTorch require) - so we'll try buffering it in to a\n",
    "                            # BytesIO instead:\n",
    "                            saved_state_dict = torch.load(io.BytesIO(f.read()))\n",
    "                            \n",
    "            except:\n",
    "                with open(os.path.join(filepath, \"model_params.json\")) as f:\n",
    "                        loaded_params = json.load(f)\n",
    "\n",
    "                saved_state_dict = torch.load(os.path.join(filepath, \"network.pt\"), map_location=\"cpu\")\n",
    " \n",
    "        except KeyError:\n",
    "            raise KeyError(\"Your zip file is missing at least one component\")\n",
    "\n",
    "        #print(loaded_params)\n",
    "        if torch.cuda.is_available():\n",
    "            device_name = 'cuda'\n",
    "        else:\n",
    "            device_name = 'cpu'\n",
    "        loaded_params[\"device_name\"] = device_name\n",
    "        self.__init__(**loaded_params)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=1024,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "        self.network.load_state_dict(saved_state_dict)\n",
    "        self.network.eval()\n",
    "        return\n",
    "\n",
    "    def fit_epoch(self, train_dataloader, valid_dataloader):\n",
    "        \"\"\"\n",
    "        Evaluates and updates network for one epoch.\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with valid set\n",
    "        \"\"\"\n",
    "        train_metrics = self.train_epoch(train_dataloader)\n",
    "        valid_metrics = self.predict_epoch(valid_dataloader)\n",
    "\n",
    "        fit_metrics = {'train': train_metrics,\n",
    "                       'valid': valid_metrics}\n",
    "\n",
    "        return fit_metrics\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_batch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_batch to use this base class')\n",
    "\n",
    "    def load_best_model(self):\n",
    "        if self.best_network is not None:\n",
    "            self.network = self.best_network\n",
    "            # self.network = self.network.load_state_dict(torch.load(\"best_network.pth\")).to(self.device)\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem or the last class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict to use this base class')\n",
    "\n",
    "    def explain(self, X):\n",
    "        \"\"\"\n",
    "        Return local explanation\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            M_explain: matrix\n",
    "                Importance per sample, per columns.\n",
    "            masks: matrix\n",
    "                Sparse matrix showing attention masks used by network.\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            for key, value in masks.items():\n",
    "                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n",
    "                                            self.reducing_matrix)\n",
    "\n",
    "            if batch_nb == 0:\n",
    "                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                             self.reducing_matrix)\n",
    "                res_masks = masks\n",
    "            else:\n",
    "                res_explain = np.vstack([res_explain,\n",
    "                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                                        self.reducing_matrix)])\n",
    "                for key, value in masks.items():\n",
    "                    res_masks[key] = np.vstack([res_masks[key], value])\n",
    "        return res_explain, res_masks\n",
    "\n",
    "    def _compute_feature_importances(self, loader):\n",
    "        self.network.eval()\n",
    "        feature_importances_ = np.zeros((self.network.post_embed_dim))\n",
    "        for data, targets,_ in loader:\n",
    "            data = data.to(self.device).float()\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n",
    "\n",
    "        feature_importances_ = csc_matrix.dot(feature_importances_,\n",
    "                                              self.reducing_matrix)\n",
    "        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n",
    "        \n",
    "\n",
    "\n",
    "class TabNetRegressor(TabModel):\n",
    "\n",
    "    def construct_loaders(self, X_train, y_scored_train, X_valid, y_valid, weights,\n",
    "                          batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if isinstance(weights, int):\n",
    "            if weights == 1:\n",
    "                raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "        if isinstance(weights, dict):\n",
    "            raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "\n",
    "\n",
    "\n",
    "        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n",
    "                                                                y_scored_train,\n",
    "                                                                X_valid,\n",
    "                                                                y_valid,\n",
    "                                                                weights,\n",
    "                                                                batch_size,\n",
    "                                                                num_workers,\n",
    "                                                                drop_last)\n",
    "        return train_dataloader, valid_dataloader\n",
    "\n",
    "    def update_fit_params(self, X_train, y_scored_train, X_valid, y_valid, loss_fn, loss_tr,\n",
    "                          weights, max_epochs, patience,batch_size, virtual_batch_size, num_workers, drop_last):\n",
    "\n",
    "\n",
    "        if loss_fn is None:\n",
    "            self.loss_fn = torch.nn.functional.mse_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "            self.loss_tr = loss_tr\n",
    "\n",
    "        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n",
    "        self.input_dim = X_train.shape[1]\n",
    "\n",
    "        if len(y_scored_train.shape) == 1:\n",
    "            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n",
    "                                if doing single regression.\"\"\")\n",
    "        assert y_scored_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n",
    "        self.output_dim = y_scored_train.shape[1]\n",
    "\n",
    "        self.updated_weights = weights\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        # Initialize counters and histories.\n",
    "        self.patience_counter = 0\n",
    "        self.epoch = 0\n",
    "        self.best_cost = np.inf\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.train()\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets_scored in train_loader:\n",
    "            batch_outs = self.train_batch(data, targets_scored)\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "        # stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n",
    "        total_loss = total_loss / len(train_loader)\n",
    "\n",
    "        epoch_metrics = {'loss_avg': total_loss,\n",
    "                         'stopping_loss': total_loss,\n",
    "                         }\n",
    "\n",
    "        # if self.scheduler is not None:\n",
    "        #     self.scheduler.step()\n",
    "            \n",
    "        return epoch_metrics\n",
    "\n",
    "    def train_batch(self, data, targets_scored):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        self.network.train()\n",
    "        data = data.to(self.device).float()\n",
    "\n",
    "        \n",
    "\n",
    "        targets_scored = targets_scored.to(self.device).float()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "\n",
    "        loss= self.loss_fn(output, targets_scored)\n",
    "\n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if self.clip_value:\n",
    "            clip_grad_norm_(self.network.parameters(), self.clip_value)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets_scored}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        self.network.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in loader:\n",
    "            batch_outs = self.predict_batch(data, targets)\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        stopping_loss = log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step(stopping_loss)\n",
    "\n",
    "        total_loss = total_loss / len(loader)\n",
    "        epoch_metrics = {'total_loss': total_loss,\n",
    "                         'stopping_loss': stopping_loss}\n",
    "\n",
    "        return epoch_metrics\n",
    "\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        data = data.to(self.device).float()\n",
    "        targets = targets.to(self.device).float()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "       \n",
    "        loss = self.loss_fn(output, targets)\n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        results = []\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            output, M_loss = self.network(data)\n",
    "            predictions = output.cpu().detach().numpy()\n",
    "            results.append(predictions)\n",
    "        res = np.vstack(results)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_name = \"../data/tabnet-weights-public/tabnet-raw-public-step1/tabnet_raw_step1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def run_training(fold, seed):\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = train_df[train_df['fold'] != fold][feat_cols]\n",
    "    valid = train_df[train_df['fold'] == fold][feat_cols]\n",
    "\n",
    "\n",
    "    X_train, y_scored_train  = train.values, train_targets_scored.iloc[:,1:].values[train.index, :]\n",
    "    X_val, y_val = valid.values, train_targets_scored.iloc[:,1:].values[valid.index, :]\n",
    "    \n",
    "\n",
    "    model = TabNetRegressor(\n",
    "                            n_d = 28,\n",
    "                            n_a = 28,\n",
    "                            n_steps = 1,\n",
    "                            gamma = 1.3,\n",
    "                            n_independent=2,\n",
    "                            n_shared=1,\n",
    "                            momentum=0.02,\n",
    "                            epsilon=1e-15,\n",
    "                            lambda_sparse = 0,\n",
    "                            optimizer_fn = optim.Adam,\n",
    "                            optimizer_params = dict(lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY),\n",
    "                            mask_type = \"entmax\",\n",
    "                            scheduler_params = dict(\n",
    "                                mode = \"min\", patience = PATIENCE_SCH, min_lr = 1e-5, factor = 0.9, verbose = True),\n",
    "                            scheduler_fn = ReduceLROnPlateau,\n",
    "                            seed = seed,\n",
    "                            verbose = 1)\n",
    "\n",
    "                             \n",
    "\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "\n",
    "\n",
    "    model.fit(\n",
    "        X_train = X_train,\n",
    "        y_scored_train=y_scored_train,\n",
    "        X_valid=X_val, \n",
    "        y_valid=y_val,\n",
    "        max_epochs = EPOCHS,\n",
    "        patience = PATIENCE,\n",
    "        batch_size = BATCH_SIZE, \n",
    "        virtual_batch_size = 32,\n",
    "        num_workers = 1,\n",
    "        drop_last = False,\n",
    "        # To use binary cross entropy because this is not a regression problem\n",
    "        loss_fn = F.binary_cross_entropy_with_logits\n",
    "    )\n",
    "\n",
    "\n",
    "    oof = np.zeros((train_df.shape[0], len(targets_scored)))\n",
    "    \n",
    "    model.load_best_model()\n",
    "    preds = model.predict(X_val)\n",
    "    oof[valid.index] = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n",
    "\n",
    "    X_test = test_df[feat_cols].values\n",
    "    preds = model.predict(X_test)\n",
    "    predictions = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200 #200\n",
    "PATIENCE_SCH=5\n",
    "PATIENCE=20 #20\n",
    "LEARNING_RATE =2e-2 #1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "\n",
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((train_df.shape[0], len(targets_scored)))\n",
    "    predictions = np.zeros((test_df.shape[0], len(targets_scored)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        print(f\"SEED {seed} - FOLD {fold}\")\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SEED 0 - FOLD 0\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 20 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  0.40031 |   0.05173 |   1.8        True\n| 2     |  0.02883 |   0.02749 |   3.5        True\n| 3     |  0.02394 |   0.02208 |   5.2        True\n| 4     |  0.02169 |   0.02124 |   6.9        True\n| 5     |  0.02101 |   0.02095 |   8.6        True\n| 6     |  0.02080 |   0.02062 |   10.2       True\n| 7     |  0.02054 |   0.02033 |   11.9       True\n| 8     |  0.02030 |   0.02013 |   13.6       True\n| 9     |  0.01980 |   0.02015 |   15.3       False\n| 10    |  0.01967 |   0.01962 |   17.0       True\n| 11    |  0.01938 |   0.01971 |   18.7       False\n| 12    |  0.01927 |   0.01922 |   20.3       True\n| 13    |  0.01866 |   0.01915 |   22.0       True\n| 14    |  0.01852 |   0.01901 |   23.7       True\n| 15    |  0.01843 |   0.01891 |   25.4       True\n| 16    |  0.01818 |   0.02277 |   27.1       False\n| 17    |  0.01817 |   0.02085 |   28.8       False\n| 18    |  0.01822 |   0.02227 |   30.5       False\n| 19    |  0.01781 |   0.01852 |   32.2       True\n| 20    |  0.01782 |   0.01857 |   33.9       False\n| 21    |  0.01779 |   0.02142 |   35.6       False\n| 22    |  0.01769 |   0.02105 |   37.3       False\n| 23    |  0.01760 |   0.02181 |   39.0       False\n| 24    |  0.01762 |   0.02182 |   40.7       False\nEpoch    25: reducing learning rate of group 0 to 1.8000e-02.\n| 25    |  0.01733 |   0.02088 |   42.4       False\n| 26    |  0.01723 |   0.02091 |   44.0       False\n| 27    |  0.01729 |   0.01816 |   45.7       True\n| 28    |  0.01726 |   0.01861 |   47.4       False\n| 29    |  0.01721 |   0.02145 |   49.1       False\n| 30    |  0.01707 |   0.01831 |   50.8       False\n| 31    |  0.01698 |   0.01795 |   52.5       True\n| 32    |  0.01700 |   0.01806 |   54.1       False\n| 33    |  0.01712 |   0.01802 |   55.9       False\n| 34    |  0.01686 |   0.01810 |   57.5       False\n| 35    |  0.01674 |   0.01812 |   59.2       False\n| 36    |  0.01682 |   0.01804 |   60.9       False\nEpoch    37: reducing learning rate of group 0 to 1.6200e-02.\n| 37    |  0.01675 |   0.01818 |   62.6       False\n| 38    |  0.01653 |   0.02005 |   64.3       False\n| 39    |  0.01642 |   0.01790 |   66.0       True\n| 40    |  0.01655 |   0.01793 |   67.7       False\n| 41    |  0.01658 |   0.01797 |   69.3       False\n| 42    |  0.01679 |   0.01801 |   71.0       False\n| 43    |  0.01655 |   0.01796 |   72.7       False\n| 44    |  0.01651 |   0.01800 |   74.4       False\nEpoch    45: reducing learning rate of group 0 to 1.4580e-02.\n| 45    |  0.01639 |   0.01805 |   76.1       False\n| 46    |  0.01636 |   0.01785 |   78.1       True\n| 47    |  0.01609 |   0.01793 |   79.8       False\n| 48    |  0.01622 |   0.01788 |   81.5       False\n| 49    |  0.01646 |   0.01825 |   83.2       False\n| 50    |  0.01647 |   0.01804 |   84.9       False\n| 51    |  0.01628 |   0.01788 |   86.6       False\nEpoch    52: reducing learning rate of group 0 to 1.3122e-02.\n| 52    |  0.01617 |   0.01788 |   88.8       False\n| 53    |  0.01611 |   0.01780 |   90.6       True\n| 54    |  0.01602 |   0.01777 |   92.7       True\n| 55    |  0.01587 |   0.01781 |   94.6       False\n| 56    |  0.01604 |   0.01778 |   96.5       False\n| 57    |  0.01620 |   0.01787 |   98.5       False\n| 58    |  0.01608 |   0.01801 |   100.2      False\n| 59    |  0.01613 |   0.01783 |   101.9      False\nEpoch    60: reducing learning rate of group 0 to 1.1810e-02.\n| 60    |  0.01599 |   0.01790 |   103.6      False\n| 61    |  0.01577 |   0.01773 |   105.4      True\n| 62    |  0.01573 |   0.01778 |   107.2      False\n| 63    |  0.01572 |   0.01809 |   108.8      False\n| 64    |  0.01616 |   0.01805 |   110.4      False\n| 65    |  0.01583 |   0.01778 |   112.0      False\n| 66    |  0.01570 |   0.01780 |   113.6      False\nEpoch    67: reducing learning rate of group 0 to 1.0629e-02.\n| 67    |  0.01554 |   0.01794 |   115.3      False\n| 68    |  0.01544 |   0.01785 |   116.9      False\n| 69    |  0.01547 |   0.01791 |   118.5      False\n| 70    |  0.01556 |   0.01807 |   120.1      False\n| 71    |  0.01544 |   0.01788 |   121.8      False\n| 72    |  0.01554 |   0.01802 |   123.4      False\nEpoch    73: reducing learning rate of group 0 to 9.5659e-03.\n| 73    |  0.01557 |   0.01788 |   125.0      False\n| 74    |  0.01544 |   0.01802 |   126.6      False\n| 75    |  0.01527 |   0.01785 |   128.2      False\n| 76    |  0.01533 |   0.01784 |   129.9      False\n| 77    |  0.01521 |   0.01855 |   131.5      False\n| 78    |  0.01540 |   0.01831 |   133.1      False\nEpoch    79: reducing learning rate of group 0 to 8.6093e-03.\n| 79    |  0.01537 |   0.01799 |   134.7      False\n| 80    |  0.01511 |   0.01806 |   136.3      False\n| 81    |  0.01495 |   0.01798 |   137.9      False\nEarly stopping occured at epoch 81\nTraining done in 137.914 seconds. Best loss : 0.01773\n---------------------------------------\nSEED 0 - FOLD 1\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 20 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  0.39899 |   0.05269 |   1.6        True\n| 2     |  0.02883 |   0.02697 |   3.2        True\n| 3     |  0.02450 |   0.02134 |   4.9        True\n| 4     |  0.02184 |   0.02083 |   6.5        True\n| 5     |  0.02123 |   0.02035 |   8.1        True\n| 6     |  0.02075 |   0.02011 |   9.7        True\n| 7     |  0.02040 |   0.02060 |   11.3       False\n| 8     |  0.01997 |   0.01941 |   13.0       True\n| 9     |  0.01965 |   0.01904 |   14.6       True\n| 10    |  0.01922 |   0.01912 |   16.2       False\n| 11    |  0.01894 |   0.01869 |   17.8       True\n| 12    |  0.01876 |   0.01878 |   19.5       False\n| 13    |  0.01843 |   0.01831 |   21.1       True\n| 14    |  0.01806 |   0.01949 |   22.7       False\n| 15    |  0.01793 |   0.01808 |   24.3       True\n| 16    |  0.01786 |   0.01901 |   26.0       False\n| 17    |  0.01789 |   0.01928 |   27.6       False\n| 18    |  0.01782 |   0.02066 |   29.3       False\n| 19    |  0.01763 |   0.01818 |   30.9       False\n| 20    |  0.01759 |   0.02011 |   32.5       False\nEpoch    21: reducing learning rate of group 0 to 1.8000e-02.\n| 21    |  0.01759 |   0.02204 |   34.1       False\n| 22    |  0.01745 |   0.02074 |   35.7       False\n| 23    |  0.01754 |   0.02039 |   37.4       False\n| 24    |  0.01720 |   0.01808 |   39.0       True\n| 25    |  0.01716 |   0.02199 |   40.6       False\n| 26    |  0.01711 |   0.01840 |   42.2       False\n| 27    |  0.01716 |   0.01792 |   43.8       True\n| 28    |  0.01699 |   0.02035 |   45.5       False\n| 29    |  0.01689 |   0.02092 |   47.1       False\n| 30    |  0.01696 |   0.01932 |   48.7       False\n| 31    |  0.01682 |   0.01774 |   50.3       True\n| 32    |  0.01671 |   0.01956 |   51.9       False\n| 33    |  0.01670 |   0.01798 |   53.6       False\n| 34    |  0.01668 |   0.01783 |   55.2       False\n| 35    |  0.01673 |   0.01772 |   56.8       True\n| 36    |  0.01672 |   0.01752 |   58.4       True\n| 37    |  0.01673 |   0.01803 |   60.0       False\n| 38    |  0.01664 |   0.01760 |   61.7       False\n| 39    |  0.01664 |   0.01765 |   63.3       False\n| 40    |  0.01655 |   0.01744 |   64.9       True\n| 41    |  0.01655 |   0.01751 |   66.6       False\n| 42    |  0.01641 |   0.01787 |   68.2       False\n| 43    |  0.01652 |   0.01742 |   69.8       True\n| 44    |  0.01640 |   0.01757 |   71.4       False\n| 45    |  0.01640 |   0.01854 |   73.1       False\n| 46    |  0.01645 |   0.01754 |   74.7       False\n| 47    |  0.01628 |   0.01748 |   76.3       False\n| 48    |  0.01639 |   0.01793 |   77.9       False\nEpoch    49: reducing learning rate of group 0 to 1.6200e-02.\n| 49    |  0.01653 |   0.01771 |   79.6       False\n| 50    |  0.01630 |   0.01741 |   81.2       True\n| 51    |  0.01620 |   0.01751 |   82.9       False\n| 52    |  0.01626 |   0.01762 |   84.5       False\n| 53    |  0.01633 |   0.01759 |   86.1       False\n| 54    |  0.01608 |   0.01755 |   87.7       False\n| 55    |  0.01603 |   0.01749 |   89.4       False\nEpoch    56: reducing learning rate of group 0 to 1.4580e-02.\n| 56    |  0.01595 |   0.01755 |   91.0       False\n| 57    |  0.01600 |   0.01812 |   92.6       False\n| 58    |  0.01596 |   0.01742 |   94.2       False\n| 59    |  0.01615 |   0.01754 |   95.9       False\n| 60    |  0.01606 |   0.01745 |   97.5       False\n| 61    |  0.01592 |   0.01750 |   99.1       False\n| 62    |  0.01581 |   0.01732 |   100.7      True\n| 63    |  0.01574 |   0.01743 |   102.4      False\n| 64    |  0.01574 |   0.01749 |   104.0      False\n| 65    |  0.01587 |   0.01769 |   105.6      False\n| 66    |  0.01583 |   0.01739 |   107.3      False\n| 67    |  0.01579 |   0.01784 |   108.9      False\nEpoch    68: reducing learning rate of group 0 to 1.3122e-02.\n| 68    |  0.01596 |   0.01743 |   110.5      False\n| 69    |  0.01577 |   0.01741 |   112.1      False\n| 70    |  0.01579 |   0.01759 |   113.8      False\n| 71    |  0.01573 |   0.01747 |   115.4      False\n| 72    |  0.01559 |   0.01759 |   117.0      False\n| 73    |  0.01552 |   0.01757 |   118.6      False\nEpoch    74: reducing learning rate of group 0 to 1.1810e-02.\n| 74    |  0.01559 |   0.01757 |   120.2      False\n| 75    |  0.01544 |   0.01750 |   121.9      False\n| 76    |  0.01553 |   0.01767 |   123.5      False\n| 77    |  0.01536 |   0.01784 |   125.1      False\n| 78    |  0.01526 |   0.01753 |   126.7      False\n| 79    |  0.01525 |   0.01759 |   128.4      False\nEpoch    80: reducing learning rate of group 0 to 1.0629e-02.\n| 80    |  0.01536 |   0.01783 |   130.0      False\n| 81    |  0.01517 |   0.01772 |   131.6      False\n| 82    |  0.01524 |   0.01762 |   133.3      False\nEarly stopping occured at epoch 82\nTraining done in 133.274 seconds. Best loss : 0.01732\n---------------------------------------\nSEED 0 - FOLD 2\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 20 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  0.40224 |   0.05289 |   1.6        True\n| 2     |  0.02908 |   0.02651 |   3.3        True\n| 3     |  0.02426 |   0.02146 |   4.9        True\n| 4     |  0.02210 |   0.02119 |   6.5        True\n| 5     |  0.02152 |   0.02085 |   8.2        True\n| 6     |  0.02112 |   0.02073 |   9.8        True\n| 7     |  0.02082 |   0.02051 |   11.5       True\n| 8     |  0.02047 |   0.02025 |   13.1       True\n| 9     |  0.02006 |   0.01968 |   14.8       True\n| 10    |  0.01955 |   0.01924 |   16.4       True\n| 11    |  0.01909 |   0.01913 |   18.0       True\n| 12    |  0.01873 |   0.01935 |   19.7       False\n| 13    |  0.01842 |   0.01884 |   21.3       True\n| 14    |  0.01826 |   0.01873 |   22.9       True\n| 15    |  0.01810 |   0.02035 |   24.6       False\n| 16    |  0.01810 |   0.02040 |   26.2       False\n| 17    |  0.01787 |   0.01886 |   27.9       False\n| 18    |  0.01757 |   0.01855 |   29.5       True\n| 19    |  0.01742 |   0.01934 |   31.2       False\n| 20    |  0.01739 |   0.01954 |   32.8       False\n| 21    |  0.01743 |   0.01806 |   34.4       True\n| 22    |  0.01735 |   0.02160 |   36.1       False\n| 23    |  0.01735 |   0.01921 |   37.7       False\n| 24    |  0.01717 |   0.02080 |   39.3       False\n| 25    |  0.01711 |   0.01815 |   41.0       False\n| 26    |  0.01710 |   0.02092 |   42.6       False\nEpoch    27: reducing learning rate of group 0 to 1.8000e-02.\n| 27    |  0.01711 |   0.01892 |   44.3       False\n| 28    |  0.01691 |   0.01802 |   45.9       True\n| 29    |  0.01684 |   0.01781 |   47.6       True\n| 30    |  0.01678 |   0.01830 |   49.2       False\n| 31    |  0.01676 |   0.01903 |   50.8       False\n| 32    |  0.01671 |   0.02028 |   52.5       False\n| 33    |  0.01668 |   0.01811 |   54.1       False\n| 34    |  0.01670 |   0.01784 |   55.8       False\nEpoch    35: reducing learning rate of group 0 to 1.6200e-02.\n| 35    |  0.01661 |   0.01784 |   57.4       False\n| 36    |  0.01643 |   0.01775 |   59.1       True\n| 37    |  0.01633 |   0.01783 |   60.7       False\n| 38    |  0.01645 |   0.01787 |   62.3       False\n| 39    |  0.01636 |   0.01796 |   64.0       False\n| 40    |  0.01642 |   0.01793 |   65.6       False\n| 41    |  0.01631 |   0.01797 |   67.3       False\n| 42    |  0.01626 |   0.01767 |   69.0       True\n| 43    |  0.01628 |   0.01809 |   70.6       False\n| 44    |  0.01620 |   0.01779 |   72.2       False\n| 45    |  0.01620 |   0.01852 |   73.9       False\n| 46    |  0.01636 |   0.01788 |   75.6       False\n| 47    |  0.01633 |   0.01779 |   77.2       False\nEpoch    48: reducing learning rate of group 0 to 1.4580e-02.\n| 48    |  0.01613 |   0.01788 |   78.9       False\n| 49    |  0.01591 |   0.01773 |   80.5       False\n| 50    |  0.01587 |   0.01782 |   82.1       False\n| 51    |  0.01588 |   0.01793 |   83.8       False\n| 52    |  0.01584 |   0.01776 |   85.5       False\n| 53    |  0.01578 |   0.01771 |   87.1       False\nEpoch    54: reducing learning rate of group 0 to 1.3122e-02.\n| 54    |  0.01585 |   0.01830 |   88.8       False\n| 55    |  0.01586 |   0.01773 |   90.4       False\n| 56    |  0.01572 |   0.01769 |   92.1       False\n| 57    |  0.01551 |   0.01779 |   93.7       False\n| 58    |  0.01553 |   0.01783 |   95.4       False\n| 59    |  0.01548 |   0.01781 |   97.0       False\nEpoch    60: reducing learning rate of group 0 to 1.1810e-02.\n| 60    |  0.01551 |   0.01783 |   98.7       False\n| 61    |  0.01540 |   0.01782 |   100.3      False\n| 62    |  0.01531 |   0.01776 |   102.0      False\nEarly stopping occured at epoch 62\nTraining done in 102.018 seconds. Best loss : 0.01767\n---------------------------------------\nSEED 0 - FOLD 3\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 20 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  0.40152 |   0.05567 |   1.7        True\n| 2     |  0.02923 |   0.02773 |   3.3        True\n| 3     |  0.02437 |   0.02205 |   5.0        True\n| 4     |  0.02182 |   0.02117 |   6.6        True\n| 5     |  0.02142 |   0.02078 |   8.3        True\n| 6     |  0.02120 |   0.02052 |   9.9        True\n| 7     |  0.02076 |   0.02023 |   11.6       True\n| 8     |  0.02048 |   0.02003 |   13.2       True\n| 9     |  0.02010 |   0.01972 |   14.9       True\n| 10    |  0.01968 |   0.01997 |   16.6       False\n| 11    |  0.01938 |   0.01948 |   18.3       True\n| 12    |  0.01899 |   0.01902 |   20.0       True\n| 13    |  0.01883 |   0.01884 |   21.6       True\n| 14    |  0.01867 |   0.01889 |   23.3       False\n| 15    |  0.01829 |   0.02058 |   25.0       False\n| 16    |  0.01803 |   0.01886 |   26.7       False\n| 17    |  0.01791 |   0.02029 |   28.3       False\n| 18    |  0.01779 |   0.02173 |   30.0       False\nEpoch    19: reducing learning rate of group 0 to 1.8000e-02.\n| 19    |  0.01779 |   0.02097 |   31.7       False\n| 20    |  0.01756 |   0.01953 |   33.3       False\n| 21    |  0.01743 |   0.02197 |   35.0       False\n| 22    |  0.01735 |   0.01838 |   36.7       True\n| 23    |  0.01728 |   0.02084 |   38.3       False\n| 24    |  0.01701 |   0.02224 |   40.0       False\n| 25    |  0.01704 |   0.02091 |   41.7       False\n| 26    |  0.01716 |   0.02064 |   43.3       False\n| 27    |  0.01710 |   0.02143 |   45.0       False\n| 28    |  0.01682 |   0.01793 |   46.6       True\n| 29    |  0.01674 |   0.01835 |   48.3       False\n| 30    |  0.01688 |   0.01808 |   49.9       False\n| 31    |  0.01674 |   0.01795 |   51.6       False\n| 32    |  0.01654 |   0.01772 |   53.3       True\n| 33    |  0.01661 |   0.01780 |   55.0       False\n| 34    |  0.01660 |   0.01774 |   56.6       False\n| 35    |  0.01653 |   0.01761 |   58.3       True\n| 36    |  0.01646 |   0.01823 |   59.9       False\n| 37    |  0.01664 |   0.01773 |   61.6       False\n| 38    |  0.01646 |   0.01755 |   63.2       True\n| 39    |  0.01628 |   0.01777 |   64.9       False\n| 40    |  0.01638 |   0.01788 |   66.5       False\n| 41    |  0.01634 |   0.01789 |   68.2       False\n| 42    |  0.01623 |   0.01765 |   69.8       False\n| 43    |  0.01633 |   0.01777 |   71.5       False\nEpoch    44: reducing learning rate of group 0 to 1.6200e-02.\n| 44    |  0.01628 |   0.01781 |   73.1       False\n| 45    |  0.01628 |   0.01765 |   74.8       False\n| 46    |  0.01613 |   0.01764 |   76.5       False\n| 47    |  0.01619 |   0.01755 |   78.1       False\n| 48    |  0.01608 |   0.01760 |   79.8       False\n| 49    |  0.01604 |   0.01765 |   81.5       False\nEpoch    50: reducing learning rate of group 0 to 1.4580e-02.\n| 50    |  0.01626 |   0.01794 |   83.1       False\n| 51    |  0.01609 |   0.01742 |   84.8       True\n| 52    |  0.01583 |   0.01766 |   86.5       False\n| 53    |  0.01579 |   0.01750 |   88.1       False\n| 54    |  0.01582 |   0.01764 |   89.8       False\n| 55    |  0.01584 |   0.01755 |   91.4       False\n| 56    |  0.01588 |   0.01761 |   93.1       False\nEpoch    57: reducing learning rate of group 0 to 1.3122e-02.\n| 57    |  0.01581 |   0.01770 |   94.8       False\n| 58    |  0.01557 |   0.01740 |   96.5       True\n| 59    |  0.01564 |   0.01773 |   98.1       False\n| 60    |  0.01550 |   0.01753 |   99.8       False\n| 61    |  0.01569 |   0.01776 |   101.4      False\n| 62    |  0.01588 |   0.01759 |   103.1      False\n| 63    |  0.01573 |   0.01750 |   104.8      False\nEpoch    64: reducing learning rate of group 0 to 1.1810e-02.\n| 64    |  0.01580 |   0.01752 |   106.5      False\n| 65    |  0.01548 |   0.01747 |   108.2      False\n| 66    |  0.01539 |   0.01762 |   109.9      False\n| 67    |  0.01527 |   0.01758 |   111.5      False\n| 68    |  0.01526 |   0.01777 |   113.2      False\n| 69    |  0.01532 |   0.01755 |   114.8      False\nEpoch    70: reducing learning rate of group 0 to 1.0629e-02.\n| 70    |  0.01536 |   0.01791 |   116.5      False\n| 71    |  0.01525 |   0.01783 |   118.2      False\n| 72    |  0.01513 |   0.01754 |   119.8      False\n| 73    |  0.01504 |   0.01782 |   121.5      False\n| 74    |  0.01509 |   0.01765 |   123.2      False\n| 75    |  0.01500 |   0.01755 |   124.9      False\nEpoch    76: reducing learning rate of group 0 to 9.5659e-03.\n| 76    |  0.01497 |   0.01778 |   126.6      False\n| 77    |  0.01485 |   0.01773 |   128.3      False\n| 78    |  0.01475 |   0.01787 |   130.0      False\nEarly stopping occured at epoch 78\nTraining done in 129.953 seconds. Best loss : 0.01740\n---------------------------------------\nSEED 0 - FOLD 4\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 20 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  0.40380 |   0.05696 |   1.6        True\n| 2     |  0.02918 |   0.02764 |   3.3        True\n| 3     |  0.02441 |   0.02191 |   5.0        True\n| 4     |  0.02168 |   0.02141 |   6.7        True\n| 5     |  0.02118 |   0.02104 |   8.3        True\n| 6     |  0.02082 |   0.02080 |   10.0       True\n| 7     |  0.02062 |   0.02054 |   11.7       True\n| 8     |  0.02025 |   0.02026 |   13.4       True\n| 9     |  0.02004 |   0.01989 |   15.0       True\n| 10    |  0.01961 |   0.01982 |   16.7       True\n| 11    |  0.01934 |   0.01935 |   18.4       True\n| 12    |  0.01897 |   0.01909 |   20.0       True\n| 13    |  0.01860 |   0.01896 |   21.7       True\n| 14    |  0.01838 |   0.01881 |   23.4       True\n| 15    |  0.01826 |   0.01875 |   25.1       True\n| 16    |  0.01801 |   0.01858 |   26.8       True\n| 17    |  0.01801 |   0.01885 |   28.5       False\n| 18    |  0.01772 |   0.01859 |   30.2       False\n| 19    |  0.01764 |   0.01988 |   31.8       False\n| 20    |  0.01762 |   0.01833 |   33.5       True\n| 21    |  0.01756 |   0.01858 |   35.1       False\n| 22    |  0.01765 |   0.02025 |   36.9       False\n| 23    |  0.01750 |   0.02022 |   38.5       False\n| 24    |  0.01770 |   0.01877 |   40.2       False\n| 25    |  0.01749 |   0.01851 |   41.9       False\nEpoch    26: reducing learning rate of group 0 to 1.8000e-02.\n| 26    |  0.01735 |   0.02254 |   43.5       False\n| 27    |  0.01729 |   0.01828 |   45.2       True\n| 28    |  0.01713 |   0.01976 |   46.9       False\n| 29    |  0.01718 |   0.01913 |   48.7       False\n| 30    |  0.01721 |   0.01831 |   50.3       False\n| 31    |  0.01706 |   0.01845 |   52.0       False\n| 32    |  0.01708 |   0.01840 |   53.7       False\nEpoch    33: reducing learning rate of group 0 to 1.6200e-02.\n| 33    |  0.01714 |   0.02038 |   55.4       False\n| 34    |  0.01693 |   0.01835 |   57.1       False\n| 35    |  0.01693 |   0.01802 |   58.8       True\n| 36    |  0.01689 |   0.01809 |   60.4       False\n| 37    |  0.01690 |   0.01803 |   62.1       False\n| 38    |  0.01685 |   0.02018 |   63.8       False\n| 39    |  0.01661 |   0.01792 |   65.5       True\n| 40    |  0.01645 |   0.01784 |   67.2       True\n| 41    |  0.01672 |   0.01837 |   68.9       False\n| 42    |  0.01671 |   0.01800 |   70.6       False\n| 43    |  0.01677 |   0.01809 |   72.3       False\n| 44    |  0.01648 |   0.01806 |   74.0       False\n| 45    |  0.01642 |   0.01788 |   75.7       False\nEpoch    46: reducing learning rate of group 0 to 1.4580e-02.\n| 46    |  0.01631 |   0.01800 |   77.4       False\n| 47    |  0.01650 |   0.01826 |   79.1       False\n| 48    |  0.01625 |   0.01838 |   80.7       False\n| 49    |  0.01646 |   0.01790 |   82.4       False\n| 50    |  0.01616 |   0.01769 |   84.1       True\n| 51    |  0.01613 |   0.01780 |   85.9       False\n| 52    |  0.01628 |   0.01777 |   87.6       False\n| 53    |  0.01618 |   0.01765 |   89.3       True\n| 54    |  0.01618 |   0.01774 |   90.9       False\n| 55    |  0.01603 |   0.01781 |   92.7       False\n| 56    |  0.01609 |   0.01769 |   94.4       False\n| 57    |  0.01623 |   0.01779 |   96.0       False\n| 58    |  0.01636 |   0.01790 |   97.7       False\nEpoch    59: reducing learning rate of group 0 to 1.3122e-02.\n| 59    |  0.01617 |   0.01772 |   99.4       False\n| 60    |  0.01598 |   0.01770 |   101.1      False\n| 61    |  0.01591 |   0.01769 |   102.8      False\n| 62    |  0.01583 |   0.01777 |   104.5      False\n| 63    |  0.01594 |   0.01774 |   106.2      False\n| 64    |  0.01593 |   0.01827 |   107.9      False\nEpoch    65: reducing learning rate of group 0 to 1.1810e-02.\n| 65    |  0.01606 |   0.01773 |   109.6      False\n| 66    |  0.01582 |   0.01767 |   111.3      False\n| 67    |  0.01575 |   0.01777 |   113.0      False\n| 68    |  0.01569 |   0.01784 |   114.8      False\n| 69    |  0.01574 |   0.01771 |   116.5      False\n| 70    |  0.01558 |   0.01772 |   118.2      False\nEpoch    71: reducing learning rate of group 0 to 1.0629e-02.\n| 71    |  0.01547 |   0.01768 |   119.9      False\n| 72    |  0.01540 |   0.01779 |   121.7      False\n| 73    |  0.01542 |   0.01777 |   123.3      False\nEarly stopping occured at epoch 73\nTraining done in 123.334 seconds. Best loss : 0.01765\n---------------------------------------\n"
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "#SEED = [0,1,2,3,4,5,6] #<-- Update\n",
    "#SEED = [0,3,6]\n",
    "SEED = [0]\n",
    "oof = np.zeros((train_df.shape[0], len(targets_scored)))\n",
    "predictions = np.zeros((test_df.shape[0], len(targets_scored)))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train_df[targets_scored] = oof\n",
    "test_df[targets_scored] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV log_loss:  0.01755560884646033\n"
    }
   ],
   "source": [
    "y_true = train_targets_scored.iloc[:,1:].values\n",
    "y_pred = train_df[targets_scored].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(targets_scored)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / len(targets_scored)\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.640122564446882\n"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantile transform\n",
    "#CV log_loss:  0.01759554358186798 - 0.6338855281458607\n",
    "\n",
    "\n",
    "#Gauss rank\n",
    "#CV log_loss:  0.017560459188415253 - 0.635720188915399\n",
    "\n",
    "#With limited stats\n",
    "#CV log_loss:  0.01758385908416158 - 0.6332655330320774\n",
    "\n",
    "\n",
    "#with pca 300/25\n",
    "#CV log_loss:  0.01755560884646033 - 0.640122564446882"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#withput dropout .1\n",
    "#CV log_loss:  0.01760583639383521 - 0.6293958771679892\n",
    "#factor .85 CV log_loss:  0.017566634446178522 - 0.6309077494697637\n",
    "#factor .9 CV log_loss:  0.017560459188415253 - 0.635720188915399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV log_loss:  0.01737219416857171 - 0.623911530267073\n",
    "#without var threshold - CV log_loss:  0.01745984046908206 - 0.65454762689149\n",
    "#without prelu - CV log_loss:  0.017398596719990684 - 0.6234924178905198\n",
    "#28/28 - CV log_loss:  0.017367893132362852 - 0.6253214143582515\n",
    "#28/28 shared 1 - CV log_loss:  0.017362615722795263 - 0.6313124750373218\n",
    "#factor .85 CV log_loss:  0.01733148506163567 - 0.6320480113875424\n",
    "#factor .8 CV log_loss:  0.017318972353999255 - 0.6334342588920354\n",
    "    #factor .75 CV log_loss:  0.017354087985405227 - 0.6350515869886433\n",
    "    #indep 3 - CV log_loss:  0.017382696307823724 - 0.6255216650265163\n",
    "    #24/28 - CV log_loss:  0.01734208324672452 - 0.6313557477377315\n",
    "    #28/24 - CV log_loss:  0.017345107979068086 - 0.6273993901501496\n",
    "    #28/30 - CV log_loss:  0.01739716192160574 - 0.6304467367115033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_csv3 = train_features[[\"sig_id\"]].merge(train_df[train_targets_scored.columns], on='sig_id', how='inner')\n",
    "sub_mdl3 = sample_submission.drop(columns=targets_scored).merge(test_df[train_targets_scored.columns], on='sig_id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "159"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset:\n",
    "    def __init__(self, features, targets_scored,targets_nscored):\n",
    "        self.features = features\n",
    "        self.targets_scored = targets_scored\n",
    "        self.targets_nscored = targets_nscored\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y_scored' : torch.tensor(self.targets_scored[idx, :], dtype=torch.float),\n",
    "            'y_nscored' : torch.tensor(self.targets_nscored[idx, :], dtype=torch.float)           \n",
    "        }\n",
    "        return dct\n",
    "\n",
    "class ValidDataset:\n",
    "    def __init__(self, features, targets_scored):\n",
    "        self.features = features\n",
    "        self.targets_scored = targets_scored\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y_scored' : torch.tensor(self.targets_scored[idx, :], dtype=torch.float),          \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets1, targets2 = data['x'].to(device), data['y_scored'].to(device), data['y_nscored'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs1,outputs2 = model(inputs)\n",
    "        loss1 = loss_fn(outputs1, targets1)\n",
    "        loss2 = loss_fn(outputs2, targets2)\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y_scored'].to(device)\n",
    "        outputs,_ = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds_scored = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_scored,_ = model(inputs)\n",
    "        \n",
    "        preds_scored.append(outputs_scored.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds_scored = np.concatenate(preds_scored)\n",
    "    \n",
    "    return preds_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, \n",
    "                 num_targets_scored,\n",
    "                 num_targets_nscored, \n",
    "                 hidden_sizes,\n",
    "                 dropout_rates):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_sizes[0]))\n",
    "        self.activation1 = torch.nn.PReLU(num_parameters = hidden_sizes[0], init = 1.0)\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_sizes[0])\n",
    "        self.dropout2 = nn.Dropout(dropout_rates[0])\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_sizes[0], hidden_sizes[1]))\n",
    "        self.activation2 = torch.nn.PReLU(num_parameters = hidden_sizes[1], init = 1.0)\n",
    "\n",
    "\n",
    "        self.batch_norm2b = nn.BatchNorm1d(hidden_sizes[1])\n",
    "        self.dropout2b = nn.Dropout(dropout_rates[1])\n",
    "        self.dense2b = nn.utils.weight_norm(nn.Linear(hidden_sizes[1], hidden_sizes[2]))\n",
    "        self.activation2b = torch.nn.PReLU(num_parameters = hidden_sizes[2], init = 1.0)\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_sizes[2])\n",
    "        self.dropout3 = nn.Dropout(dropout_rates[2])\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_sizes[2], num_targets_scored))\n",
    "        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_sizes[2], num_targets_nscored))\n",
    "\n",
    "    def init_bias(self,pos_scored_rate,pos_nscored_rate):\n",
    "        self.dense3.bias.data = nn.Parameter(torch.tensor(pos_scored_rate, dtype=torch.float))\n",
    "        self.dense4.bias.data = nn.Parameter(torch.tensor(pos_nscored_rate, dtype=torch.float))\n",
    "    \n",
    "    def recalibrate_layer(self, layer):\n",
    "        if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "            print ('recalibrate layer.weight_v')\n",
    "            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "        if(torch.isnan(layer.weight).sum() > 0):\n",
    "            print ('recalibrate layer.weight')\n",
    "            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "            layer.weight += 1e-7\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        self.recalibrate_layer(self.dense1)\n",
    "        x = self.activation1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        self.recalibrate_layer(self.dense2)\n",
    "        x = self.activation2(self.dense2(x))\n",
    "\n",
    "        x = self.batch_norm2b(x)\n",
    "        x = self.dropout2b(x)\n",
    "        self.recalibrate_layer(self.dense2b)\n",
    "        x = self.activation2b(self.dense2b(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        self.recalibrate_layer(self.dense3)\n",
    "        x1 = self.dense3(x)\n",
    "\n",
    "        self.recalibrate_layer(self.dense4)\n",
    "        x2 = self.dense4(x)\n",
    "        return x1,x2\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "EPOCHS = 40\n",
    "PATIENCE=40\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5        \n",
    "EARLY_STOPPING_STEPS = PATIENCE+5\n",
    "EARLY_STOP = False\n",
    "\n",
    "hidden_sizes = [1200,1000,1000]\n",
    "dropout_rates = [0.2619422201258426,0.2619422201258426,0.27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=len(feat_cols)\n",
    "num_targets_scored=len(targets_scored)\n",
    "num_targets_nscored=len(targets_nscored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = train_df[train_df['fold'] != fold][feat_cols]\n",
    "    valid = train_df[train_df['fold'] == fold][feat_cols]\n",
    "\n",
    "\n",
    "    X_train, y_scored_train,y_nscored_train   = train.values, train_targets_scored.iloc[:,1:].values[train.index, :], train_targets_nonscored.iloc[:,1:].values[train.index, :]\n",
    "    X_val, y_val = valid.values, train_targets_scored.iloc[:,1:].values[valid.index, :]\n",
    "    \n",
    "    train_dataset = TrainDataset(X_train, y_scored_train, y_nscored_train)\n",
    "    valid_dataset = ValidDataset(X_val, y_val)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets_scored=num_targets_scored,\n",
    "        num_targets_nscored=num_targets_nscored,\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates,\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,pct_start=0.1, div_factor=1e4, final_div_factor=1e5,\n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "   \n",
    "    oof = np.zeros((train_df.shape[0], num_targets_scored))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, device)\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, device)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "        #scheduler.step(valid_loss)\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[valid.index] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_m1_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_df[feat_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets_scored=num_targets_scored,\n",
    "        num_targets_nscored=num_targets_nscored,\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_m1_.pth\"))\n",
    "    model.to(device)\n",
    "    \n",
    "    predictions_scored = np.zeros((test_df.shape[0], num_targets_scored))\n",
    "    predictions_scored = inference_fn(model, testloader, device)\n",
    "    \n",
    "    return oof, predictions_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((train_df.shape[0], num_targets_scored))\n",
    "    predictions_scored = np.zeros((test_df.shape[0], num_targets_scored))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_scored_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions_scored += pred_scored_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "FOLD: 0, EPOCH: 0, train_loss: 1.3482022180276758, valid_loss: 0.3372230759688786\nFOLD: 0, EPOCH: 1, train_loss: 0.12530934857204556, valid_loss: 0.023039331872548377\nFOLD: 0, EPOCH: 2, train_loss: 0.03445156423977631, valid_loss: 0.02146141566336155\nFOLD: 0, EPOCH: 3, train_loss: 0.03216122753699036, valid_loss: 0.02038281916507653\nFOLD: 0, EPOCH: 4, train_loss: 0.031423080520813954, valid_loss: 0.01901291012763977\nFOLD: 0, EPOCH: 5, train_loss: 0.029755684601909974, valid_loss: 0.01855149742748056\nFOLD: 0, EPOCH: 6, train_loss: 0.029626938185709363, valid_loss: 0.018501553205507143\nFOLD: 0, EPOCH: 7, train_loss: 0.02908175381119637, valid_loss: 0.018332943666194167\nFOLD: 0, EPOCH: 8, train_loss: 0.028939105186830547, valid_loss: 0.018168817992721284\nFOLD: 0, EPOCH: 9, train_loss: 0.028849283520899275, valid_loss: 0.018302622943052224\nFOLD: 0, EPOCH: 10, train_loss: 0.028824313420473653, valid_loss: 0.018372940538184983\nFOLD: 0, EPOCH: 11, train_loss: 0.028844105011290488, valid_loss: 0.018383191313062396\nFOLD: 0, EPOCH: 12, train_loss: 0.02891431436600054, valid_loss: 0.018331920302339964\nFOLD: 0, EPOCH: 13, train_loss: 0.028830498484346795, valid_loss: 0.01821832193859986\nFOLD: 0, EPOCH: 14, train_loss: 0.02886260417290032, valid_loss: 0.018302405971501554\nFOLD: 0, EPOCH: 15, train_loss: 0.028892778010819766, valid_loss: 0.018145318036632878\nFOLD: 0, EPOCH: 16, train_loss: 0.02891945855363327, valid_loss: 0.018357241499636856\nFOLD: 0, EPOCH: 17, train_loss: 0.02890425080926541, valid_loss: 0.01830141187778541\nFOLD: 0, EPOCH: 18, train_loss: 0.028840277193333295, valid_loss: 0.018289018103054593\nFOLD: 0, EPOCH: 19, train_loss: 0.02889624417430776, valid_loss: 0.017978259468717235\nFOLD: 0, EPOCH: 20, train_loss: 0.028916426989085534, valid_loss: 0.018073529936373234\nFOLD: 0, EPOCH: 21, train_loss: 0.0287852066788165, valid_loss: 0.0182809014405523\nFOLD: 0, EPOCH: 22, train_loss: 0.028817191555657807, valid_loss: 0.01836137103715113\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 23, train_loss: 0.028778546804781344, valid_loss: 0.018135733609752996\nFOLD: 0, EPOCH: 24, train_loss: 0.028520205222508487, valid_loss: 0.01816580266292606\nFOLD: 0, EPOCH: 25, train_loss: 0.028347390566897744, valid_loss: 0.01797251616205488\nFOLD: 0, EPOCH: 26, train_loss: 0.02823596406618462, valid_loss: 0.018012359951223647\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 27, train_loss: 0.02816475453950903, valid_loss: 0.018002558419747013\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 28, train_loss: 0.027915015077108845, valid_loss: 0.017939762105899197\nFOLD: 0, EPOCH: 29, train_loss: 0.02768242742647143, valid_loss: 0.017778665306312696\nFOLD: 0, EPOCH: 30, train_loss: 0.027613021241610542, valid_loss: 0.017719837597438268\nFOLD: 0, EPOCH: 31, train_loss: 0.027552613358506384, valid_loss: 0.017665617620306356\nFOLD: 0, EPOCH: 32, train_loss: 0.02743793133755817, valid_loss: 0.01773936833654131\nFOLD: 0, EPOCH: 33, train_loss: 0.027335974894573584, valid_loss: 0.01765830647200346\nFOLD: 0, EPOCH: 34, train_loss: 0.027282685287954175, valid_loss: 0.01763519567570516\nFOLD: 0, EPOCH: 35, train_loss: 0.02718944917432964, valid_loss: 0.01763021033257246\nFOLD: 0, EPOCH: 36, train_loss: 0.02706484092563829, valid_loss: 0.017572354418890816\nFOLD: 0, EPOCH: 37, train_loss: 0.026966288042090395, valid_loss: 0.01756314970552921\nFOLD: 0, EPOCH: 38, train_loss: 0.026840320487013635, valid_loss: 0.017556839622557163\nFOLD: 0, EPOCH: 39, train_loss: 0.026771375484874144, valid_loss: 0.017516986786254813\nFOLD: 0, EPOCH: 40, train_loss: 0.026679185520419302, valid_loss: 0.017466842010617257\nFOLD: 0, EPOCH: 41, train_loss: 0.02660506506286123, valid_loss: 0.017457665024059158\nFOLD: 0, EPOCH: 42, train_loss: 0.026539524893879014, valid_loss: 0.017455427055912357\nFOLD: 0, EPOCH: 43, train_loss: 0.026480145944172844, valid_loss: 0.01747906000486442\nFOLD: 0, EPOCH: 44, train_loss: 0.026457794106510633, valid_loss: 0.017444915723587785\nFOLD: 1, EPOCH: 0, train_loss: 1.3463950696652822, valid_loss: 0.3309194786208017\nFOLD: 1, EPOCH: 1, train_loss: 0.12303171257903106, valid_loss: 0.02219221235385963\nFOLD: 1, EPOCH: 2, train_loss: 0.03456439998299971, valid_loss: 0.02190087235399655\nFOLD: 1, EPOCH: 3, train_loss: 0.03273957997669269, valid_loss: 0.020021493094308035\nFOLD: 1, EPOCH: 4, train_loss: 0.030711707149217598, valid_loss: 0.019387558847665787\nFOLD: 1, EPOCH: 5, train_loss: 0.029906127629053855, valid_loss: 0.018195252759116036\nFOLD: 1, EPOCH: 6, train_loss: 0.029408447770741735, valid_loss: 0.01785643965538059\nFOLD: 1, EPOCH: 7, train_loss: 0.029550990538440482, valid_loss: 0.017979475323643003\nFOLD: 1, EPOCH: 8, train_loss: 0.029151515246634067, valid_loss: 0.01795250704245908\nFOLD: 1, EPOCH: 9, train_loss: 0.029054752272302215, valid_loss: 0.017662566314850536\nFOLD: 1, EPOCH: 10, train_loss: 0.029002824662266856, valid_loss: 0.017980596822287354\nFOLD: 1, EPOCH: 11, train_loss: 0.029003428363234458, valid_loss: 0.017946658975311686\nFOLD: 1, EPOCH: 12, train_loss: 0.029024774331028444, valid_loss: 0.017747721315494607\nFOLD: 1, EPOCH: 13, train_loss: 0.02895517606478538, valid_loss: 0.018014254846743176\nFOLD: 1, EPOCH: 14, train_loss: 0.02909119499262667, valid_loss: 0.018281537720135282\nFOLD: 1, EPOCH: 15, train_loss: 0.028999322699043, valid_loss: 0.017850295534091336\nFOLD: 1, EPOCH: 16, train_loss: 0.029062625020742416, valid_loss: 0.017697100234883172\nFOLD: 1, EPOCH: 17, train_loss: 0.02907601616134609, valid_loss: 0.01813141887209245\nFOLD: 1, EPOCH: 18, train_loss: 0.02907269336555126, valid_loss: 0.017910271031515938\nFOLD: 1, EPOCH: 19, train_loss: 0.029059911078780237, valid_loss: 0.018121462634631564\nFOLD: 1, EPOCH: 20, train_loss: 0.02907528286378314, valid_loss: 0.017802302113601138\nFOLD: 1, EPOCH: 21, train_loss: 0.029016101379355376, valid_loss: 0.017656103149056433\nFOLD: 1, EPOCH: 22, train_loss: 0.02894381832086692, valid_loss: 0.01774053568286555\nFOLD: 1, EPOCH: 23, train_loss: 0.028879565547091247, valid_loss: 0.017655026220849582\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 24, train_loss: 0.028712845033537732, valid_loss: 0.01751956891800676\nFOLD: 1, EPOCH: 25, train_loss: 0.02850484155988606, valid_loss: 0.01755239527140345\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 26, train_loss: 0.0283573657872468, valid_loss: 0.017469208554497788\nFOLD: 1, EPOCH: 27, train_loss: 0.02814793705015722, valid_loss: 0.01736248694360256\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 28, train_loss: 0.02805241386331346, valid_loss: 0.01725032784576927\nFOLD: 1, EPOCH: 29, train_loss: 0.027876341090041355, valid_loss: 0.017296648930226054\nFOLD: 1, EPOCH: 30, train_loss: 0.027741772482973815, valid_loss: 0.017176868074706624\nFOLD: 1, EPOCH: 31, train_loss: 0.02767757342679657, valid_loss: 0.01718955369932311\nFOLD: 1, EPOCH: 32, train_loss: 0.02757362617574034, valid_loss: 0.017321747381772314\nFOLD: 1, EPOCH: 33, train_loss: 0.027479154907547643, valid_loss: 0.017214439623057843\nFOLD: 1, EPOCH: 34, train_loss: 0.027357024990402868, valid_loss: 0.01715065131762198\nFOLD: 1, EPOCH: 35, train_loss: 0.02727518139583786, valid_loss: 0.017151574684040886\nFOLD: 1, EPOCH: 36, train_loss: 0.027163630011525466, valid_loss: 0.01705359670200518\nFOLD: 1, EPOCH: 37, train_loss: 0.027095550847967175, valid_loss: 0.017122900166681835\nFOLD: 1, EPOCH: 38, train_loss: 0.02692673159559278, valid_loss: 0.01707620383905513\nFOLD: 1, EPOCH: 39, train_loss: 0.02686246432853441, valid_loss: 0.017114493250846862\nFOLD: 1, EPOCH: 40, train_loss: 0.02679371182555265, valid_loss: 0.01704008398311479\nFOLD: 1, EPOCH: 41, train_loss: 0.02670945092546244, valid_loss: 0.01705656158072608\nFOLD: 1, EPOCH: 42, train_loss: 0.026657506538024785, valid_loss: 0.017053607877876076\nFOLD: 1, EPOCH: 43, train_loss: 0.02655971353452136, valid_loss: 0.017053062841296197\nFOLD: 1, EPOCH: 44, train_loss: 0.026556160708848576, valid_loss: 0.017076564872903485\nFOLD: 2, EPOCH: 0, train_loss: 1.3474369305763803, valid_loss: 0.3377487010815564\nFOLD: 2, EPOCH: 1, train_loss: 0.12434492627308316, valid_loss: 0.02240820716628257\nFOLD: 2, EPOCH: 2, train_loss: 0.03482095432216233, valid_loss: 0.0218345268684275\nFOLD: 2, EPOCH: 3, train_loss: 0.0342507721162843, valid_loss: 0.035872286897810066\nFOLD: 2, EPOCH: 4, train_loss: 0.03198344235981468, valid_loss: 0.019647763001129907\nFOLD: 2, EPOCH: 5, train_loss: 0.03042535063722273, valid_loss: 0.018829408375655905\nFOLD: 2, EPOCH: 6, train_loss: 0.029803133821182878, valid_loss: 0.018215543261783963\nFOLD: 2, EPOCH: 7, train_loss: 0.029347988280610447, valid_loss: 0.018279725247446227\nFOLD: 2, EPOCH: 8, train_loss: 0.029049575559958053, valid_loss: 0.01812678241335294\nFOLD: 2, EPOCH: 9, train_loss: 0.02887597856839208, valid_loss: 0.01802967510679189\nFOLD: 2, EPOCH: 10, train_loss: 0.02878410200568011, valid_loss: 0.01823156064047533\nFOLD: 2, EPOCH: 11, train_loss: 0.028778583395981442, valid_loss: 0.01802037048208363\nFOLD: 2, EPOCH: 12, train_loss: 0.028775041474260552, valid_loss: 0.01830177425461657\nFOLD: 2, EPOCH: 13, train_loss: 0.0287915159723837, valid_loss: 0.018343206878532383\nFOLD: 2, EPOCH: 14, train_loss: 0.028778664590994808, valid_loss: 0.019024428089751917\nFOLD: 2, EPOCH: 15, train_loss: 0.02880324387963671, valid_loss: 0.018247398383477154\nFOLD: 2, EPOCH: 16, train_loss: 0.02886565369519874, valid_loss: 0.018538490595186457\nFOLD: 2, EPOCH: 17, train_loss: 0.028967536306076676, valid_loss: 0.01803009316106053\nFOLD: 2, EPOCH: 18, train_loss: 0.028850097875416713, valid_loss: 0.018386628204847082\nFOLD: 2, EPOCH: 19, train_loss: 0.02879168782519163, valid_loss: 0.018441962352132097\nFOLD: 2, EPOCH: 20, train_loss: 0.02884034227824559, valid_loss: 0.018002154250793597\nFOLD: 2, EPOCH: 21, train_loss: 0.02878669212932569, valid_loss: 0.018247762256685424\nFOLD: 2, EPOCH: 22, train_loss: 0.028741486302583757, valid_loss: 0.018027166616829002\nFOLD: 2, EPOCH: 23, train_loss: 0.028708053344901462, valid_loss: 0.018107712761882472\nFOLD: 2, EPOCH: 24, train_loss: 0.028631936622797137, valid_loss: 0.01793453793096192\nFOLD: 2, EPOCH: 25, train_loss: 0.028632985482359454, valid_loss: 0.017883984098101362\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 26, train_loss: 0.028316664625040806, valid_loss: 0.017800150932196307\nFOLD: 2, EPOCH: 27, train_loss: 0.028119561500357885, valid_loss: 0.017883836729999852\nFOLD: 2, EPOCH: 28, train_loss: 0.028015540695212185, valid_loss: 0.017731056493871352\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 29, train_loss: 0.027847903991376398, valid_loss: 0.017598186893498197\nFOLD: 2, EPOCH: 30, train_loss: 0.027728168497772984, valid_loss: 0.017506513087188497\nFOLD: 2, EPOCH: 31, train_loss: 0.027579812727270336, valid_loss: 0.017527327872812748\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 32, train_loss: 0.02738676488018819, valid_loss: 0.01745838255566709\nFOLD: 2, EPOCH: 33, train_loss: 0.02722697865462651, valid_loss: 0.017521527814952767\nFOLD: 2, EPOCH: 34, train_loss: 0.02706800004209045, valid_loss: 0.017437244245015523\nFOLD: 2, EPOCH: 35, train_loss: 0.026926874992077368, valid_loss: 0.01748591519015677\nFOLD: 2, EPOCH: 36, train_loss: 0.026836243437698287, valid_loss: 0.017471174490364158\nFOLD: 2, EPOCH: 37, train_loss: 0.02677561297849582, valid_loss: 0.017366434392683646\nFOLD: 2, EPOCH: 38, train_loss: 0.026631685531269895, valid_loss: 0.017376349033678278\nFOLD: 2, EPOCH: 39, train_loss: 0.026519443391122086, valid_loss: 0.017373034812729147\nFOLD: 2, EPOCH: 40, train_loss: 0.02647021960766211, valid_loss: 0.01731269042391111\nFOLD: 2, EPOCH: 41, train_loss: 0.02637097278922579, valid_loss: 0.017323278849396634\nFOLD: 2, EPOCH: 42, train_loss: 0.026264696103269165, valid_loss: 0.017285503008786368\nFOLD: 2, EPOCH: 43, train_loss: 0.02623925635414402, valid_loss: 0.01732312761904562\nFOLD: 2, EPOCH: 44, train_loss: 0.026188035067306818, valid_loss: 0.017326471916235545\nFOLD: 3, EPOCH: 0, train_loss: 1.3471571211397213, valid_loss: 0.33272577208631177\nFOLD: 3, EPOCH: 1, train_loss: 0.12330962916033981, valid_loss: 0.02209456089664908\nFOLD: 3, EPOCH: 2, train_loss: 0.03399774362842967, valid_loss: 0.02125158714240088\nFOLD: 3, EPOCH: 3, train_loss: 0.032976729849720526, valid_loss: 0.019658186527736047\nFOLD: 3, EPOCH: 4, train_loss: 0.030408768459175624, valid_loss: 0.01867889815612751\nFOLD: 3, EPOCH: 5, train_loss: 0.030216265265850254, valid_loss: 0.018433134345447317\nFOLD: 3, EPOCH: 6, train_loss: 0.02926589568981724, valid_loss: 0.01804223982617259\nFOLD: 3, EPOCH: 7, train_loss: 0.029031554699270396, valid_loss: 0.017946207978050497\nFOLD: 3, EPOCH: 8, train_loss: 0.02905680625325572, valid_loss: 0.01816806131426026\nFOLD: 3, EPOCH: 9, train_loss: 0.029008300104824296, valid_loss: 0.01801145641023622\nFOLD: 3, EPOCH: 10, train_loss: 0.028946361239374118, valid_loss: 0.01806780863005449\nFOLD: 3, EPOCH: 11, train_loss: 0.028896524373741045, valid_loss: 0.018058716100366676\nFOLD: 3, EPOCH: 12, train_loss: 0.028972774689650014, valid_loss: 0.018203438259661198\nFOLD: 3, EPOCH: 13, train_loss: 0.02893733148918535, valid_loss: 0.018303094705676332\nFOLD: 3, EPOCH: 14, train_loss: 0.028973228712804125, valid_loss: 0.018110134419711196\nFOLD: 3, EPOCH: 15, train_loss: 0.028954263341470356, valid_loss: 0.01829047506565557\nFOLD: 3, EPOCH: 16, train_loss: 0.02901719812385357, valid_loss: 0.01803304348140955\nFOLD: 3, EPOCH: 17, train_loss: 0.02899795449780722, valid_loss: 0.017858453889322633\nFOLD: 3, EPOCH: 18, train_loss: 0.029059586203555123, valid_loss: 0.018357979708953816\nFOLD: 3, EPOCH: 19, train_loss: 0.029009876704781595, valid_loss: 0.018024846034891465\nFOLD: 3, EPOCH: 20, train_loss: 0.02899315756113425, valid_loss: 0.01782002645161222\nFOLD: 3, EPOCH: 21, train_loss: 0.02900323547749189, valid_loss: 0.01783427916576757\nFOLD: 3, EPOCH: 22, train_loss: 0.028911131549708164, valid_loss: 0.017912941683522043\nFOLD: 3, EPOCH: 23, train_loss: 0.028871512067687774, valid_loss: 0.018091702751596186\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 24, train_loss: 0.028626660632826116, valid_loss: 0.017783897216705716\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 25, train_loss: 0.028326290108970482, valid_loss: 0.017476262990385294\nFOLD: 3, EPOCH: 26, train_loss: 0.028146593688722073, valid_loss: 0.017420657362569782\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 27, train_loss: 0.02813718202829796, valid_loss: 0.017416808918556747\nFOLD: 3, EPOCH: 28, train_loss: 0.027912915677484804, valid_loss: 0.017370188718332964\nFOLD: 3, EPOCH: 29, train_loss: 0.027809424801682033, valid_loss: 0.017401092494016186\nFOLD: 3, EPOCH: 30, train_loss: 0.027747336870236117, valid_loss: 0.017378075048327446\nFOLD: 3, EPOCH: 31, train_loss: 0.02764403115767632, valid_loss: 0.017372628920437658\nFOLD: 3, EPOCH: 32, train_loss: 0.027557424738676877, valid_loss: 0.01732308735304019\nFOLD: 3, EPOCH: 33, train_loss: 0.02746301028795921, valid_loss: 0.017349406419431463\nFOLD: 3, EPOCH: 34, train_loss: 0.027381958779844926, valid_loss: 0.01732775763444164\nFOLD: 3, EPOCH: 35, train_loss: 0.027285630726357445, valid_loss: 0.017249474274542403\nFOLD: 3, EPOCH: 36, train_loss: 0.027192078802707423, valid_loss: 0.017300422303378582\nFOLD: 3, EPOCH: 37, train_loss: 0.02707478409483485, valid_loss: 0.017225275583126965\nFOLD: 3, EPOCH: 38, train_loss: 0.02701406674391597, valid_loss: 0.017262884525253493\nFOLD: 3, EPOCH: 39, train_loss: 0.02687486924611739, valid_loss: 0.01719112360083005\nFOLD: 3, EPOCH: 40, train_loss: 0.026787028586777457, valid_loss: 0.017172116758849693\nFOLD: 3, EPOCH: 41, train_loss: 0.026689999784431317, valid_loss: 0.017160920344073984\nFOLD: 3, EPOCH: 42, train_loss: 0.026632399264260802, valid_loss: 0.017146045205128545\nFOLD: 3, EPOCH: 43, train_loss: 0.02659839314211459, valid_loss: 0.01713266094927402\nFOLD: 3, EPOCH: 44, train_loss: 0.026559295323099533, valid_loss: 0.01715729892363443\nFOLD: 4, EPOCH: 0, train_loss: 1.3459605735583897, valid_loss: 0.32882519960403445\nFOLD: 4, EPOCH: 1, train_loss: 0.12229204659153076, valid_loss: 0.022461421149117606\nFOLD: 4, EPOCH: 2, train_loss: 0.03486476680875695, valid_loss: 0.021230302804282734\nFOLD: 4, EPOCH: 3, train_loss: 0.03294527832500256, valid_loss: 0.01985264654670443\nFOLD: 4, EPOCH: 4, train_loss: 0.030492690425828424, valid_loss: 0.018523040573511805\nFOLD: 4, EPOCH: 5, train_loss: 0.02990015427561572, valid_loss: 0.018976790830492973\nFOLD: 4, EPOCH: 6, train_loss: 0.029537377770255953, valid_loss: 0.018129124385969978\nFOLD: 4, EPOCH: 7, train_loss: 0.029226564836219278, valid_loss: 0.018644084462097713\nFOLD: 4, EPOCH: 8, train_loss: 0.029032393392637697, valid_loss: 0.017986619578940526\nFOLD: 4, EPOCH: 9, train_loss: 0.028965234606914275, valid_loss: 0.018625731500131745\nFOLD: 4, EPOCH: 10, train_loss: 0.028963687061502114, valid_loss: 0.018185462376901083\nFOLD: 4, EPOCH: 11, train_loss: 0.028933984954861828, valid_loss: 0.01828647591173649\nFOLD: 4, EPOCH: 12, train_loss: 0.02897782150628793, valid_loss: 0.018226750886866025\nFOLD: 4, EPOCH: 13, train_loss: 0.029051224517561224, valid_loss: 0.018260394196425167\nFOLD: 4, EPOCH: 14, train_loss: 0.028983216433629503, valid_loss: 0.019398957757013186\nFOLD: 4, EPOCH: 15, train_loss: 0.029077927371228697, valid_loss: 0.01812992755855833\nFOLD: 4, EPOCH: 16, train_loss: 0.02902381009265889, valid_loss: 0.018187006296856063\nFOLD: 4, EPOCH: 17, train_loss: 0.029035846274482073, valid_loss: 0.018551565440637723\nFOLD: 4, EPOCH: 18, train_loss: 0.02908804713592042, valid_loss: 0.01846867161137717\nFOLD: 4, EPOCH: 19, train_loss: 0.02900935914775316, valid_loss: 0.018370151839085986\nFOLD: 4, EPOCH: 20, train_loss: 0.029041857791751842, valid_loss: 0.01821273613188948\nFOLD: 4, EPOCH: 21, train_loss: 0.029056088631823115, valid_loss: 0.018211949935981204\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 22, train_loss: 0.028908193097823727, valid_loss: 0.018062665526356016\nFOLD: 4, EPOCH: 23, train_loss: 0.028683174211178383, valid_loss: 0.018155091788087573\nFOLD: 4, EPOCH: 24, train_loss: 0.0285186809997489, valid_loss: 0.01814140688095774\nFOLD: 4, EPOCH: 25, train_loss: 0.02847108437958425, valid_loss: 0.018057207018136977\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 26, train_loss: 0.02837921923747028, valid_loss: 0.018273658731154035\nFOLD: 4, EPOCH: 27, train_loss: 0.028221016042750246, valid_loss: 0.01784981478537832\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 28, train_loss: 0.02811327810487608, valid_loss: 0.017746810774718014\nFOLD: 4, EPOCH: 29, train_loss: 0.027896375116640632, valid_loss: 0.017761498742869922\nFOLD: 4, EPOCH: 30, train_loss: 0.02775561292893695, valid_loss: 0.017723292697753225\nFOLD: 4, EPOCH: 31, train_loss: 0.027689830801130213, valid_loss: 0.01756816062011889\nFOLD: 4, EPOCH: 32, train_loss: 0.027605606827640187, valid_loss: 0.017587941672120775\nFOLD: 4, EPOCH: 33, train_loss: 0.027537043308363342, valid_loss: 0.01761043622557606\nFOLD: 4, EPOCH: 34, train_loss: 0.027434460628424247, valid_loss: 0.017537282460502217\nFOLD: 4, EPOCH: 35, train_loss: 0.02734258935453683, valid_loss: 0.017468158474990298\nFOLD: 4, EPOCH: 36, train_loss: 0.027239909014888923, valid_loss: 0.017394146881997585\nFOLD: 4, EPOCH: 37, train_loss: 0.02714964966323689, valid_loss: 0.0174923731812409\nFOLD: 4, EPOCH: 38, train_loss: 0.027045722724529948, valid_loss: 0.017417043021747045\nFOLD: 4, EPOCH: 39, train_loss: 0.02698341952840777, valid_loss: 0.017380305326410703\nFOLD: 4, EPOCH: 40, train_loss: 0.026851453757199058, valid_loss: 0.01737574003636837\nFOLD: 4, EPOCH: 41, train_loss: 0.026799180075852542, valid_loss: 0.01736307806734528\nFOLD: 4, EPOCH: 42, train_loss: 0.02674372018362484, valid_loss: 0.01736453018550362\nFOLD: 4, EPOCH: 43, train_loss: 0.02667764128342162, valid_loss: 0.01735058461448976\nFOLD: 4, EPOCH: 44, train_loss: 0.026628154486309, valid_loss: 0.017351534802998815\n"
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "#SEED = [0,1,2,3,4,5,6] #<-- Update\n",
    "#SEED = [0,3,6]\n",
    "SEED = [0]\n",
    "oof = np.zeros((train_df.shape[0], num_targets_scored))\n",
    "predictions_scored = np.zeros((test_df.shape[0], num_targets_scored))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_scored_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions_scored += predictions_scored_ / len(SEED)\n",
    "\n",
    "train_df[targets_scored] = oof\n",
    "test_df[targets_scored] = predictions_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV log_loss:  0.017274183785952447\n"
    }
   ],
   "source": [
    "y_true = train_targets_scored.iloc[:,1:].values\n",
    "y_pred = train_df[targets_scored].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(targets_scored)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / len(targets_scored)\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6357329906483653\n"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantile trasnformer\n",
    "#epoch 30 CV log_loss:  0.017281096794149006 - 0.6325158983911762\n",
    "\n",
    "\n",
    "\n",
    "#Guass rank\n",
    "#epoch 30 CV log_loss:  0.017204733833972693 - 0.6342767831647366\n",
    "\n",
    "#epoch 35 - limited stats : CV log_loss:  0.01721146984554604 - 0.6363437074675264\n",
    "\n",
    "#epoch 35 - 300/25 CV log_loss:  0.01725006318878045 - 0.634493462850173\n",
    "#epoch 30 - 300/25 CV log_loss:  0.01731779870747339 - 0.6364895187062414\n",
    "#epoch 40 - 300/25 CV log_loss:  0.017229626356520613 - 0.6367918568899399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_csv1 = train_features[[\"sig_id\"]].merge(train_df[train_targets_scored.columns], on='sig_id', how='inner')\n",
    "sub_mdl1 = sample_submission.drop(columns=targets_scored).merge(test_df[train_targets_scored.columns], on='sig_id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios= [.65,.35]\n",
    "\n",
    "train = oof_csv1.copy()\n",
    "train[targets_scored] =  ratios[0]*oof_csv1[targets_scored] \\\n",
    "                            + ratios[1]*oof_csv3[targets_scored]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV log_loss:  0.01709943203617292\n"
    }
   ],
   "source": [
    "y_true = train_targets_scored.iloc[:,1:].values\n",
    "y_pred = train[targets_scored].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(targets_scored)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / len(targets_scored)\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6413380854538896\n"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quantile transformer\n",
    "#.65,.35 - CV log_loss:  0.01709354636507837 - 0.6376276808970782\n",
    "\n",
    "\n",
    "#Gauss rank\n",
    "#.65,.35 - CV log_loss:  0.017053751545143662 - 0.6390575627851145\n",
    "\n",
    "#limited stats - CV log_loss:  0.017065342460943815 - 0.638098807190973\n",
    "\n",
    "#pca 600 / 50 - CV log_loss:  0.017103519516101885 - 0.6386087888359083\n",
    "#pca 300 / 25 - CV log_loss:  0.017091408859421175 - 0.6398402257073265\n",
    "#pca 300 / 25 (mdl1 40epoch ) - CV log_loss:  0.01709943203617292 - 0.6413380854538896"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sub_mdl1.copy()\n",
    "sub[targets_scored]  =  ratios[0]*sub_mdl1[targets_scored] \\\n",
    "                            + ratios[1]*sub_mdl3[targets_scored]\n",
    "\n",
    "\n",
    "sub = sample_submission[[\"sig_id\"]].merge(sub, on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}