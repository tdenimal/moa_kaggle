{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1604785665205",
   "display_name": "Python 3.7.9 64-bit ('moa_kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# class FaissKMeans:\n",
    "#     def __init__(self, n_clusters=8, n_init=10, max_iter=300,seed=42):\n",
    "#         self.n_clusters = n_clusters\n",
    "#         self.n_init = n_init\n",
    "#         self.max_iter = max_iter\n",
    "#         self.kmeans = None\n",
    "#         self.cluster_centers_ = None\n",
    "#         self.inertia_ = None\n",
    "#         self.seed = seed\n",
    "\n",
    "#     def fit(self, X):\n",
    "#         self.kmeans = faiss.Kmeans(d=X.shape[1],\n",
    "#                                    k=self.n_clusters,\n",
    "#                                    niter=self.max_iter,\n",
    "#                                    nredo=self.n_init,\n",
    "#                                    seed=self.seed)\n",
    "\n",
    "#         self.kmeans.train(X.astype(np.float32))\n",
    "#         self.cluster_centers_ = self.kmeans.centroids\n",
    "#         self.inertia_ = self.kmeans.obj[-1]\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         return self.kmeans.index.search(X.astype(np.float32), 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['train_targets_scored.csv',\n 'sample_submission.csv',\n '.gitkeep',\n 'train_drug.csv',\n 'train_features.csv',\n 'test_features.csv',\n 'train_targets_nonscored.csv']"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "\n",
    "data_dir = '../data/01_raw'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "no_ctl = True\n",
    "ncompo_genes = 600\n",
    "ncompo_cells = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(data_dir+'/train_features.csv')\n",
    "train_targets_scored = pd.read_csv(data_dir+'/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(data_dir+'/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv(data_dir+'/test_features.csv')\n",
    "sample_submission = pd.read_csv(data_dir+'/sample_submission.csv')\n",
    "drug = pd.read_csv(data_dir+'/train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_scored = train_targets_scored.columns[1:]\n",
    "scored = train_targets_scored.merge(drug, on='sig_id', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "not_ctl\n"
    }
   ],
   "source": [
    "if no_ctl:\n",
    "    # cp_type == ctl_vehicle\n",
    "    print(\"not_ctl\")\n",
    "    train_features = train_features[train_features[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
    "    test_features = test_features[test_features[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
    "    train_targets_scored = train_targets_scored.iloc[train_features.index]\n",
    "    train_targets_nonscored = train_targets_nonscored.iloc[train_features.index]\n",
    "    train_features.reset_index(drop = True, inplace = True)\n",
    "    train_features.reset_index(drop = True, inplace = True)\n",
    "    train_targets_scored.reset_index(drop = True, inplace = True)\n",
    "    train_targets_nonscored.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Indiquer si valeur dans le range max, min\n",
    "\n",
    "# import seaborn as sns\n",
    "# data = pd.concat([train_features,test_features],axis=0)\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"ctl_vehicle\"][\"c-4\"],label=\"normal\")\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"trt_cp\"][\"c-4\"],label=\"treated\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets = train_targets_scored[[c for c in train_targets_scored.columns if (c != \"sig_id\")]].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_inhibitor\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_activator\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_agonist\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_antagonist\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# train_features[GENES].apply(lambda x : stats.moment(x,moment=5),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RankGauss\n",
    "\n",
    "for col in (GENES + CELLS):\n",
    "    transformer = QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n",
    "    vec_len = len(train_features[col].values)\n",
    "    vec_len_test = len(test_features[col].values)\n",
    "    raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENES\n",
    "n_comp = ncompo_genes \n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELLS\n",
    "n_comp = ncompo_cells\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(21948, 1526)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = GENES\n",
    "    features_c = CELLS\n",
    "    \n",
    "    for df in train, test:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1) ## <==\n",
    "        # df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1) ## <==\n",
    "        # df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        #df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        # df['g_q25'] = df[features_g].quantile(q=.25,axis = 1)\n",
    "        # df['g_q50'] = df[features_g].quantile(q=.5,axis = 1)\n",
    "        # df['g_q75'] = df[features_g].quantile(q=.75,axis = 1)\n",
    "        #df['g_var'] = df[features_g].apply(axis=1,func=stats.variation)\n",
    "        # df['g_mad'] = df[features_g].mad(axis = 1)\n",
    "\n",
    "\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1) ## <==\n",
    "        # df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1) ## <==\n",
    "        # df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        #df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        # df['c_q25'] = df[features_c].quantile(q=.25,axis = 1)\n",
    "        # df['c_q50'] = df[features_c].quantile(q=.5,axis = 1)\n",
    "        # df['c_q75'] = df[features_c].quantile(q=.75,axis = 1)\n",
    "        # df['c_var'] = df[features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['c_mad'] = df[features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1) ## <==\n",
    "        # df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        # df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        # df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        # df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        # df['gc_q25'] = df[features_g + features_c].quantile(q=.25,axis = 1)\n",
    "        # df['gc_q50'] = df[features_g + features_c].quantile(q=.5,axis = 1)\n",
    "        # df['gc_q75'] = df[features_g + features_c].quantile(q=.75,axis = 1)\n",
    "        # df['gc_var'] = df[features_g + features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['gc_mad'] = df[features_g + features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "train_features,test_features=fe_stats(train_features,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features_gc = train_features[[\"sig_id\"]+GENES+CELLS].copy()\n",
    "# test_features_gc = test_features[[\"sig_id\"]+GENES+CELLS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(21948, 1047)"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "var_thresh = VarianceThreshold(0.8)  #<-- Update\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# def fe_cluster2(train, test, n_clusters = 3, SEED = 42):\n",
    "    \n",
    "\n",
    "#     def create_cluster(train, test, n_clusters = n_clusters):\n",
    "#         train_ = train.copy()\n",
    "#         test_ = test.copy()\n",
    "#         data = pd.concat([train_, test_], axis = 0)\n",
    "#         kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data[[c for c in data.columns if c not in [\"sig_id\",\"cp_type\",\"cp_dose\",\"cp_time\"]]])\n",
    "#         train['cluster'] = kmeans.labels_[:train.shape[0]]\n",
    "#         test['cluster'] = kmeans.labels_[train.shape[0]:]\n",
    "#         train = pd.get_dummies(train, columns = ['cluster'])\n",
    "#         test = pd.get_dummies(test, columns = ['cluster'])\n",
    "#         return train, test\n",
    "    \n",
    "#     train, test = create_cluster(train, test, n_clusters = n_clusters)\n",
    "#     return train, test\n",
    "\n",
    "\n",
    "\n",
    "# train_features,test_features=fe_cluster2(train_features,test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([train1, test1], axis = 0)\n",
    "\n",
    "# distortion = []\n",
    "# for k in range(1,10):\n",
    "#     kmeans = KMeans(n_clusters = k, random_state = 42).fit(data)\n",
    "#     distortion += [kmeans.inertia_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(1,10),distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = train_features.merge(train_features_gc.loc[:,[col for col in train_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")\n",
    "# test_features = test_features.merge(test_features_gc.loc[:,[col for col in test_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_targets_nonscored, on='sig_id')\n",
    "# train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "# test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target_scored = train[train_targets_scored.columns]\n",
    "target_nscored = train[train_targets_nonscored.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test_features.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scored_cols = target_scored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "target_nscored_cols = target_nscored.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds = train.copy()\n",
    "\n",
    "# mskf = MultilabelStratifiedKFold(n_splits=7)\n",
    "\n",
    "# for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target_scored)):\n",
    "#     folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "# folds['kfold'] = folds['kfold'].astype(int)\n",
    "# folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset:\n",
    "    def __init__(self, features, targets_scored,targets_nscored):\n",
    "        self.features = features\n",
    "        self.targets_scored = targets_scored\n",
    "        self.targets_nscored = targets_nscored\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y_scored' : torch.tensor(self.targets_scored[idx, :], dtype=torch.float),\n",
    "            'y_nscored' : torch.tensor(self.targets_nscored[idx, :], dtype=torch.float)           \n",
    "        }\n",
    "        return dct\n",
    "\n",
    "class ValidDataset:\n",
    "    def __init__(self, features, targets_scored):\n",
    "        self.features = features\n",
    "        self.targets_scored = targets_scored\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y_scored' : torch.tensor(self.targets_scored[idx, :], dtype=torch.float),          \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets1, targets2 = data['x'].to(device), data['y_scored'].to(device), data['y_nscored'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs1,outputs2 = model(inputs)\n",
    "        loss1 = loss_fn(outputs1, targets1)\n",
    "        loss2 = loss_fn(outputs2, targets2)\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y_scored'].to(device)\n",
    "        outputs,_ = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs,_ = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, \n",
    "                 num_targets_scored,\n",
    "                 num_targets_nscored, \n",
    "                 hidden_sizes,\n",
    "                 dropout_rates):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_sizes[0]))\n",
    "        self.activation1 = torch.nn.PReLU(num_parameters = hidden_sizes[0], init = 1.0)\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_sizes[0])\n",
    "        self.dropout2 = nn.Dropout(dropout_rates[0])\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_sizes[0], hidden_sizes[1]))\n",
    "        self.activation2 = torch.nn.PReLU(num_parameters = hidden_sizes[1], init = 1.0)\n",
    "\n",
    "\n",
    "        self.batch_norm2b = nn.BatchNorm1d(hidden_sizes[1])\n",
    "        self.dropout2b = nn.Dropout(dropout_rates[1])\n",
    "        self.dense2b = nn.utils.weight_norm(nn.Linear(hidden_sizes[1], hidden_sizes[2]))\n",
    "        self.activation2b = torch.nn.PReLU(num_parameters = hidden_sizes[2], init = 1.0)\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_sizes[2])\n",
    "        self.dropout3 = nn.Dropout(dropout_rates[2])\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_sizes[2], num_targets_scored))\n",
    "        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_sizes[2], num_targets_nscored))\n",
    "\n",
    "    def init_bias(self,pos_scored_rate,pos_nscored_rate):\n",
    "        self.dense3.bias.data = nn.Parameter(torch.tensor(pos_scored_rate, dtype=torch.float))\n",
    "        self.dense4.bias.data = nn.Parameter(torch.tensor(pos_nscored_rate, dtype=torch.float))\n",
    "    \n",
    "    def recalibrate_layer(self, layer):\n",
    "        if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "            print ('recalibrate layer.weight_v')\n",
    "            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "        if(torch.isnan(layer.weight).sum() > 0):\n",
    "            print ('recalibrate layer.weight')\n",
    "            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "            layer.weight += 1e-7\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        self.recalibrate_layer(self.dense1)\n",
    "        x = self.activation1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        self.recalibrate_layer(self.dense2)\n",
    "        x = self.activation2(self.dense2(x))\n",
    "\n",
    "        x = self.batch_norm2b(x)\n",
    "        x = self.dropout2b(x)\n",
    "        self.recalibrate_layer(self.dense2b)\n",
    "        x = self.activation2b(self.dense2b(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        self.recalibrate_layer(self.dense3)\n",
    "        x1 = self.dense3(x)\n",
    "\n",
    "        self.recalibrate_layer(self.dense4)\n",
    "        x2 = self.dense4(x)\n",
    "        return x1,x2\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 35\n",
    "#EPOCHS = 300 #200\n",
    "PATIENCE=40\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7           \n",
    "EARLY_STOPPING_STEPS = PATIENCE+5\n",
    "EARLY_STOP = False\n",
    "\n",
    "#hidden_size=1500\n",
    "hidden_sizes = [1500,1200,1000,1000]\n",
    "dropout_rates = [0.25,0.2,0.2]\n",
    "#dropout_rate = 0.2619422201258426\n",
    "#dropout_rate = 0.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCATE DRUGS\n",
    "vc = scored.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc<=18].index.sort_values()\n",
    "vc2 = vc.loc[vc>18].index.sort_values()\n",
    "\n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}; dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = scored.groupby('drug_id')[targets_scored].mean().loc[vc1]\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_scored])):\n",
    "    dd = {k:fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "\n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_scored])):\n",
    "    dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "# ASSIGN FOLDS\n",
    "folds = train.merge(drug,on=\"sig_id\")\n",
    "folds['fold'] = folds.drug_id.map(dct1)\n",
    "folds.loc[folds.fold.isna(),'fold'] =\\\n",
    "    folds.loc[folds.fold.isna(),'sig_id'].map(dct2)\n",
    "folds.fold = folds.fold.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1048"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "feature_cols = [c for c in process_data(train).columns if c not in (target_scored_cols + target_nscored_cols)]\n",
    "feature_cols = [c for c in feature_cols if c not in ['fold','sig_id']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=len(feature_cols)\n",
    "num_targets_scored=len(target_scored_cols)\n",
    "num_targets_nscored=len(target_nscored_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(20228, 1654)\n(20228, 1656)\n(3624, 1046)\n(20228, 207)\n(20228, 403)\n(3982, 207)\n"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target_scored.shape)\n",
    "print(target_nscored.shape)\n",
    "print(sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "\n",
    "    \n",
    "    trn_idx = train[train['fold'] != fold].index\n",
    "    val_idx = train[train['fold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['fold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    x_train, y_scored_train, y_nscored_train  = train_df[feature_cols].values, train_df[target_scored_cols].values, train_df[target_nscored_cols].values\n",
    "    x_valid, y_scored_valid =  valid_df[feature_cols].values, valid_df[target_scored_cols].values\n",
    "    \n",
    "    train_dataset = TrainDataset(x_train, y_scored_train, y_nscored_train)\n",
    "    valid_dataset = ValidDataset(x_valid, y_scored_valid)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets_scored=num_targets_scored,\n",
    "        num_targets_nscored=num_targets_nscored,\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates,\n",
    "    )\n",
    "\n",
    "    # model.init_bias(pos_scored_rate,pos_nscored_rate)\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,pct_start=0.1, div_factor=1e4, final_div_factor=1e5,\n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "\n",
    "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode = \"min\", patience = PATIENCE, min_lr = 1e-6, factor = 0.9)\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "   \n",
    "    oof = np.zeros((len(train), target_scored.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "        #scheduler.step(valid_loss)\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets_scored=num_targets_scored,\n",
    "        num_targets_nscored=num_targets_nscored,\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target_scored.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_scored_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_scored_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "D: 0, EPOCH: 17, train_loss: 0.028995395911967054, valid_loss: 0.017842996055665222\nFOLD: 0, EPOCH: 18, train_loss: 0.02886849155594759, valid_loss: 0.018026729802722515\nFOLD: 0, EPOCH: 19, train_loss: 0.02883591316640377, valid_loss: 0.017803203233558197\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 20, train_loss: 0.028644349604078075, valid_loss: 0.017767310061532517\nFOLD: 0, EPOCH: 21, train_loss: 0.0283398993231137, valid_loss: 0.017640707690430725\nFOLD: 0, EPOCH: 22, train_loss: 0.028172820559976733, valid_loss: 0.01745149760466555\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 23, train_loss: 0.028089126161135295, valid_loss: 0.0174835294323123\nFOLD: 0, EPOCH: 24, train_loss: 0.0278020310900448, valid_loss: 0.017445142016462658\nFOLD: 0, EPOCH: 25, train_loss: 0.027687723940128788, valid_loss: 0.01729076201825038\nFOLD: 0, EPOCH: 26, train_loss: 0.027528991265332, valid_loss: 0.017261613281848637\nFOLD: 0, EPOCH: 27, train_loss: 0.02741170280119952, valid_loss: 0.017369363861887352\nFOLD: 0, EPOCH: 28, train_loss: 0.027237060863305542, valid_loss: 0.017258228936597057\nFOLD: 0, EPOCH: 29, train_loss: 0.027084680411088115, valid_loss: 0.017148361136407955\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 30, train_loss: 0.026922260561738822, valid_loss: 0.017198973537787147\nFOLD: 0, EPOCH: 31, train_loss: 0.02676243585643961, valid_loss: 0.01716503719596759\nFOLD: 0, EPOCH: 32, train_loss: 0.02667520204832887, valid_loss: 0.017109463882187138\nFOLD: 0, EPOCH: 33, train_loss: 0.026617250295684618, valid_loss: 0.017093880506961243\nFOLD: 0, EPOCH: 34, train_loss: 0.026552698807790875, valid_loss: 0.017131437507012615\nFOLD: 1, EPOCH: 0, train_loss: 1.2029259304205577, valid_loss: 0.09942188734809558\nFOLD: 1, EPOCH: 1, train_loss: 0.05380949353178342, valid_loss: 0.02163337334059179\nFOLD: 1, EPOCH: 2, train_loss: 0.034829474019783514, valid_loss: 0.023802617099136114\nFOLD: 1, EPOCH: 3, train_loss: 0.032019755520202496, valid_loss: 0.019237899454310536\nFOLD: 1, EPOCH: 4, train_loss: 0.030663731459666183, valid_loss: 0.018880598479881883\nFOLD: 1, EPOCH: 5, train_loss: 0.02995634170042144, valid_loss: 0.01836153290544947\nFOLD: 1, EPOCH: 6, train_loss: 0.029544085098637474, valid_loss: 0.01821331784594804\nFOLD: 1, EPOCH: 7, train_loss: 0.029314796695554698, valid_loss: 0.018351153509380918\nFOLD: 1, EPOCH: 8, train_loss: 0.029253772287457076, valid_loss: 0.018314075268184144\nFOLD: 1, EPOCH: 9, train_loss: 0.02910297188769888, valid_loss: 0.0183197579268987\nFOLD: 1, EPOCH: 10, train_loss: 0.02911944652873057, valid_loss: 0.018402091576717794\nFOLD: 1, EPOCH: 11, train_loss: 0.029017338142902763, valid_loss: 0.01828228492134561\nFOLD: 1, EPOCH: 12, train_loss: 0.029036145243379805, valid_loss: 0.018397005507722497\nFOLD: 1, EPOCH: 13, train_loss: 0.029027552212829943, valid_loss: 0.01814186933916062\nFOLD: 1, EPOCH: 14, train_loss: 0.028970035551874725, valid_loss: 0.018194318593790133\nFOLD: 1, EPOCH: 15, train_loss: 0.02900508283464997, valid_loss: 0.018185595554920535\nFOLD: 1, EPOCH: 16, train_loss: 0.02890528861295294, valid_loss: 0.01834023619691531\nFOLD: 1, EPOCH: 17, train_loss: 0.028834088436431354, valid_loss: 0.018173791041287284\nFOLD: 1, EPOCH: 18, train_loss: 0.028772313672083397, valid_loss: 0.01810916963343819\nFOLD: 1, EPOCH: 19, train_loss: 0.028721108439343945, valid_loss: 0.018121829605661333\nFOLD: 1, EPOCH: 20, train_loss: 0.028546470707213438, valid_loss: 0.0180473190266639\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 21, train_loss: 0.02829123834768931, valid_loss: 0.01778788084629923\nFOLD: 1, EPOCH: 22, train_loss: 0.028085329245637964, valid_loss: 0.017633108849016327\nFOLD: 1, EPOCH: 23, train_loss: 0.02797052424263071, valid_loss: 0.017650812941913802\nFOLD: 1, EPOCH: 24, train_loss: 0.02780263334236763, valid_loss: 0.017757569284488756\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 25, train_loss: 0.027646893897542248, valid_loss: 0.017580034017252427\nFOLD: 1, EPOCH: 26, train_loss: 0.02740302647429484, valid_loss: 0.017506555227252345\nFOLD: 1, EPOCH: 27, train_loss: 0.027203511846838176, valid_loss: 0.017386365798301995\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 28, train_loss: 0.0269848320633173, valid_loss: 0.01739594708972921\nFOLD: 1, EPOCH: 29, train_loss: 0.026846495726042325, valid_loss: 0.017397306898298364\nFOLD: 1, EPOCH: 30, train_loss: 0.02674545234552136, valid_loss: 0.017343552317470312\nFOLD: 1, EPOCH: 31, train_loss: 0.026610784522361226, valid_loss: 0.017315126683873434\nFOLD: 1, EPOCH: 32, train_loss: 0.026502172331567164, valid_loss: 0.017324504209682345\nFOLD: 1, EPOCH: 33, train_loss: 0.026457567485394302, valid_loss: 0.01732302034118523\nFOLD: 1, EPOCH: 34, train_loss: 0.026415296988906684, valid_loss: 0.01731487875804305\nFOLD: 2, EPOCH: 0, train_loss: 1.203696889035842, valid_loss: 0.09889374219852945\nFOLD: 2, EPOCH: 1, train_loss: 0.0524578124339528, valid_loss: 0.022486430066435234\nFOLD: 2, EPOCH: 2, train_loss: 0.03393877379815368, valid_loss: 0.02159361978587897\nFOLD: 2, EPOCH: 3, train_loss: 0.031430367051678544, valid_loss: 0.020743814985389294\nFOLD: 2, EPOCH: 4, train_loss: 0.030090910163434112, valid_loss: 0.01946842111647129\nFOLD: 2, EPOCH: 5, train_loss: 0.0295125345891232, valid_loss: 0.01896669756135215\nFOLD: 2, EPOCH: 6, train_loss: 0.029407868928769055, valid_loss: 0.019140850185700085\nFOLD: 2, EPOCH: 7, train_loss: 0.029264311552705133, valid_loss: 0.01936026209074518\nFOLD: 2, EPOCH: 8, train_loss: 0.02920419744708959, valid_loss: 0.018960044921740242\nFOLD: 2, EPOCH: 9, train_loss: 0.029156934702768922, valid_loss: 0.01911262933002866\nFOLD: 2, EPOCH: 10, train_loss: 0.029075190767317134, valid_loss: 0.01865523462386235\nFOLD: 2, EPOCH: 11, train_loss: 0.028917251688921276, valid_loss: 0.018932458823141846\nFOLD: 2, EPOCH: 12, train_loss: 0.028923278770354742, valid_loss: 0.01890884396498618\nFOLD: 2, EPOCH: 13, train_loss: 0.028986536199226975, valid_loss: 0.01887441305038722\nFOLD: 2, EPOCH: 14, train_loss: 0.02893019762054524, valid_loss: 0.01908959367352983\nFOLD: 2, EPOCH: 15, train_loss: 0.028914616867790326, valid_loss: 0.018872903902893482\nFOLD: 2, EPOCH: 16, train_loss: 0.02885380242129459, valid_loss: 0.018760767805835476\nFOLD: 2, EPOCH: 17, train_loss: 0.028842999065733132, valid_loss: 0.01863108551048714\nFOLD: 2, EPOCH: 18, train_loss: 0.028793773161904776, valid_loss: 0.018745339920987255\nFOLD: 2, EPOCH: 19, train_loss: 0.028620841433568037, valid_loss: 0.018453721159502216\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 20, train_loss: 0.028410419205422786, valid_loss: 0.01846385892966519\nFOLD: 2, EPOCH: 21, train_loss: 0.028250462608411908, valid_loss: 0.01859091643405997\nFOLD: 2, EPOCH: 22, train_loss: 0.028014450087485945, valid_loss: 0.018285964413181595\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 23, train_loss: 0.027900654151487875, valid_loss: 0.01839313004165888\nFOLD: 2, EPOCH: 24, train_loss: 0.027707363807541484, valid_loss: 0.018311925677825577\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 25, train_loss: 0.027541108237689033, valid_loss: 0.018125991134539894\nFOLD: 2, EPOCH: 26, train_loss: 0.02731982129625976, valid_loss: 0.01818023128030093\nFOLD: 2, EPOCH: 27, train_loss: 0.02716946877155672, valid_loss: 0.0181008685461205\nFOLD: 2, EPOCH: 28, train_loss: 0.027000523679067984, valid_loss: 0.018087797271816627\nFOLD: 2, EPOCH: 29, train_loss: 0.026936919113401982, valid_loss: 0.01809220064593398\nFOLD: 2, EPOCH: 30, train_loss: 0.02685476520427448, valid_loss: 0.01808608914523021\nFOLD: 2, EPOCH: 31, train_loss: 0.026723544633782962, valid_loss: 0.01802176152072523\nFOLD: 2, EPOCH: 32, train_loss: 0.026601692360332784, valid_loss: 0.018039930490371975\nFOLD: 2, EPOCH: 33, train_loss: 0.026555838296189904, valid_loss: 0.01803504137079353\nFOLD: 2, EPOCH: 34, train_loss: 0.026504598119679618, valid_loss: 0.018034002338738545\nFOLD: 3, EPOCH: 0, train_loss: 1.2001537718974493, valid_loss: 0.09850062008785165\nFOLD: 3, EPOCH: 1, train_loss: 0.05250807429718621, valid_loss: 0.021667325869202614\nFOLD: 3, EPOCH: 2, train_loss: 0.033134428304894006, valid_loss: 0.022368270741856617\nFOLD: 3, EPOCH: 3, train_loss: 0.031324851696434265, valid_loss: 0.01910135700650837\nFOLD: 3, EPOCH: 4, train_loss: 0.030051838195718387, valid_loss: 0.01834508728073991\nFOLD: 3, EPOCH: 5, train_loss: 0.029810328639167195, valid_loss: 0.01878489545829918\nFOLD: 3, EPOCH: 6, train_loss: 0.029535509317236787, valid_loss: 0.018167844852027687\nFOLD: 3, EPOCH: 7, train_loss: 0.029340717160855147, valid_loss: 0.01830743803926136\nFOLD: 3, EPOCH: 8, train_loss: 0.029296629459542388, valid_loss: 0.018212059188796127\nFOLD: 3, EPOCH: 9, train_loss: 0.0291821784202886, valid_loss: 0.017938528695832127\nFOLD: 3, EPOCH: 10, train_loss: 0.029166861601612148, valid_loss: 0.01839169503554054\nFOLD: 3, EPOCH: 11, train_loss: 0.029149075970053673, valid_loss: 0.01816791806208051\nFOLD: 3, EPOCH: 12, train_loss: 0.029181751285624856, valid_loss: 0.018465164400961086\nFOLD: 3, EPOCH: 13, train_loss: 0.029133002451785347, valid_loss: 0.01824202842038611\nFOLD: 3, EPOCH: 14, train_loss: 0.029146422207465068, valid_loss: 0.018637365702053776\nFOLD: 3, EPOCH: 15, train_loss: 0.029062206418636966, valid_loss: 0.018251550181404404\nFOLD: 3, EPOCH: 16, train_loss: 0.029007625081302488, valid_loss: 0.0183103090233129\nFOLD: 3, EPOCH: 17, train_loss: 0.02891031377400984, valid_loss: 0.017949508988986843\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 18, train_loss: 0.028870953972834873, valid_loss: 0.017526348850325398\nFOLD: 3, EPOCH: 19, train_loss: 0.028514602029805675, valid_loss: 0.01763297180118768\nFOLD: 3, EPOCH: 20, train_loss: 0.028431059190017337, valid_loss: 0.017422572264204853\nFOLD: 3, EPOCH: 21, train_loss: 0.02841271075200947, valid_loss: 0.017564520887706592\nFOLD: 3, EPOCH: 22, train_loss: 0.02819731511066065, valid_loss: 0.01743664450781501\nFOLD: 3, EPOCH: 23, train_loss: 0.02805012242649408, valid_loss: 0.017409511760849018\nFOLD: 3, EPOCH: 24, train_loss: 0.027943902676377225, valid_loss: 0.01733408718491378\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 25, train_loss: 0.027723393567344722, valid_loss: 0.017158177640774975\nFOLD: 3, EPOCH: 26, train_loss: 0.027553351198816124, valid_loss: 0.0172407993155977\nFOLD: 3, EPOCH: 27, train_loss: 0.027405411251546705, valid_loss: 0.017199334121592667\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 28, train_loss: 0.027222523164442358, valid_loss: 0.017133591615635414\nFOLD: 3, EPOCH: 29, train_loss: 0.0270564417793032, valid_loss: 0.017044665899289692\nFOLD: 3, EPOCH: 30, train_loss: 0.026940896956469205, valid_loss: 0.017043743282556534\nFOLD: 3, EPOCH: 31, train_loss: 0.026808678145136905, valid_loss: 0.01704447453274675\nFOLD: 3, EPOCH: 32, train_loss: 0.026729514206047442, valid_loss: 0.017001490311130234\nFOLD: 3, EPOCH: 33, train_loss: 0.026634307920604068, valid_loss: 0.01702574871318496\nFOLD: 3, EPOCH: 34, train_loss: 0.026583077303846094, valid_loss: 0.01702574672906295\nFOLD: 4, EPOCH: 0, train_loss: 1.19978777231539, valid_loss: 0.09698334271493166\nFOLD: 4, EPOCH: 1, train_loss: 0.05231719340800362, valid_loss: 0.022567516192793846\nFOLD: 4, EPOCH: 2, train_loss: 0.04374105175135329, valid_loss: 0.02106566694767579\nFOLD: 4, EPOCH: 3, train_loss: 0.03251137209179647, valid_loss: 0.020551905076464882\nFOLD: 4, EPOCH: 4, train_loss: 0.031586455181241035, valid_loss: 0.019581687960611736\nFOLD: 4, EPOCH: 5, train_loss: 0.03065517487223534, valid_loss: 0.01895188163642002\nFOLD: 4, EPOCH: 6, train_loss: 0.030115342726383137, valid_loss: 0.018591837147655693\nFOLD: 4, EPOCH: 7, train_loss: 0.029612770879312474, valid_loss: 0.01875343655600496\nFOLD: 4, EPOCH: 8, train_loss: 0.02931387460900142, valid_loss: 0.018299774266779423\nFOLD: 4, EPOCH: 9, train_loss: 0.029049319242510724, valid_loss: 0.018934601512940033\nFOLD: 4, EPOCH: 10, train_loss: 0.028943634345470107, valid_loss: 0.018176823732969555\nFOLD: 4, EPOCH: 11, train_loss: 0.028778764624696446, valid_loss: 0.018000779228042\nFOLD: 4, EPOCH: 12, train_loss: 0.02860223059542477, valid_loss: 0.018125412823713345\nFOLD: 4, EPOCH: 13, train_loss: 0.028609384590869442, valid_loss: 0.01808724588836017\nFOLD: 4, EPOCH: 14, train_loss: 0.0285539174972869, valid_loss: 0.018296981999731583\nFOLD: 4, EPOCH: 15, train_loss: 0.028525570969042534, valid_loss: 0.017922291289205135\nFOLD: 4, EPOCH: 16, train_loss: 0.02846886370988453, valid_loss: 0.017950516477551148\nFOLD: 4, EPOCH: 17, train_loss: 0.028417456374668023, valid_loss: 0.018027324796370838\nFOLD: 4, EPOCH: 18, train_loss: 0.028370960862101877, valid_loss: 0.018028478907502216\nFOLD: 4, EPOCH: 19, train_loss: 0.028277416713535786, valid_loss: 0.017893232931585415\nFOLD: 4, EPOCH: 20, train_loss: 0.028225214686244726, valid_loss: 0.018159052640523598\nFOLD: 4, EPOCH: 21, train_loss: 0.02805582113454447, valid_loss: 0.017789296684381756\nFOLD: 4, EPOCH: 22, train_loss: 0.027997797601582372, valid_loss: 0.018177354586837086\nFOLD: 4, EPOCH: 23, train_loss: 0.027775229598559877, valid_loss: 0.017699859105050564\nFOLD: 4, EPOCH: 24, train_loss: 0.027562570607508805, valid_loss: 0.01752560277995856\nFOLD: 4, EPOCH: 25, train_loss: 0.027385285932241994, valid_loss: 0.017462301756376804\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 26, train_loss: 0.027068572255837565, valid_loss: 0.01749036453016426\nFOLD: 4, EPOCH: 27, train_loss: 0.026562614640330568, valid_loss: 0.017413689552441887\nFOLD: 4, EPOCH: 28, train_loss: 0.026223443119841462, valid_loss: 0.017262752977726253\nFOLD: 4, EPOCH: 29, train_loss: 0.025925131473580703, valid_loss: 0.017224310735321564\nFOLD: 4, EPOCH: 30, train_loss: 0.0256581846761572, valid_loss: 0.0173038053933693\nFOLD: 4, EPOCH: 31, train_loss: 0.025341819423963043, valid_loss: 0.017286643426379433\nFOLD: 4, EPOCH: 32, train_loss: 0.025139006279299363, valid_loss: 0.01728884559934554\nFOLD: 4, EPOCH: 33, train_loss: 0.02495702666997471, valid_loss: 0.017266062412249004\nFOLD: 4, EPOCH: 34, train_loss: 0.02487826962297892, valid_loss: 0.01728125819531472\nFOLD: 5, EPOCH: 0, train_loss: 1.2035850768580156, valid_loss: 0.10079147893449535\nFOLD: 5, EPOCH: 1, train_loss: 0.05238430336227312, valid_loss: 0.022065517857022907\nFOLD: 5, EPOCH: 2, train_loss: 0.03373948766795151, valid_loss: 0.02705515572882217\nFOLD: 5, EPOCH: 3, train_loss: 0.031552256879341954, valid_loss: 0.23570051011831863\nFOLD: 5, EPOCH: 4, train_loss: 0.033712130711030436, valid_loss: 0.024596301109894463\nFOLD: 5, EPOCH: 5, train_loss: 0.030541924444739434, valid_loss: 0.021895752490862556\nFOLD: 5, EPOCH: 6, train_loss: 0.02993558267788852, valid_loss: 0.019968234164559322\nFOLD: 5, EPOCH: 7, train_loss: 0.0295994023737662, valid_loss: 0.01921266187792239\nFOLD: 5, EPOCH: 8, train_loss: 0.02923834796392304, valid_loss: 0.018767559333987858\nFOLD: 5, EPOCH: 9, train_loss: 0.029045716790920672, valid_loss: 0.018561386057864063\nFOLD: 5, EPOCH: 10, train_loss: 0.02888388216824216, valid_loss: 0.018322403988112575\nFOLD: 5, EPOCH: 11, train_loss: 0.028846483513274613, valid_loss: 0.01852446810706802\nFOLD: 5, EPOCH: 12, train_loss: 0.028897152898614022, valid_loss: 0.018790956424630207\nFOLD: 5, EPOCH: 13, train_loss: 0.028843689367503804, valid_loss: 0.018537797195755917\nFOLD: 5, EPOCH: 14, train_loss: 0.028879087171791232, valid_loss: 0.01861164164122032\nFOLD: 5, EPOCH: 15, train_loss: 0.028737587881658005, valid_loss: 0.018363757301931797\nFOLD: 5, EPOCH: 16, train_loss: 0.02885118398048422, valid_loss: 0.01833142741056888\nFOLD: 5, EPOCH: 17, train_loss: 0.02874515891787322, valid_loss: 0.01828187291064988\nFOLD: 5, EPOCH: 18, train_loss: 0.028662443886894512, valid_loss: 0.018395730294287205\nFOLD: 5, EPOCH: 19, train_loss: 0.02853331805261619, valid_loss: 0.018181425357318443\nFOLD: 5, EPOCH: 20, train_loss: 0.028418153510702884, valid_loss: 0.0183938000270206\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 21, train_loss: 0.02825648007530938, valid_loss: 0.018271784419598786\nFOLD: 5, EPOCH: 22, train_loss: 0.027949280160314897, valid_loss: 0.018248469771250435\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 23, train_loss: 0.027702568893266076, valid_loss: 0.017902875926507557\nFOLD: 5, EPOCH: 24, train_loss: 0.027506226057405856, valid_loss: 0.017996004053755947\nFOLD: 5, EPOCH: 25, train_loss: 0.027343862566768247, valid_loss: 0.018004852387568226\nFOLD: 5, EPOCH: 26, train_loss: 0.027138779939645353, valid_loss: 0.01789454378835533\nFOLD: 5, EPOCH: 27, train_loss: 0.02700543630977764, valid_loss: 0.017815669715080574\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 28, train_loss: 0.026881941512007925, valid_loss: 0.017819144439114178\nFOLD: 5, EPOCH: 29, train_loss: 0.026644441230660853, valid_loss: 0.017770886907111042\nFOLD: 5, EPOCH: 30, train_loss: 0.02643851108629914, valid_loss: 0.01775373923389808\nFOLD: 5, EPOCH: 31, train_loss: 0.026458347394295475, valid_loss: 0.01771277011088703\nFOLD: 5, EPOCH: 32, train_loss: 0.02627036687644089, valid_loss: 0.017741668604962204\nFOLD: 5, EPOCH: 33, train_loss: 0.026190610520322535, valid_loss: 0.01769997669464868\nFOLD: 5, EPOCH: 34, train_loss: 0.02610811752760235, valid_loss: 0.017693467114282692\nFOLD: 6, EPOCH: 0, train_loss: 1.200609524241265, valid_loss: 0.09393229691878609\nFOLD: 6, EPOCH: 1, train_loss: 0.05256310464156901, valid_loss: 0.02209882494872031\nFOLD: 6, EPOCH: 2, train_loss: 0.054948687211007756, valid_loss: 0.021452357911545296\nFOLD: 6, EPOCH: 3, train_loss: 0.032625455258633286, valid_loss: 0.020564770892910336\nFOLD: 6, EPOCH: 4, train_loss: 0.031956149717135465, valid_loss: 0.019915302405538765\nFOLD: 6, EPOCH: 5, train_loss: 0.031075622776851934, valid_loss: 0.0190175236000315\nFOLD: 6, EPOCH: 6, train_loss: 0.030264792059931683, valid_loss: 0.018762004035322563\nFOLD: 6, EPOCH: 7, train_loss: 0.029798069010105205, valid_loss: 0.018241968572787617\nFOLD: 6, EPOCH: 8, train_loss: 0.02951937019551063, valid_loss: 0.018005857730041378\nFOLD: 6, EPOCH: 9, train_loss: 0.029151309897904012, valid_loss: 0.017848831884886906\nFOLD: 6, EPOCH: 10, train_loss: 0.029019132338683393, valid_loss: 0.017792514282400192\nFOLD: 6, EPOCH: 11, train_loss: 0.028920448248219842, valid_loss: 0.017940839752554893\nFOLD: 6, EPOCH: 12, train_loss: 0.0287679278784815, valid_loss: 0.018346761069867924\nFOLD: 6, EPOCH: 13, train_loss: 0.028684538871269014, valid_loss: 0.017874591295485912\nFOLD: 6, EPOCH: 14, train_loss: 0.02853632756673238, valid_loss: 0.017768269971660946\nFOLD: 6, EPOCH: 15, train_loss: 0.028516576063874012, valid_loss: 0.01751097465824822\nFOLD: 6, EPOCH: 16, train_loss: 0.028410126441432274, valid_loss: 0.01730139237707076\nFOLD: 6, EPOCH: 17, train_loss: 0.02831224531537908, valid_loss: 0.01746341039467117\nFOLD: 6, EPOCH: 18, train_loss: 0.028265787452897605, valid_loss: 0.01746227924266587\nFOLD: 6, EPOCH: 19, train_loss: 0.0280743380460669, valid_loss: 0.017522883480009827\nFOLD: 6, EPOCH: 20, train_loss: 0.027967370457618553, valid_loss: 0.017341795215464157\nFOLD: 6, EPOCH: 21, train_loss: 0.027864266803269002, valid_loss: 0.017564165486913662\nFOLD: 6, EPOCH: 22, train_loss: 0.02768646228565451, valid_loss: 0.017471207589234993\nFOLD: 6, EPOCH: 23, train_loss: 0.0274923425167799, valid_loss: 0.017371104706240738\nFOLD: 6, EPOCH: 24, train_loss: 0.027235761846360916, valid_loss: 0.017099958763498325\nFOLD: 6, EPOCH: 25, train_loss: 0.02710343268699944, valid_loss: 0.01715015745519296\nFOLD: 6, EPOCH: 26, train_loss: 0.026714668458546784, valid_loss: 0.017259377378808415\nFOLD: 6, EPOCH: 27, train_loss: 0.026376008083496022, valid_loss: 0.017318303656318913\nFOLD: 6, EPOCH: 28, train_loss: 0.025870264110648456, valid_loss: 0.017279320396482944\nFOLD: 6, EPOCH: 29, train_loss: 0.025348817381788704, valid_loss: 0.017113632401046547\nFOLD: 6, EPOCH: 30, train_loss: 0.024583778782364202, valid_loss: 0.017142145934960117\nFOLD: 6, EPOCH: 31, train_loss: 0.0238366040866822, valid_loss: 0.017253780535057835\nFOLD: 6, EPOCH: 32, train_loss: 0.023192926994798815, valid_loss: 0.01717766706386338\nFOLD: 6, EPOCH: 33, train_loss: 0.02271838586174828, valid_loss: 0.01725460532242837\nFOLD: 6, EPOCH: 34, train_loss: 0.02251322500352912, valid_loss: 0.017179939652914585\n"
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "# SEED = [0,1,2,3,4,5,6] #<-- Update\n",
    "SEED = [0]\n",
    "oof = np.zeros((len(train), len(target_scored_cols)))\n",
    "predictions = np.zeros((len(test), len(target_scored_cols)))\n",
    "\n",
    "# mean_scored = np.mean(train[target_scored_cols].values,axis=0)\n",
    "# mean_nscored = np.mean(train[target_nscored_cols].values,axis=0)\n",
    "# pos_scored_rate = np.log(np.where(mean_scored==0, 1e-8, mean_scored))\n",
    "# pos_nscored_rate = np.log(np.where(mean_nscored==0, 1e-8, mean_nscored))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_scored_cols] = oof\n",
    "test[target_scored_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV log_loss:  0.015875732374537842\n"
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_scored_cols).merge(train[['sig_id']+target_scored_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "y_true = train_targets_scored[target_scored_cols].values\n",
    "y_pred = valid_results[target_scored_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_scored_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target_scored.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6599064166567501\n"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline\n",
    "#CV log_loss:  0.015877615364984873 - 0.6541240486632599\n",
    "#without kmeans CV log_loss: 0.015854058954568463 - 0.659868777535432\n",
    "#epochs 35 - CV log_loss:  0.01585343762816286 - 0.6599438843390867\n",
    "#epocjs 40 - CV log_loss:  0.01586852336393351 - 0.6557867961263701\n",
    "#all stats - CV log_loss:  0.015855185527591735 -0.6570807318381957\n",
    "\n",
    "# hidden_sizes = [1500,1200,1100,900]\n",
    "# dropout_rates = [0.25,0.2,0.2]\n",
    "#CV log_loss:  0.015865879211216915 - 0.6559542487539943\n",
    "\n",
    "# hidden_sizes = [1500,1200,1000,900]\n",
    "# dropout_rates = [0.25,0.2,0.2]\n",
    "#CV log_loss:  0.015854724972601784 - 0.652808391941145\n",
    "\n",
    "\n",
    "\n",
    "# hidden_sizes = [1400,1200,1100,900]\n",
    "# dropout_rates = [0.25,0.2,0.2]\n",
    "# CV log_loss:  0.0159006577610937 - 0.6556368091911823\n",
    "\n",
    "# hidden_sizes = [1500,1400,1200,1000]\n",
    "# dropout_rates = [0.2,0.2,0.2]\n",
    "#CV log_loss:  0.015903499471456212 - 0.6545642003005651\n",
    "\n",
    "# hidden_sizes = [1500,1400,1200,1000]\n",
    "# dropout_rates = [0.2,0.2,0.2]\n",
    "# CV log_loss:  0.015902450359650298 - 0.6534999255322688\n",
    "\n",
    "# hidden_sizes = [1500,1400,1200,1000]\n",
    "# dropout_rates = [0.25,0.25,0.25]\n",
    "# CV log_loss:  0.015894421855461457 - 0.6585559568507875\n",
    "\n",
    "# hidden_sizes = [1200,1200,1100,1000]\n",
    "# dropout_rates = [0.2619422201258426,0.2619422201258426,0.2619422201258426]\n",
    "# CV log_loss:  0.015915461833464227 - 0.6551242407337411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auc .8073 -> .6541 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV log_loss:  0.014772181976560779 - hidden size 1500\n",
    "#CV log loss:  0.014761207506656483 - hidden size 1600\n",
    "#CV log_loss:  0.014757089674479135 - hidden size 1600 + recalibrate layers\n",
    "#CV log_loss:  0.014798489345352649 - hidden size 1600 + recalibrate layers + 30epochs\n",
    "#CV log_loss:  0.014993945755985132 - hidden size 1600 + recalibrate layers - batch size 256\n",
    "#CV log_loss:  0.014925429807583115 - hidden size 1600 + recalibrate layers - batch size 256 + 50 epcohs\n",
    "#CV log_loss:  0.01501367781064934  - hidden size 1600 + recalibrate layers - batch size 256 + 50 epcohs div_factor=1e4\n",
    "#CV log_loss:  0.015038120295874759 - hidden size 1600 + recalibrate layers - batch size 256 + 50 epcohs pct_start=0.2\n",
    "#CV log_loss:  0.014925429807583115 - hidden size 1600 + recalibrate layers - batch size 256 + 50 epcohs 2*lr\n",
    "#CV log_loss:  0.014862676189325522 - hidden size 2000 + recalibrate layers - batch size 256 + 50 epcohs\n",
    "#CV log_loss:  0.014805892283163887 - hidden size 2000 + recalibrate layers - batch size 256 + 70 epcohs\n",
    "\n",
    "\n",
    "\n",
    "#CV log_loss:  0.014701033773812383 - hidden size 1000 + recalibrate layers + 30 epochs + 1layer + stats g35 c5\n",
    "#CV log_loss:  0.01470633477903798  - hidden size 1000 + recalibrate layers + 30 epochs + 1layer + stats g15 c5\n",
    "\n",
    "#CV log_loss:  0.014688195780301675 - hidden size 1000 + recalibrate layers + 30 epochs + 1layer + stats + varianceth 0.81\n",
    "\n",
    "#CV log_loss:  0.01473922963978107 - hidden size 1000 + recalibrate layers + 40 epochs + 1layer\n",
    "#CV log_loss:  0.014704245299661241 - hidden size 900 + recalibrate layers + 30 epochs + 1layer\n",
    "#CV log_loss:  0.014700136730168381 - hidden size 1100 + recalibrate layers + 30 epochs + 1layer\n",
    "\n",
    "\n",
    "#CV log_loss:  0.015298968285739336 - hidden size 1600 + recalibrate layers - batch size 64\n",
    "\n",
    "#CV log_loss:  0.014708188641129298 - hidden size 1000 + recalibrate layers + 30 epochs\n",
    "#CV log_loss:  0.014686009232339573 - hidden size 1000 + recalibrate layers + 30 epochs + 1layer\n",
    "#CV log_loss:  0.014675118998674056 - hidden size 1000 + recalibrate layers + 30 epochs + 1layer + stats\n",
    "#\n",
    "\n",
    "#CV log_loss:  0.01468776733769557 without stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CV log_loss:  0.014751931843087755 baseline\n",
    "#CV log_loss:  0.014746491073514958 baseline + std g,c\n",
    "#CV log_loss:  0.014753663287033052 baseline + std g,c + median g,c\n",
    "#CV log_loss:  0.014747499643338137 baseline + std g,c + sum g,c + kurt g,c\n",
    "#CV log_loss:  0.014745229709145196 baseline + std g,c + sum g,c + mean g,c\n",
    "#CV log_loss:  0.014765909985927743 baseline + std g,c + sum g,c + skew g,c\n",
    "#CV log_loss:  0.01475305848080635  baseline + std g,c + sum g,c + g_var\n",
    "#CV log_loss:  0.014738260008810762 baseline + std g,c + sum g,c\n",
    "#CV log_loss:  0.014735805473683174 baseline + std g,c + sum g,c,gc\n",
    "#CV log_loss:  0.014750251194250852 baseline + std g,c,gc + sum g,c,gc\n",
    "#CV log_loss:  0.01474079804921367  baseline + std g,c + sum g,c,gc + variance 80(threshold)\n",
    "\n",
    "#CV log_loss:  0.014704018818104764 baseline + std g,c + sum g,c,gc + variance 80(threshold) &PCA\n",
    "#CV log loss:  0.014669995520015213 baseline + std g,c + sum g,c,gc + variance 80(threshold) &PCA + clusters3 - CV log_loss:  0.014520597500516905 /0.8153730747072218 (seeds)\n",
    "\n",
    "#CV log_loss:  0.014566130099844471/0.815729267497772 avec prelu, 30 epochs\n",
    "\n",
    "\n",
    "#CV log_loss:  0.014685491308574154 baseline + std g,c + sum g,c,gc + variance 80(threshold) &PCA + clusters5\n",
    "\n",
    "\n",
    "#CV log_loss:  0.015099951953285232 baseline + PCA features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}