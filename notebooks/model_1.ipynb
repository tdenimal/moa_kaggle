{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1604053442552",
   "display_name": "Python 3.7.9 64-bit ('moa_kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['train_targets_scored.csv',\n 'sample_submission.csv',\n '.gitkeep',\n 'train_features.csv',\n 'test_features.csv',\n 'train_targets_nonscored.csv']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "data_dir = '../data/01_raw'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(data_dir+'/train_features.csv')\n",
    "train_targets_scored = pd.read_csv(data_dir+'/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(data_dir+'/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv(data_dir+'/test_features.csv')\n",
    "sample_submission = pd.read_csv(data_dir+'/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Indiquer si valeur dans le range max, min\n",
    "\n",
    "# import seaborn as sns\n",
    "# data = pd.concat([train_features,test_features],axis=0)\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"ctl_vehicle\"][\"c-4\"],label=\"normal\")\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"trt_cp\"][\"c-4\"],label=\"treated\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets = train_targets_scored[[c for c in train_targets_scored.columns if (c != \"sig_id\")]].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_inhibitor\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_activator\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_agonist\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_antagonist\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# train_features[GENES].apply(lambda x : stats.moment(x,moment=5),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RankGauss\n",
    "\n",
    "for col in (GENES + CELLS):\n",
    "\n",
    "    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
    "    vec_len = len(train_features[col].values)\n",
    "    vec_len_test = len(test_features[col].values)\n",
    "    raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENES\n",
    "n_comp = 600  #<--Update\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELLS\n",
    "n_comp = 50  #<--Update\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(23814, 1526)"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = GENES\n",
    "    features_c = CELLS\n",
    "    \n",
    "    for df in train, test:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1) ## <==\n",
    "        # df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1) ## <==\n",
    "        # df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        #df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        # df['g_q25'] = df[features_g].quantile(q=.25,axis = 1)\n",
    "        # df['g_q50'] = df[features_g].quantile(q=.5,axis = 1)\n",
    "        # df['g_q75'] = df[features_g].quantile(q=.75,axis = 1)\n",
    "        #df['g_var'] = df[features_g].apply(axis=1,func=stats.variation)\n",
    "        # df['g_mad'] = df[features_g].mad(axis = 1)\n",
    "\n",
    "\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1) ## <==\n",
    "        # df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1) ## <==\n",
    "        # df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        #df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        # df['c_q25'] = df[features_c].quantile(q=.25,axis = 1)\n",
    "        # df['c_q50'] = df[features_c].quantile(q=.5,axis = 1)\n",
    "        # df['c_q75'] = df[features_c].quantile(q=.75,axis = 1)\n",
    "        # df['c_var'] = df[features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['c_mad'] = df[features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1) ## <==\n",
    "        # df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        # df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        # df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        # df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        # df['gc_q25'] = df[features_g + features_c].quantile(q=.25,axis = 1)\n",
    "        # df['gc_q50'] = df[features_g + features_c].quantile(q=.5,axis = 1)\n",
    "        # df['gc_q75'] = df[features_g + features_c].quantile(q=.75,axis = 1)\n",
    "        # df['gc_var'] = df[features_g + features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['gc_mad'] = df[features_g + features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "train_features,test_features=fe_stats(train_features,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features_gc = train_features[[\"sig_id\"]+GENES+CELLS].copy()\n",
    "# test_features_gc = test_features[[\"sig_id\"]+GENES+CELLS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(23814, 1043)"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "var_thresh = VarianceThreshold(0.8)  #<-- Update\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# def fe_cluster(train, test, n_clusters_g = 15, n_clusters_c = 5, SEED = 42):\n",
    "    \n",
    "#     features_g = GENES\n",
    "#     features_c = CELLS\n",
    "    \n",
    "#     def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n",
    "#         train_ = train[features].copy()\n",
    "#         test_ = test[features].copy()\n",
    "#         data = pd.concat([train_, test_], axis = 0)\n",
    "#         kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n",
    "#         train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n",
    "#         test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n",
    "#         train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n",
    "#         test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "#         return train, test\n",
    "    \n",
    "#     #train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n",
    "#     train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n",
    "#     return train, test\n",
    "\n",
    "# train_features_gc ,test_features_gc=fe_cluster(train_features_gc,test_features_gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def fe_cluster2(train, test, n_clusters = 3, SEED = 42):\n",
    "    \n",
    "\n",
    "    def create_cluster(train, test, n_clusters = n_clusters):\n",
    "        train_ = train.copy()\n",
    "        test_ = test.copy()\n",
    "        data = pd.concat([train_, test_], axis = 0)\n",
    "        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data[[c for c in data.columns if c not in [\"sig_id\",\"cp_type\",\"cp_dose\",\"cp_time\"]]])\n",
    "        train['cluster'] = kmeans.labels_[:train.shape[0]]\n",
    "        test['cluster'] = kmeans.labels_[train.shape[0]:]\n",
    "        train = pd.get_dummies(train, columns = ['cluster'])\n",
    "        test = pd.get_dummies(test, columns = ['cluster'])\n",
    "        return train, test\n",
    "    \n",
    "    train, test = create_cluster(train, test, n_clusters = n_clusters)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "\n",
    "train_features,test_features=fe_cluster2(train_features,test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([train1, test1], axis = 0)\n",
    "\n",
    "# distortion = []\n",
    "# for k in range(1,10):\n",
    "#     kmeans = KMeans(n_clusters = k, random_state = 42).fit(data)\n",
    "#     distortion += [kmeans.inertia_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(1,10),distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = train_features.merge(train_features_gc.loc[:,[col for col in train_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")\n",
    "# test_features = test_features.merge(test_features_gc.loc[:,[col for col in test_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(23814, 1046)"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_targets_nonscored, on='sig_id')\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target_scored = train[train_targets_scored.columns]\n",
    "target_nscored = train[train_targets_nonscored.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scored_cols = target_scored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "target_nscored_cols = target_nscored.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             sig_id cp_time cp_dose         0         1         2         3  \\\n0      id_000644bb2      24      D1  1.134849  0.907687 -0.416385 -0.966814   \n1      id_000779bfc      72      D1  0.119282  0.681738  0.272399  0.080113   \n2      id_000a6266a      48      D1  0.779973  0.946463  1.425350 -0.132928   \n3      id_0015fd391      48      D1 -0.734910 -0.274641 -0.438509  0.759097   \n4      id_001626bd3      72      D2 -0.452718 -0.477513  0.972316  0.970731   \n...             ...     ...     ...       ...       ...       ...       ...   \n21943  id_fff8c2444      72      D1  0.237856 -1.228203  0.218376 -0.365976   \n21944  id_fffb1ceed      24      D2  0.209361 -0.022389 -0.235888 -0.796989   \n21945  id_fffb70c0c      24      D2 -1.911021  0.587228 -0.588417  1.296405   \n21946  id_fffcb9e7c      24      D1  0.816407  0.417618  0.431631  0.300617   \n21947  id_ffffdd77b      72      D1 -1.243096  1.567730 -0.269573  1.083636   \n\n              4         5         6  ...  \\\n0     -0.254723 -1.017473 -1.364787  ...   \n1      1.205169  0.686517  0.313396  ...   \n2     -0.006122  1.492493  0.235577  ...   \n3      2.346330 -0.858153 -2.288417  ...   \n4      1.463427 -0.869555 -0.375501  ...   \n...         ...       ...       ...  ...   \n21943 -0.330177  0.569243 -0.150978  ...   \n21944 -0.674009  0.919312  0.735603  ...   \n21945 -1.002640  0.850589 -0.304313  ...   \n21946  1.070346 -0.024189  0.048942  ...   \n21947 -0.511235 -2.099634 -1.622462  ...   \n\n       vesicular_monoamine_transporter_inhibitor  vitamin_k_antagonist  \\\n0                                              0                     0   \n1                                              0                     0   \n2                                              0                     0   \n3                                              0                     0   \n4                                              0                     0   \n...                                          ...                   ...   \n21943                                          0                     0   \n21944                                          0                     0   \n21945                                          0                     0   \n21946                                          0                     0   \n21947                                          0                     0   \n\n       voltage-gated_calcium_channel_ligand  \\\n0                                         0   \n1                                         0   \n2                                         0   \n3                                         0   \n4                                         0   \n...                                     ...   \n21943                                     0   \n21944                                     0   \n21945                                     0   \n21946                                     0   \n21947                                     0   \n\n       voltage-gated_potassium_channel_activator  \\\n0                                              0   \n1                                              0   \n2                                              0   \n3                                              0   \n4                                              0   \n...                                          ...   \n21943                                          0   \n21944                                          0   \n21945                                          0   \n21946                                          0   \n21947                                          0   \n\n       voltage-gated_sodium_channel_blocker  wdr5_mll_interaction_inhibitor  \\\n0                                         0                               0   \n1                                         0                               0   \n2                                         0                               0   \n3                                         0                               0   \n4                                         0                               0   \n...                                     ...                             ...   \n21943                                     0                               0   \n21944                                     0                               0   \n21945                                     0                               0   \n21946                                     0                               0   \n21947                                     0                               0   \n\n       wnt_agonist  xanthine_oxidase_inhibitor  xiap_inhibitor  kfold  \n0                0                           0               0      5  \n1                0                           0               0      0  \n2                0                           0               0      6  \n3                0                           0               0      0  \n4                0                           0               0      4  \n...            ...                         ...             ...    ...  \n21943            0                           0               0      5  \n21944            0                           0               0      1  \n21945            0                           0               0      5  \n21946            0                           0               0      1  \n21947            0                           0               0      6  \n\n[21948 rows x 1654 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>...</th>\n      <th>vesicular_monoamine_transporter_inhibitor</th>\n      <th>vitamin_k_antagonist</th>\n      <th>voltage-gated_calcium_channel_ligand</th>\n      <th>voltage-gated_potassium_channel_activator</th>\n      <th>voltage-gated_sodium_channel_blocker</th>\n      <th>wdr5_mll_interaction_inhibitor</th>\n      <th>wnt_agonist</th>\n      <th>xanthine_oxidase_inhibitor</th>\n      <th>xiap_inhibitor</th>\n      <th>kfold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>1.134849</td>\n      <td>0.907687</td>\n      <td>-0.416385</td>\n      <td>-0.966814</td>\n      <td>-0.254723</td>\n      <td>-1.017473</td>\n      <td>-1.364787</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_000779bfc</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.119282</td>\n      <td>0.681738</td>\n      <td>0.272399</td>\n      <td>0.080113</td>\n      <td>1.205169</td>\n      <td>0.686517</td>\n      <td>0.313396</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_000a6266a</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>0.779973</td>\n      <td>0.946463</td>\n      <td>1.425350</td>\n      <td>-0.132928</td>\n      <td>-0.006122</td>\n      <td>1.492493</td>\n      <td>0.235577</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_0015fd391</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>-0.734910</td>\n      <td>-0.274641</td>\n      <td>-0.438509</td>\n      <td>0.759097</td>\n      <td>2.346330</td>\n      <td>-0.858153</td>\n      <td>-2.288417</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_001626bd3</td>\n      <td>72</td>\n      <td>D2</td>\n      <td>-0.452718</td>\n      <td>-0.477513</td>\n      <td>0.972316</td>\n      <td>0.970731</td>\n      <td>1.463427</td>\n      <td>-0.869555</td>\n      <td>-0.375501</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21943</th>\n      <td>id_fff8c2444</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.237856</td>\n      <td>-1.228203</td>\n      <td>0.218376</td>\n      <td>-0.365976</td>\n      <td>-0.330177</td>\n      <td>0.569243</td>\n      <td>-0.150978</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>21944</th>\n      <td>id_fffb1ceed</td>\n      <td>24</td>\n      <td>D2</td>\n      <td>0.209361</td>\n      <td>-0.022389</td>\n      <td>-0.235888</td>\n      <td>-0.796989</td>\n      <td>-0.674009</td>\n      <td>0.919312</td>\n      <td>0.735603</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21945</th>\n      <td>id_fffb70c0c</td>\n      <td>24</td>\n      <td>D2</td>\n      <td>-1.911021</td>\n      <td>0.587228</td>\n      <td>-0.588417</td>\n      <td>1.296405</td>\n      <td>-1.002640</td>\n      <td>0.850589</td>\n      <td>-0.304313</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>21946</th>\n      <td>id_fffcb9e7c</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>0.816407</td>\n      <td>0.417618</td>\n      <td>0.431631</td>\n      <td>0.300617</td>\n      <td>1.070346</td>\n      <td>-0.024189</td>\n      <td>0.048942</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21947</th>\n      <td>id_ffffdd77b</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>-1.243096</td>\n      <td>1.567730</td>\n      <td>-0.269573</td>\n      <td>1.083636</td>\n      <td>-0.511235</td>\n      <td>-2.099634</td>\n      <td>-1.622462</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>21948 rows × 1654 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "folds = train.copy()\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=7)\n",
    "\n",
    "for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target_scored)):\n",
    "    folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "folds['kfold'] = folds['kfold'].astype(int)\n",
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(21948, 1653)\n(21948, 1654)\n(3624, 1045)\n(21948, 207)\n(21948, 403)\n(3982, 207)\n"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target_scored.shape)\n",
    "print(target_nscored.shape)\n",
    "print(sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset:\n",
    "    def __init__(self, features, targets_scored,targets_nscored):\n",
    "        self.features = features\n",
    "        self.targets_scored = targets_scored\n",
    "        self.targets_nscored = targets_nscored\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y_scored' : torch.tensor(self.targets_scored[idx, :], dtype=torch.float),\n",
    "            'y_nscored' : torch.tensor(self.targets_nscored[idx, :], dtype=torch.float)           \n",
    "        }\n",
    "        return dct\n",
    "\n",
    "class ValidDataset:\n",
    "    def __init__(self, features, targets_scored):\n",
    "        self.features = features\n",
    "        self.targets_scored = targets_scored\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y_scored' : torch.tensor(self.targets_scored[idx, :], dtype=torch.float),          \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets1, targets2 = data['x'].to(device), data['y_scored'].to(device), data['y_nscored'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs1,outputs2 = model(inputs)\n",
    "        loss1 = loss_fn(outputs1, targets1)\n",
    "        loss2 = loss_fn(outputs2, targets2)\n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y_scored'].to(device)\n",
    "        outputs,_ = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs,_ = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets_scored,num_targets_nscored, hidden_size,dropout_rate):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        self.activation1 = torch.nn.PReLU(num_parameters = hidden_size, init = 1.0)\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        self.activation2 = torch.nn.PReLU(num_parameters = hidden_size, init = 1.0)\n",
    "\n",
    "\n",
    "        self.batch_norm2b = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2b = nn.Dropout(dropout_rate)\n",
    "        self.dense2b = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        self.activation2b = torch.nn.PReLU(num_parameters = hidden_size, init = 1.0)\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets_scored))\n",
    "        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets_nscored))\n",
    "\n",
    "    def init_bias(self,pos_scored_rate,pos_nscored_rate):\n",
    "        self.dense3.bias.data = nn.Parameter(torch.tensor(pos_scored_rate, dtype=torch.float))\n",
    "        self.dense4.bias.data = nn.Parameter(torch.tensor(pos_nscored_rate, dtype=torch.float))\n",
    "    \n",
    "    def recalibrate_layer(self, layer):\n",
    "        if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "            print ('recalibrate layer.weight_v')\n",
    "            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "        if(torch.isnan(layer.weight).sum() > 0):\n",
    "            print ('recalibrate layer.weight')\n",
    "            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "            layer.weight += 1e-7\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        self.recalibrate_layer(self.dense1)\n",
    "        x = self.activation1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        self.recalibrate_layer(self.dense2)\n",
    "        x = self.activation2(self.dense2(x))\n",
    "\n",
    "        x = self.batch_norm2b(x)\n",
    "        x = self.dropout2b(x)\n",
    "        self.recalibrate_layer(self.dense2b)\n",
    "        x = self.activation2b(self.dense2b(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        self.recalibrate_layer(self.dense3)\n",
    "        x1 = self.dense3(x)\n",
    "\n",
    "        self.recalibrate_layer(self.dense4)\n",
    "        x2 = self.dense4(x)\n",
    "        return x1,x2\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1047"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "feature_cols = [c for c in process_data(folds).columns if c not in (target_scored_cols + target_nscored_cols)]\n",
    "feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 30\n",
    "#EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7           \n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = False\n",
    "\n",
    "num_features=len(feature_cols)\n",
    "num_targets_scored=len(target_scored_cols)\n",
    "num_targets_nscored=len(target_nscored_cols)\n",
    "#hidden_size=1500\n",
    "hidden_size=1000\n",
    "dropout_rate = 0.2619422201258426\n",
    "#dropout_rate = 0.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    x_train, y_scored_train, y_nscored_train  = train_df[feature_cols].values, train_df[target_scored_cols].values, train_df[target_nscored_cols].values\n",
    "    x_valid, y_scored_valid =  valid_df[feature_cols].values, valid_df[target_scored_cols].values\n",
    "    \n",
    "    train_dataset = TrainDataset(x_train, y_scored_train, y_nscored_train)\n",
    "    valid_dataset = ValidDataset(x_valid, y_scored_valid)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets_scored=num_targets_scored,\n",
    "        num_targets_nscored=num_targets_nscored,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "    )\n",
    "\n",
    "    # model.init_bias(pos_scored_rate,pos_nscored_rate)\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,pct_start=0.1, div_factor=1e4, final_div_factor=1e5,\n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.1, patience=5, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "   \n",
    "    oof = np.zeros((len(train), target_scored.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "        # scheduler.step(valid_loss)\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets_scored=num_targets_scored,\n",
    "        num_targets_nscored=num_targets_nscored,\n",
    "        hidden_size=hidden_size,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target_scored.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_scored_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_scored_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "FOLD: 0, EPOCH: 0, train_loss: 1.0999934744774078, valid_loss: 0.0446367010474205\nFOLD: 0, EPOCH: 1, train_loss: 0.03975195412327643, valid_loss: 0.02129738174378872\nFOLD: 0, EPOCH: 2, train_loss: 0.03331593024943556, valid_loss: 0.025873917043209075\nFOLD: 0, EPOCH: 3, train_loss: 0.03154831250407258, valid_loss: 0.018814123794436456\nFOLD: 0, EPOCH: 4, train_loss: 0.029813154302790863, valid_loss: 0.01817529186606407\nFOLD: 0, EPOCH: 5, train_loss: 0.02986360658086887, valid_loss: 0.017925632409751414\nFOLD: 0, EPOCH: 6, train_loss: 0.029174980422367856, valid_loss: 0.017884132005274295\nFOLD: 0, EPOCH: 7, train_loss: 0.029156252919208436, valid_loss: 0.017616165541112422\nFOLD: 0, EPOCH: 8, train_loss: 0.02911221668073515, valid_loss: 0.017687747105956077\nFOLD: 0, EPOCH: 9, train_loss: 0.029003649899343245, valid_loss: 0.01782228786498308\nFOLD: 0, EPOCH: 10, train_loss: 0.028935192887778995, valid_loss: 0.017730763666331768\nFOLD: 0, EPOCH: 11, train_loss: 0.029004062963079433, valid_loss: 0.01776230029761791\nFOLD: 0, EPOCH: 12, train_loss: 0.02893104855300618, valid_loss: 0.017580098174512386\nFOLD: 0, EPOCH: 13, train_loss: 0.028903998231806723, valid_loss: 0.017409453354775905\nFOLD: 0, EPOCH: 14, train_loss: 0.02877535446718031, valid_loss: 0.01742483299225569\nFOLD: 0, EPOCH: 15, train_loss: 0.02878527798164053, valid_loss: 0.017527722157537937\nFOLD: 0, EPOCH: 16, train_loss: 0.028777075031784928, valid_loss: 0.017310485355556012\nFOLD: 0, EPOCH: 17, train_loss: 0.028568486499340354, valid_loss: 0.01723233122378588\nFOLD: 0, EPOCH: 18, train_loss: 0.02848106874849926, valid_loss: 0.017175575830042363\nFOLD: 0, EPOCH: 19, train_loss: 0.028353183561948693, valid_loss: 0.017059126123785972\nFOLD: 0, EPOCH: 20, train_loss: 0.028180015900609444, valid_loss: 0.01685957979410887\nFOLD: 0, EPOCH: 21, train_loss: 0.02792554401925632, valid_loss: 0.01682967469096184\nFOLD: 0, EPOCH: 22, train_loss: 0.027726196652265632, valid_loss: 0.016636832281947136\nFOLD: 0, EPOCH: 23, train_loss: 0.02741583780113126, valid_loss: 0.016563753075897695\nFOLD: 0, EPOCH: 24, train_loss: 0.027154807153404976, valid_loss: 0.016307566091418268\nFOLD: 0, EPOCH: 25, train_loss: 0.026805427381578758, valid_loss: 0.016176373697817326\nFOLD: 0, EPOCH: 26, train_loss: 0.026454531251877345, valid_loss: 0.016082701832056047\nFOLD: 0, EPOCH: 27, train_loss: 0.026160533356220542, valid_loss: 0.016015379652380944\nFOLD: 0, EPOCH: 28, train_loss: 0.025935148554188863, valid_loss: 0.01597947433590889\nFOLD: 0, EPOCH: 29, train_loss: 0.02579546242090715, valid_loss: 0.0159668630361557\nFOLD: 1, EPOCH: 0, train_loss: 1.0983092741293161, valid_loss: 0.03992478340864181\nFOLD: 1, EPOCH: 1, train_loss: 0.03986461675998305, valid_loss: 0.021482107490301133\nFOLD: 1, EPOCH: 2, train_loss: 0.03286996687806788, valid_loss: 0.019890028685331344\nFOLD: 1, EPOCH: 3, train_loss: 0.031277923458287504, valid_loss: 0.018628587871789934\nFOLD: 1, EPOCH: 4, train_loss: 0.02995026470193652, valid_loss: 0.01765138056129217\nFOLD: 1, EPOCH: 5, train_loss: 0.029472315595263525, valid_loss: 0.01767829529941082\nFOLD: 1, EPOCH: 6, train_loss: 0.029894400502041896, valid_loss: 0.01761926993727684\nFOLD: 1, EPOCH: 7, train_loss: 0.02920137858968608, valid_loss: 0.017588888704776765\nFOLD: 1, EPOCH: 8, train_loss: 0.029144658003838694, valid_loss: 0.017341734282672404\nFOLD: 1, EPOCH: 9, train_loss: 0.02917541848609642, valid_loss: 0.017458738386631013\nFOLD: 1, EPOCH: 10, train_loss: 0.029022724259974195, valid_loss: 0.017338515035808088\nFOLD: 1, EPOCH: 11, train_loss: 0.029057341393362097, valid_loss: 0.017322611957788468\nFOLD: 1, EPOCH: 12, train_loss: 0.02903050333768332, valid_loss: 0.01759181771427393\nFOLD: 1, EPOCH: 13, train_loss: 0.029009353610224463, valid_loss: 0.017267865389585496\nFOLD: 1, EPOCH: 14, train_loss: 0.028923604548687025, valid_loss: 0.017315457463264464\nFOLD: 1, EPOCH: 15, train_loss: 0.028904810927960337, valid_loss: 0.017186883948743344\nFOLD: 1, EPOCH: 16, train_loss: 0.02880320438266206, valid_loss: 0.017121874131262302\nFOLD: 1, EPOCH: 17, train_loss: 0.028718338252938524, valid_loss: 0.017137096486985682\nFOLD: 1, EPOCH: 18, train_loss: 0.028604841100520827, valid_loss: 0.01682288769632578\nFOLD: 1, EPOCH: 19, train_loss: 0.0283742386746366, valid_loss: 0.016889136806130408\nFOLD: 1, EPOCH: 20, train_loss: 0.028245173305982633, valid_loss: 0.016823037527501583\nFOLD: 1, EPOCH: 21, train_loss: 0.028047169184907763, valid_loss: 0.016607959419488907\nFOLD: 1, EPOCH: 22, train_loss: 0.027801755422941683, valid_loss: 0.016386475414037704\nFOLD: 1, EPOCH: 23, train_loss: 0.02752776181667435, valid_loss: 0.01625729750841856\nFOLD: 1, EPOCH: 24, train_loss: 0.027213160569469135, valid_loss: 0.01613001897931099\nFOLD: 1, EPOCH: 25, train_loss: 0.026953876867484884, valid_loss: 0.01598228417336941\nFOLD: 1, EPOCH: 26, train_loss: 0.026538140045440927, valid_loss: 0.015823293067514895\nFOLD: 1, EPOCH: 27, train_loss: 0.026217870452270215, valid_loss: 0.015783739238977433\nFOLD: 1, EPOCH: 28, train_loss: 0.025939507219864398, valid_loss: 0.015757675915956497\nFOLD: 1, EPOCH: 29, train_loss: 0.02584169272865568, valid_loss: 0.015731957703828812\nFOLD: 2, EPOCH: 0, train_loss: 1.0956147455033802, valid_loss: 0.04221134379506111\nFOLD: 2, EPOCH: 1, train_loss: 0.03997330095137463, valid_loss: 0.02148998886346817\nFOLD: 2, EPOCH: 2, train_loss: 0.03558886644183373, valid_loss: 0.021701664254069327\nFOLD: 2, EPOCH: 3, train_loss: 0.032183620090387305, valid_loss: 0.019473502412438393\nFOLD: 2, EPOCH: 4, train_loss: 0.030832360200837357, valid_loss: 0.0183740558847785\nFOLD: 2, EPOCH: 5, train_loss: 0.02995131952928848, valid_loss: 0.01850929394364357\nFOLD: 2, EPOCH: 6, train_loss: 0.029421452483555086, valid_loss: 0.017872065007686615\nFOLD: 2, EPOCH: 7, train_loss: 0.029206438408214217, valid_loss: 0.01757300812751055\nFOLD: 2, EPOCH: 8, train_loss: 0.029076825573939046, valid_loss: 0.017748411297798156\nFOLD: 2, EPOCH: 9, train_loss: 0.02898741732699936, valid_loss: 0.017777757085859776\nFOLD: 2, EPOCH: 10, train_loss: 0.028943464045926015, valid_loss: 0.017471327260136605\nFOLD: 2, EPOCH: 11, train_loss: 0.028931421690246685, valid_loss: 0.01755682084709406\nFOLD: 2, EPOCH: 12, train_loss: 0.028946394797693304, valid_loss: 0.017582592181861402\nFOLD: 2, EPOCH: 13, train_loss: 0.02884908565351752, valid_loss: 0.01749705407768488\nFOLD: 2, EPOCH: 14, train_loss: 0.028782025347051977, valid_loss: 0.01754928324371576\nFOLD: 2, EPOCH: 15, train_loss: 0.02874300351404414, valid_loss: 0.017366738691926002\nFOLD: 2, EPOCH: 16, train_loss: 0.028653662601093047, valid_loss: 0.017198597826063634\nFOLD: 2, EPOCH: 17, train_loss: 0.028576930984854698, valid_loss: 0.017218474447727204\nFOLD: 2, EPOCH: 18, train_loss: 0.028396014681681483, valid_loss: 0.017038510888814928\nFOLD: 2, EPOCH: 19, train_loss: 0.028277769431370458, valid_loss: 0.017042271383106707\nFOLD: 2, EPOCH: 20, train_loss: 0.028097163746450222, valid_loss: 0.016956024765968324\nFOLD: 2, EPOCH: 21, train_loss: 0.02787532052975528, valid_loss: 0.016789828054606916\nFOLD: 2, EPOCH: 22, train_loss: 0.02759582659571755, valid_loss: 0.016524147316813467\nFOLD: 2, EPOCH: 23, train_loss: 0.027324689311437868, valid_loss: 0.016332346834242343\nFOLD: 2, EPOCH: 24, train_loss: 0.026960719490841945, valid_loss: 0.016215484663844107\nFOLD: 2, EPOCH: 25, train_loss: 0.026620531796800847, valid_loss: 0.016097195260226727\nFOLD: 2, EPOCH: 26, train_loss: 0.02620673951293741, valid_loss: 0.01602814231067896\nFOLD: 2, EPOCH: 27, train_loss: 0.02584649575557433, valid_loss: 0.015935865603387354\nFOLD: 2, EPOCH: 28, train_loss: 0.0255450774491036, valid_loss: 0.01593213390558958\nFOLD: 2, EPOCH: 29, train_loss: 0.02540054348405121, valid_loss: 0.01590486105531454\nFOLD: 3, EPOCH: 0, train_loss: 1.0959536232510392, valid_loss: 0.04079671740531921\nFOLD: 3, EPOCH: 1, train_loss: 0.04057494069443268, valid_loss: 0.02158201448619366\nFOLD: 3, EPOCH: 2, train_loss: 0.032384504367705104, valid_loss: 0.019292023256421088\nFOLD: 3, EPOCH: 3, train_loss: 0.03055510680083515, valid_loss: 0.01872499242424965\nFOLD: 3, EPOCH: 4, train_loss: 0.02986839853328507, valid_loss: 0.01806013993918896\nFOLD: 3, EPOCH: 5, train_loss: 0.029622448088765956, valid_loss: 0.018223414048552513\nFOLD: 3, EPOCH: 6, train_loss: 0.029378240598606414, valid_loss: 0.01814570315182209\nFOLD: 3, EPOCH: 7, train_loss: 0.029250831096148004, valid_loss: 0.017749752625823022\nFOLD: 3, EPOCH: 8, train_loss: 0.029079754399705907, valid_loss: 0.018082692325115203\nFOLD: 3, EPOCH: 9, train_loss: 0.029046245286760686, valid_loss: 0.017641827836632727\nFOLD: 3, EPOCH: 10, train_loss: 0.028965294424469778, valid_loss: 0.01765572600066662\nFOLD: 3, EPOCH: 11, train_loss: 0.029053885274294283, valid_loss: 0.01801666520535946\nFOLD: 3, EPOCH: 12, train_loss: 0.02896761017389038, valid_loss: 0.017750283926725386\nFOLD: 3, EPOCH: 13, train_loss: 0.02892073234986691, valid_loss: 0.017460047230124474\nFOLD: 3, EPOCH: 14, train_loss: 0.028854982952783707, valid_loss: 0.01763911232352257\nFOLD: 3, EPOCH: 15, train_loss: 0.02877507263756528, valid_loss: 0.017438979260623454\nFOLD: 3, EPOCH: 16, train_loss: 0.02875446869047726, valid_loss: 0.017409545592963694\nFOLD: 3, EPOCH: 17, train_loss: 0.028596926277794806, valid_loss: 0.017362681739032267\nFOLD: 3, EPOCH: 18, train_loss: 0.028526798866334416, valid_loss: 0.017113284096121787\nFOLD: 3, EPOCH: 19, train_loss: 0.02834832497245195, valid_loss: 0.017140455022454263\nFOLD: 3, EPOCH: 20, train_loss: 0.028242810280854198, valid_loss: 0.017080313973128796\nFOLD: 3, EPOCH: 21, train_loss: 0.0279712511264548, valid_loss: 0.01681823071092367\nFOLD: 3, EPOCH: 22, train_loss: 0.027732776224511823, valid_loss: 0.016671847961843015\nFOLD: 3, EPOCH: 23, train_loss: 0.0274947994977844, valid_loss: 0.016546773873269558\nFOLD: 3, EPOCH: 24, train_loss: 0.027191740986542638, valid_loss: 0.016418144889175892\nFOLD: 3, EPOCH: 25, train_loss: 0.026845825569970266, valid_loss: 0.016380617432296275\nFOLD: 3, EPOCH: 26, train_loss: 0.02651248721494561, valid_loss: 0.016211411990225313\nFOLD: 3, EPOCH: 27, train_loss: 0.026207523103676685, valid_loss: 0.016151041723787785\nFOLD: 3, EPOCH: 28, train_loss: 0.025980934982194382, valid_loss: 0.016113677285611628\nFOLD: 3, EPOCH: 29, train_loss: 0.025893323161468213, valid_loss: 0.016086642257869245\nFOLD: 4, EPOCH: 0, train_loss: 1.0977384818046272, valid_loss: 0.043861235827207565\nFOLD: 4, EPOCH: 1, train_loss: 0.039625690122242686, valid_loss: 0.022128868401050567\nFOLD: 4, EPOCH: 2, train_loss: 0.035845321685481234, valid_loss: 0.020707140043377877\nFOLD: 4, EPOCH: 3, train_loss: 0.031839409437613424, valid_loss: 0.019380180910229684\nFOLD: 4, EPOCH: 4, train_loss: 0.030326992197304357, valid_loss: 0.018419302105903625\nFOLD: 4, EPOCH: 5, train_loss: 0.029763424591649146, valid_loss: 0.01802234709262848\nFOLD: 4, EPOCH: 6, train_loss: 0.029269884068037377, valid_loss: 0.017963613867759704\nFOLD: 4, EPOCH: 7, train_loss: 0.029148265706742702, valid_loss: 0.017665999606251717\nFOLD: 4, EPOCH: 8, train_loss: 0.029029801424567393, valid_loss: 0.018122261986136437\nFOLD: 4, EPOCH: 9, train_loss: 0.02888715373618262, valid_loss: 0.017543879598379136\nFOLD: 4, EPOCH: 10, train_loss: 0.02883637132642626, valid_loss: 0.01764813207089901\nFOLD: 4, EPOCH: 11, train_loss: 0.02882015708909959, valid_loss: 0.01771046794950962\nFOLD: 4, EPOCH: 12, train_loss: 0.028768530381577357, valid_loss: 0.017665216326713563\nFOLD: 4, EPOCH: 13, train_loss: 0.02880124481660979, valid_loss: 0.017486376315355302\nFOLD: 4, EPOCH: 14, train_loss: 0.02872447672040284, valid_loss: 0.017489541098475458\nFOLD: 4, EPOCH: 15, train_loss: 0.028595160895667108, valid_loss: 0.01725028030574322\nFOLD: 4, EPOCH: 16, train_loss: 0.028546084884275385, valid_loss: 0.01739358574151993\nFOLD: 4, EPOCH: 17, train_loss: 0.028424568613674366, valid_loss: 0.017383644804358483\nFOLD: 4, EPOCH: 18, train_loss: 0.028423242765117665, valid_loss: 0.017054766304790973\nFOLD: 4, EPOCH: 19, train_loss: 0.028119125029667704, valid_loss: 0.0169214041903615\nFOLD: 4, EPOCH: 20, train_loss: 0.02799513554644017, valid_loss: 0.016929226703941824\nFOLD: 4, EPOCH: 21, train_loss: 0.02779241220480731, valid_loss: 0.01675152827054262\nFOLD: 4, EPOCH: 22, train_loss: 0.027485422764708396, valid_loss: 0.016604069359600543\nFOLD: 4, EPOCH: 23, train_loss: 0.027215513166318945, valid_loss: 0.016485095620155335\nFOLD: 4, EPOCH: 24, train_loss: 0.02682440048780571, valid_loss: 0.016314546689391137\nFOLD: 4, EPOCH: 25, train_loss: 0.026469607565070497, valid_loss: 0.016193956360220908\nFOLD: 4, EPOCH: 26, train_loss: 0.026046898760864523, valid_loss: 0.016098811067640783\nFOLD: 4, EPOCH: 27, train_loss: 0.025637393259779127, valid_loss: 0.016037268340587617\nFOLD: 4, EPOCH: 28, train_loss: 0.025348888177956854, valid_loss: 0.016023023389279842\nFOLD: 4, EPOCH: 29, train_loss: 0.025172068591730126, valid_loss: 0.015982941649854185\nFOLD: 5, EPOCH: 0, train_loss: 1.0989989414811134, valid_loss: 0.04392334222793579\nFOLD: 5, EPOCH: 1, train_loss: 0.03949528788121379, valid_loss: 0.02215952381491661\nFOLD: 5, EPOCH: 2, train_loss: 0.0332943079803063, valid_loss: 0.019978972300887108\nFOLD: 5, EPOCH: 3, train_loss: 0.03097560046380069, valid_loss: 0.01848701052367687\nFOLD: 5, EPOCH: 4, train_loss: 0.029575172773733432, valid_loss: 0.01820714198052883\nFOLD: 5, EPOCH: 5, train_loss: 0.02942902569462653, valid_loss: 0.018064083606004713\nFOLD: 5, EPOCH: 6, train_loss: 0.02980818172447941, valid_loss: 0.01792797714471817\nFOLD: 5, EPOCH: 7, train_loss: 0.02921221348247966, valid_loss: 0.017820225954055788\nFOLD: 5, EPOCH: 8, train_loss: 0.029083920566707243, valid_loss: 0.017745016887784005\nFOLD: 5, EPOCH: 9, train_loss: 0.029073498028070747, valid_loss: 0.01769703783094883\nFOLD: 5, EPOCH: 10, train_loss: 0.02894421760626391, valid_loss: 0.017504190877079964\nFOLD: 5, EPOCH: 11, train_loss: 0.028987156848112743, valid_loss: 0.017617103159427643\nFOLD: 5, EPOCH: 12, train_loss: 0.028901230531180797, valid_loss: 0.017765602096915245\nFOLD: 5, EPOCH: 13, train_loss: 0.02894034870222312, valid_loss: 0.017656564265489578\nFOLD: 5, EPOCH: 14, train_loss: 0.028835657515189274, valid_loss: 0.01751356288790703\nFOLD: 5, EPOCH: 15, train_loss: 0.02880640981756911, valid_loss: 0.017571334317326544\nFOLD: 5, EPOCH: 16, train_loss: 0.02868785093329391, valid_loss: 0.017367349602282048\nFOLD: 5, EPOCH: 17, train_loss: 0.0285895413459361, valid_loss: 0.017542944848537446\nFOLD: 5, EPOCH: 18, train_loss: 0.028503047986602296, valid_loss: 0.017221743576228618\nFOLD: 5, EPOCH: 19, train_loss: 0.02836328435714553, valid_loss: 0.017178208157420157\nFOLD: 5, EPOCH: 20, train_loss: 0.0281191814158644, valid_loss: 0.016959766037762166\nFOLD: 5, EPOCH: 21, train_loss: 0.027939194116462656, valid_loss: 0.016824786737561225\nFOLD: 5, EPOCH: 22, train_loss: 0.027713592804106724, valid_loss: 0.0167133991420269\nFOLD: 5, EPOCH: 23, train_loss: 0.027499602271281943, valid_loss: 0.016585658565163613\nFOLD: 5, EPOCH: 24, train_loss: 0.02716972562326055, valid_loss: 0.016501586362719537\nFOLD: 5, EPOCH: 25, train_loss: 0.026814908714217392, valid_loss: 0.016286961399018765\nFOLD: 5, EPOCH: 26, train_loss: 0.026472479991969607, valid_loss: 0.016179863773286344\nFOLD: 5, EPOCH: 27, train_loss: 0.026148387135899797, valid_loss: 0.016131566464900972\nFOLD: 5, EPOCH: 28, train_loss: 0.025899926517285458, valid_loss: 0.016118282489478587\nFOLD: 5, EPOCH: 29, train_loss: 0.02578578850089693, valid_loss: 0.016116840429604054\nFOLD: 6, EPOCH: 0, train_loss: 1.098569108819475, valid_loss: 0.04498441085219383\nFOLD: 6, EPOCH: 1, train_loss: 0.03980895795789706, valid_loss: 0.021699033826589584\nFOLD: 6, EPOCH: 2, train_loss: 0.033251545753101915, valid_loss: 0.025066028758883478\nFOLD: 6, EPOCH: 3, train_loss: 0.03184523780633803, valid_loss: 0.019280529506504537\nFOLD: 6, EPOCH: 4, train_loss: 0.030329639307495687, valid_loss: 0.018064082264900208\nFOLD: 6, EPOCH: 5, train_loss: 0.029605683788251715, valid_loss: 0.017600663118064405\nFOLD: 6, EPOCH: 6, train_loss: 0.02950618288093278, valid_loss: 0.017522711269557477\nFOLD: 6, EPOCH: 7, train_loss: 0.029149218671480005, valid_loss: 0.01749286603182554\nFOLD: 6, EPOCH: 8, train_loss: 0.02907013411627335, valid_loss: 0.017856500893831253\nFOLD: 6, EPOCH: 9, train_loss: 0.028990562016866644, valid_loss: 0.017725231796503066\nFOLD: 6, EPOCH: 10, train_loss: 0.029032704097275833, valid_loss: 0.017697845548391343\nFOLD: 6, EPOCH: 11, train_loss: 0.028927532816622532, valid_loss: 0.01763875048607588\nFOLD: 6, EPOCH: 12, train_loss: 0.028907535217550337, valid_loss: 0.017565219216048717\nFOLD: 6, EPOCH: 13, train_loss: 0.028943028845855978, valid_loss: 0.01742658145725727\nFOLD: 6, EPOCH: 14, train_loss: 0.028868096684213397, valid_loss: 0.01747513398528099\nFOLD: 6, EPOCH: 15, train_loss: 0.028770503797093217, valid_loss: 0.01734043575823307\nFOLD: 6, EPOCH: 16, train_loss: 0.02875630965545064, valid_loss: 0.017270997278392316\nFOLD: 6, EPOCH: 17, train_loss: 0.028660508506253464, valid_loss: 0.0171252328902483\nFOLD: 6, EPOCH: 18, train_loss: 0.02847283655384771, valid_loss: 0.017181487269699574\nFOLD: 6, EPOCH: 19, train_loss: 0.028359214908310344, valid_loss: 0.016968791969120503\nFOLD: 6, EPOCH: 20, train_loss: 0.02815779109745204, valid_loss: 0.01684444263577461\nFOLD: 6, EPOCH: 21, train_loss: 0.027886662176068947, valid_loss: 0.01668825738132\nFOLD: 6, EPOCH: 22, train_loss: 0.02765756840405821, valid_loss: 0.016566543355584145\nFOLD: 6, EPOCH: 23, train_loss: 0.027415583049561702, valid_loss: 0.016343518905341626\nFOLD: 6, EPOCH: 24, train_loss: 0.027107207053879492, valid_loss: 0.01622873164713383\nFOLD: 6, EPOCH: 25, train_loss: 0.026750062788728955, valid_loss: 0.01611318439245224\nFOLD: 6, EPOCH: 26, train_loss: 0.026352873135383436, valid_loss: 0.016055031828582287\nFOLD: 6, EPOCH: 27, train_loss: 0.0260426745857714, valid_loss: 0.015967617444694042\nFOLD: 6, EPOCH: 28, train_loss: 0.02578585758349117, valid_loss: 0.01595021665096283\nFOLD: 6, EPOCH: 29, train_loss: 0.025657830772553982, valid_loss: 0.015951457358896734\n"
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "# SEED = [0,1,2,3,4,5,6] #<-- Update\n",
    "SEED = [0]\n",
    "oof = np.zeros((len(train), len(target_scored_cols)))\n",
    "predictions = np.zeros((len(test), len(target_scored_cols)))\n",
    "\n",
    "# mean_scored = np.mean(train[target_scored_cols].values,axis=0)\n",
    "# mean_nscored = np.mean(train[target_nscored_cols].values,axis=0)\n",
    "# pos_scored_rate = np.log(np.where(mean_scored==0, 1e-8, mean_scored))\n",
    "# pos_nscored_rate = np.log(np.where(mean_nscored==0, 1e-8, mean_nscored))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_scored_cols] = oof\n",
    "test[target_scored_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV log_loss:  0.014675799127773535\n"
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_scored_cols).merge(train[['sig_id']+target_scored_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "y_true = train_targets_scored[target_scored_cols].values\n",
    "y_pred = valid_results[target_scored_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_scored_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target_scored.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.8073362902119574\n"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV log_loss:  0.014772181976560779 - hidden size 1500\n",
    "#CV log loss:  0.014761207506656483 - hidden size 1600\n",
    "#CV log_loss:  0.014757089674479135 - hidden size 1600 + recalibrate layers\n",
    "#CV log_loss:  0.014798489345352649 - hidden size 1600 + recalibrate layers + 30epochs\n",
    "#CV log_loss:  0.014993945755985132 - hidden size 1600 + recalibrate layers - batch size 256\n",
    "#CV log_loss:  0.014925429807583115 - hidden size 1600 + recalibrate layers - batch size 256 + 50 epcohs\n",
    "#CV log_loss:  0.01501367781064934  - hidden size 1600 + recalibrate layers - batch size 256 + 50 epcohs div_factor=1e4\n",
    "#CV log_loss:  0.015038120295874759 - hidden size 1600 + recalibrate layers - batch size 256 + 50 epcohs pct_start=0.2\n",
    "#CV log_loss:  0.014925429807583115 - hidden size 1600 + recalibrate layers - batch size 256 + 50 epcohs 2*lr\n",
    "#CV log_loss:  0.014862676189325522 - hidden size 2000 + recalibrate layers - batch size 256 + 50 epcohs\n",
    "#CV log_loss:  0.014805892283163887 - hidden size 2000 + recalibrate layers - batch size 256 + 70 epcohs\n",
    "\n",
    "\n",
    "\n",
    "#CV log_loss:  0.014701033773812383 - hidden size 1000 + recalibrate layers + 30 epochs + 1layer + stats g35 c5\n",
    "#CV log_loss:  0.01470633477903798  - hidden size 1000 + recalibrate layers + 30 epochs + 1layer + stats g15 c5\n",
    "\n",
    "#CV log_loss:  0.014688195780301675 - hidden size 1000 + recalibrate layers + 30 epochs + 1layer + stats + varianceth 0.81\n",
    "\n",
    "#CV log_loss:  0.01473922963978107 - hidden size 1000 + recalibrate layers + 40 epochs + 1layer\n",
    "#CV log_loss:  0.014704245299661241 - hidden size 900 + recalibrate layers + 30 epochs + 1layer\n",
    "#CV log_loss:  0.014700136730168381 - hidden size 1100 + recalibrate layers + 30 epochs + 1layer\n",
    "\n",
    "\n",
    "#CV log_loss:  0.015298968285739336 - hidden size 1600 + recalibrate layers - batch size 64\n",
    "\n",
    "#CV log_loss:  0.014708188641129298 - hidden size 1000 + recalibrate layers + 30 epochs\n",
    "#CV log_loss:  0.014686009232339573 - hidden size 1000 + recalibrate layers + 30 epochs + 1layer\n",
    "#CV log_loss:  0.014675118998674056 - hidden size 1000 + recalibrate layers + 30 epochs + 1layer + stats\n",
    "#\n",
    "\n",
    "#CV log_loss:  0.01468776733769557 without stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#CV log_loss:  0.014751931843087755 baseline\n",
    "#CV log_loss:  0.014746491073514958 baseline + std g,c\n",
    "#CV log_loss:  0.014753663287033052 baseline + std g,c + median g,c\n",
    "#CV log_loss:  0.014747499643338137 baseline + std g,c + sum g,c + kurt g,c\n",
    "#CV log_loss:  0.014745229709145196 baseline + std g,c + sum g,c + mean g,c\n",
    "#CV log_loss:  0.014765909985927743 baseline + std g,c + sum g,c + skew g,c\n",
    "#CV log_loss:  0.01475305848080635  baseline + std g,c + sum g,c + g_var\n",
    "#CV log_loss:  0.014738260008810762 baseline + std g,c + sum g,c\n",
    "#CV log_loss:  0.014735805473683174 baseline + std g,c + sum g,c,gc\n",
    "#CV log_loss:  0.014750251194250852 baseline + std g,c,gc + sum g,c,gc\n",
    "#CV log_loss:  0.01474079804921367  baseline + std g,c + sum g,c,gc + variance 80(threshold)\n",
    "\n",
    "#CV log_loss:  0.014704018818104764 baseline + std g,c + sum g,c,gc + variance 80(threshold) &PCA\n",
    "#CV log loss:  0.014669995520015213 baseline + std g,c + sum g,c,gc + variance 80(threshold) &PCA + clusters3 - CV log_loss:  0.014520597500516905 /0.8153730747072218 (seeds)\n",
    "\n",
    "#CV log_loss:  0.014566130099844471/0.815729267497772 avec prelu, 30 epochs\n",
    "\n",
    "\n",
    "#CV log_loss:  0.014685491308574154 baseline + std g,c + sum g,c,gc + variance 80(threshold) &PCA + clusters5\n",
    "\n",
    "\n",
    "#CV log_loss:  0.015099951953285232 baseline + PCA features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}