{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1605625148746",
   "display_name": "Python 3.7.9 64-bit ('moa_kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['train_targets_scored.csv',\n 'sample_submission.csv',\n '.gitkeep',\n 'train_drug.csv',\n 'train_features.csv',\n 'test_features.csv',\n 'train_targets_nonscored.csv']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "data_dir = '../data/01_raw'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "no_ctl = True\n",
    "ncompo_genes = 600\n",
    "ncompo_cells = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(data_dir+'/train_features.csv')\n",
    "train_targets_scored = pd.read_csv(data_dir+'/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(data_dir+'/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv(data_dir+'/test_features.csv')\n",
    "sample_submission = pd.read_csv(data_dir+'/sample_submission.csv')\n",
    "drug = pd.read_csv(data_dir+'/train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_scored = train_targets_scored.columns[1:]\n",
    "scored = train_targets_scored.merge(drug, on='sig_id', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "not_ctl\n"
    }
   ],
   "source": [
    "if no_ctl:\n",
    "    # cp_type == ctl_vehicle\n",
    "    print(\"not_ctl\")\n",
    "    train_features = train_features[train_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "    test_features = test_features[test_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "    train_targets_scored = train_targets_scored.iloc[train_features.index]\n",
    "    train_targets_nonscored = train_targets_nonscored.iloc[train_features.index]\n",
    "    train_features.reset_index(drop = True, inplace = True)\n",
    "    test_features.reset_index(drop = True, inplace = True)\n",
    "    train_targets_scored.reset_index(drop = True, inplace = True)\n",
    "    train_targets_nonscored.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Indiquer si valeur dans le range max, min\n",
    "\n",
    "# import seaborn as sns\n",
    "# data = pd.concat([train_features,test_features],axis=0)\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"ctl_vehicle\"][\"c-4\"],label=\"normal\")\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"trt_cp\"][\"c-4\"],label=\"treated\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets = train_targets_scored[[c for c in train_targets_scored.columns if (c != \"sig_id\")]].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_inhibitor\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_activator\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_agonist\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_antagonist\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# train_features[GENES].apply(lambda x : stats.moment(x,moment=5),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True gauss rank\n",
    "import cupy as cp\n",
    "from cupyx.scipy.special import erfinv\n",
    "epsilon = 1e-6\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES + CELLS]), pd.DataFrame(test_features[GENES + CELLS])])\n",
    "\n",
    "for k in (GENES + CELLS):\n",
    "    r_gpu = cp.array(data.loc[:,k])\n",
    "    r_gpu = r_gpu.argsort().argsort()\n",
    "    r_gpu = (r_gpu/r_gpu.max()-0.5)*2 \n",
    "    r_gpu = cp.clip(r_gpu,-1+epsilon,1-epsilon)\n",
    "    r_gpu = erfinv(r_gpu) \n",
    "    data.loc[:,k] = cp.asnumpy( r_gpu * np.sqrt(2) )\n",
    "\n",
    "train_features[GENES + CELLS] = data[:train_features.shape[0]]; test_features[GENES + CELLS] = data[-test_features.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RankGauss\n",
    "\n",
    "# for col in (GENES + CELLS):\n",
    "#     transformer = QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n",
    "#     vec_len = len(train_features[col].values)\n",
    "#     vec_len_test = len(test_features[col].values)\n",
    "#     raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "#     transformer.fit(raw_vec)\n",
    "\n",
    "#     train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "#     test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENES\n",
    "n_comp = ncompo_genes \n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELLS\n",
    "n_comp = ncompo_cells\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(21948, 1526)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = GENES\n",
    "    features_c = CELLS\n",
    "\n",
    "    gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517',      'g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416',             'g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']\n",
    "    \n",
    "    for df in train, test:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1) ## <==\n",
    "        # df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1) ## <==\n",
    "        # df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        # df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        # df['g_q25'] = df[features_g].quantile(q=.25,axis = 1)\n",
    "        # df['g_q50'] = df[features_g].quantile(q=.5,axis = 1)\n",
    "        # df['g_q75'] = df[features_g].quantile(q=.75,axis = 1)\n",
    "        #df['g_var'] = df[features_g].apply(axis=1,func=stats.variation)\n",
    "        # df['g_mad'] = df[features_g].mad(axis = 1)\n",
    "\n",
    "\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1) ## <==\n",
    "        # df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1) ## <==\n",
    "        # df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        # df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        # # df['c_q25'] = df[features_c].quantile(q=.25,axis = 1)\n",
    "        # df['c_q50'] = df[features_c].quantile(q=.5,axis = 1)\n",
    "        # df['c_q75'] = df[features_c].quantile(q=.75,axis = 1)\n",
    "        # df['c_var'] = df[features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['c_mad'] = df[features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1) ## <==\n",
    "        # df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        # df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        # df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        # df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        # # df['gc_q25'] = df[features_g + features_c].quantile(q=.25,axis = 1)\n",
    "        # df['gc_q50'] = df[features_g + features_c].quantile(q=.5,axis = 1)\n",
    "        # df['gc_q75'] = df[features_g + features_c].quantile(q=.75,axis = 1)\n",
    "        # df['gc_var'] = df[features_g + features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['gc_mad'] = df[features_g + features_c].mad(axis = 1)\n",
    "\n",
    "        # df['c52_c42'] = df['c-52'] * df['c-42']\n",
    "        # df['c13_c73'] = df['c-13'] * df['c-73']\n",
    "        # df['c26_c13'] = df['c-23'] * df['c-13']\n",
    "        # df['c33_c6'] = df['c-33'] * df['c-6']\n",
    "        # df['c11_c55'] = df['c-11'] * df['c-55']\n",
    "        # df['c38_c63'] = df['c-38'] * df['c-63']\n",
    "        # df['c38_c94'] = df['c-38'] * df['c-94']\n",
    "        # df['c13_c94'] = df['c-13'] * df['c-94']\n",
    "        # df['c4_c52'] = df['c-4'] * df['c-52']\n",
    "        # df['c4_c42'] = df['c-4'] * df['c-42']\n",
    "        # df['c13_c38'] = df['c-13'] * df['c-38']\n",
    "        # df['c55_c2'] = df['c-55'] * df['c-2']\n",
    "        # df['c55_c4'] = df['c-55'] * df['c-4']\n",
    "        # df['c4_c13'] = df['c-4'] * df['c-13']\n",
    "        # df['c82_c42'] = df['c-82'] * df['c-42']\n",
    "        # df['c66_c42'] = df['c-66'] * df['c-42']\n",
    "        # df['c6_c38'] = df['c-6'] * df['c-38']\n",
    "        # df['c2_c13'] = df['c-2'] * df['c-13']\n",
    "        # df['c62_c42'] = df['c-62'] * df['c-42']\n",
    "        # df['c90_c55'] = df['c-90'] * df['c-55']\n",
    "\n",
    "        # for feature in features_c:\n",
    "        #      df[f'{feature}_squared'] = df[feature] ** 2     \n",
    "                \n",
    "        # for feature in gsquarecols:\n",
    "        #     df[f'{feature}_squared'] = df[feature] ** 2\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "train_features,test_features=fe_stats(train_features,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features_gc = train_features[[\"sig_id\"]+GENES+CELLS].copy()\n",
    "# test_features_gc = test_features[[\"sig_id\"]+GENES+CELLS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(21948, 1049)"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "var_thresh = VarianceThreshold(0.8)  #<-- Update\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# def fe_cluster2(train, test, n_clusters = 3, SEED = 42):\n",
    "    \n",
    "\n",
    "#     def create_cluster(train, test, n_clusters = n_clusters):\n",
    "#         train_ = train.copy()\n",
    "#         test_ = test.copy()\n",
    "#         data = pd.concat([train_, test_], axis = 0)\n",
    "#         kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data[[c for c in data.columns if c not in [\"sig_id\",\"cp_type\",\"cp_dose\",\"cp_time\"]]])\n",
    "#         train['cluster'] = kmeans.labels_[:train.shape[0]]\n",
    "#         test['cluster'] = kmeans.labels_[train.shape[0]:]\n",
    "#         train = pd.get_dummies(train, columns = ['cluster'])\n",
    "#         test = pd.get_dummies(test, columns = ['cluster'])\n",
    "#         return train, test\n",
    "    \n",
    "#     train, test = create_cluster(train, test, n_clusters = n_clusters)\n",
    "#     return train, test\n",
    "\n",
    "\n",
    "\n",
    "# train_features,test_features=fe_cluster2(train_features,test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([train1, test1], axis = 0)\n",
    "\n",
    "# distortion = []\n",
    "# for k in range(1,10):\n",
    "#     kmeans = KMeans(n_clusters = k, random_state = 42).fit(data)\n",
    "#     distortion += [kmeans.inertia_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(1,10),distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = train_features.merge(train_features_gc.loc[:,[col for col in train_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")\n",
    "# test_features = test_features.merge(test_features_gc.loc[:,[col for col in test_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_targets_nonscored, on='sig_id')\n",
    "# train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "# test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target_scored = train[train_targets_scored.columns]\n",
    "target_nscored = train[train_targets_nonscored.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test_features.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scored_cols = target_scored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "target_nscored_cols = target_nscored.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds = train.copy()\n",
    "\n",
    "# mskf = MultilabelStratifiedKFold(n_splits=7)\n",
    "\n",
    "# for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target_scored)):\n",
    "#     folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "# folds['kfold'] = folds['kfold'].astype(int)\n",
    "# folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float),          \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, \n",
    "                 num_targets, \n",
    "                 hidden_sizes,\n",
    "                 dropout_rates):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_sizes[0]))\n",
    "        self.activation1 = torch.nn.PReLU(num_parameters = hidden_sizes[0], init = 1.0)\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_sizes[0])\n",
    "        self.dropout2 = nn.Dropout(dropout_rates[0])\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_sizes[0], hidden_sizes[1]))\n",
    "        self.activation2 = torch.nn.PReLU(num_parameters = hidden_sizes[1], init = 1.0)\n",
    "  \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_sizes[1])\n",
    "        self.dropout3 = nn.Dropout(dropout_rates[1])\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_sizes[1], num_targets))\n",
    "\n",
    "    def init_bias(self,pos_scored_rate,pos_nscored_rate):\n",
    "        self.dense3.bias.data = nn.Parameter(torch.tensor(pos_scored_rate, dtype=torch.float))\n",
    "    \n",
    "    def recalibrate_layer(self, layer):\n",
    "        if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "            print ('recalibrate layer.weight_v')\n",
    "            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "        if(torch.isnan(layer.weight).sum() > 0):\n",
    "            print ('recalibrate layer.weight')\n",
    "            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "            layer.weight += 1e-7\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        self.recalibrate_layer(self.dense1)\n",
    "        x = self.activation1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        self.recalibrate_layer(self.dense2)\n",
    "        x = self.activation2(self.dense2(x))\n",
    "\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        self.recalibrate_layer(self.dense3)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PREEPOCHS = 25\n",
    "EPOCHS = 70\n",
    "#EPOCHS = 300 #200\n",
    "PATIENCE=40\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "#LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7           \n",
    "EARLY_STOPPING_STEPS = PATIENCE+5\n",
    "EARLY_STOP = False\n",
    "\n",
    "#hidden_size=1500\n",
    "hidden_sizes = [1300,1000]\n",
    "dropout_rates = [0.24,0.25]\n",
    "#dropout_rate = 0.2619422201258426\n",
    "#dropout_rate = 0.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCATE DRUGS\n",
    "vc = scored.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc<=18].index.sort_values()\n",
    "vc2 = vc.loc[vc>18].index.sort_values()\n",
    "\n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}; dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = scored.groupby('drug_id')[targets_scored].mean().loc[vc1]\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_scored])):\n",
    "    dd = {k:fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "\n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_scored])):\n",
    "    dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "# ASSIGN FOLDS\n",
    "folds = train.merge(drug,on=\"sig_id\")\n",
    "folds['fold'] = folds.drug_id.map(dct1)\n",
    "folds.loc[folds.fold.isna(),'fold'] =\\\n",
    "    folds.loc[folds.fold.isna(),'sig_id'].map(dct2)\n",
    "folds.fold = folds.fold.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1050"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "feature_cols = [c for c in process_data(train).columns if c not in (target_scored_cols + target_nscored_cols)]\n",
    "feature_cols = [c for c in feature_cols if c not in ['fold','sig_id']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=len(feature_cols)\n",
    "num_targets_scored=len(target_scored_cols)\n",
    "num_targets_nscored=len(target_nscored_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(21948, 1656)\n(21948, 1658)\n(3624, 1048)\n(21948, 207)\n(21948, 403)\n(3982, 207)\n"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target_scored.shape)\n",
    "print(target_nscored.shape)\n",
    "print(sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, seed, preTrain=True):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "\n",
    "    \n",
    "    trn_idx = train[train['fold'] != fold].index\n",
    "    val_idx = train[train['fold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['fold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    x_train, y_scored_train  = train_df[feature_cols].values, train_df[target_scored_cols].values\n",
    "    x_valid, y_scored_valid  =  valid_df[feature_cols].values, valid_df[target_scored_cols].values\n",
    "\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "\n",
    "\n",
    "    \n",
    "    if preTrain:\n",
    "        print(f\"Beginning pretraining for fold {fold}\")\n",
    "        y_nscored_train = train_df[target_nscored_cols].values\n",
    "        y_nscored_valid = valid_df[target_nscored_cols].values\n",
    "\n",
    "        train_dataset = TrainDataset(x_train, y_nscored_train)\n",
    "        valid_dataset = TrainDataset(x_valid, y_nscored_train)\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        num_targets=len(target_nscored_cols)\n",
    "\n",
    "\n",
    "\n",
    "        model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "        )\n",
    "\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,pct_start=0.1, div_factor=1e4, final_div_factor=1e5,\n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "\n",
    "\n",
    "        early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "        early_step = 0\n",
    "        best_loss = np.inf\n",
    "    \n",
    "        #Main pretrain loop\n",
    "        for epoch in range(PREEPOCHS):\n",
    "        \n",
    "            train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "            print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "            #scheduler.step(valid_loss)\n",
    "        \n",
    "            if valid_loss < best_loss:\n",
    "            \n",
    "                best_loss = valid_loss\n",
    "                torch.save(model.state_dict(), f\"preFOLD{fold}_m1b_.pth\")\n",
    "        \n",
    "            elif(EARLY_STOP == True):\n",
    "            \n",
    "                early_step += 1\n",
    "                if (early_step >= early_stopping_steps):\n",
    "                    break\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    if preTrain:\n",
    "        model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=len(target_nscored_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(torch.load(f\"preFOLD{fold}_m1b_.pth\"))\n",
    "        # model.batch_norm3 = nn.BatchNorm1d(hidden_sizes[1])\n",
    "        # model.dropout3 = nn.Dropout(dropout_rates[1])\n",
    "        model.dense3 = nn.utils.weight_norm(nn.Linear(hidden_sizes[1], len(target_scored_cols)))\n",
    "    else:\n",
    "        model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=len(target_scored_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "        )\n",
    "\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,pct_start=0.1, div_factor=1e4, final_div_factor=1e5,\n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "\n",
    "    train_dataset = TrainDataset(x_train, y_scored_train)\n",
    "    valid_dataset = TrainDataset(x_valid, y_scored_valid)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "   \n",
    "    oof = np.zeros((len(train), target_scored.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    #Main train loop\n",
    "    print(f\"Beginning training for fold {fold}\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "        #scheduler.step(valid_loss)\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_m1b_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=len(target_scored_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_m1b_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target_scored.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_scored_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_scored_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Beginning pretraining for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.7307123880128603, valid_loss: 0.696028470993042\nFOLD: 0, EPOCH: 1, train_loss: 0.6394638837994756, valid_loss: 0.42051705718040466\nFOLD: 0, EPOCH: 2, train_loss: 0.15126363485044725, valid_loss: 0.019557266629167964\nFOLD: 0, EPOCH: 3, train_loss: 0.014052144075567657, valid_loss: 0.007022310979664326\nFOLD: 0, EPOCH: 4, train_loss: 0.00951104993755753, valid_loss: 0.005857023610068219\nFOLD: 0, EPOCH: 5, train_loss: 0.008856854477041477, valid_loss: 0.005085070883589131\nFOLD: 0, EPOCH: 6, train_loss: 0.008523153380264301, valid_loss: 0.005001619657767671\nFOLD: 0, EPOCH: 7, train_loss: 0.008703222850690017, valid_loss: 0.004884069785475731\nFOLD: 0, EPOCH: 8, train_loss: 0.008387055650756165, valid_loss: 0.004782757322703089\nFOLD: 0, EPOCH: 9, train_loss: 0.008329399106268948, valid_loss: 0.004845736548304558\nFOLD: 0, EPOCH: 10, train_loss: 0.008306052095281917, valid_loss: 0.004967217998845237\nFOLD: 0, EPOCH: 11, train_loss: 0.008263611083699239, valid_loss: 0.004759699438831636\nFOLD: 0, EPOCH: 12, train_loss: 0.008244511621022547, valid_loss: 0.0055836208962968415\nFOLD: 0, EPOCH: 13, train_loss: 0.008207172092453047, valid_loss: 0.0048617639445832795\nFOLD: 0, EPOCH: 14, train_loss: 0.00815601203892682, valid_loss: 0.004981816147587129\nFOLD: 0, EPOCH: 15, train_loss: 0.008128080768762407, valid_loss: 0.004687148944607803\nFOLD: 0, EPOCH: 16, train_loss: 0.008097862912895711, valid_loss: 0.004734296351671219\nFOLD: 0, EPOCH: 17, train_loss: 0.008067748610030961, valid_loss: 0.004951297239001308\nFOLD: 0, EPOCH: 18, train_loss: 0.008066705704943554, valid_loss: 0.0050013345400137564\nFOLD: 0, EPOCH: 19, train_loss: 0.008047237602138036, valid_loss: 0.004744385036506823\nFOLD: 0, EPOCH: 20, train_loss: 0.00801922641442837, valid_loss: 0.004794894251972437\nFOLD: 0, EPOCH: 21, train_loss: 0.008037498623535439, valid_loss: 0.004850375399525676\nFOLD: 0, EPOCH: 22, train_loss: 0.008020993548671942, valid_loss: 0.004865456771637712\nFOLD: 0, EPOCH: 23, train_loss: 0.00803666904165938, valid_loss: 0.004884738408561263\nFOLD: 0, EPOCH: 24, train_loss: 0.008035431251030517, valid_loss: 0.004873889964073896\nBeginning training for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.5458342432975769, valid_loss: 0.19144329002925328\nFOLD: 0, EPOCH: 1, train_loss: 0.06763377871263672, valid_loss: 0.021826913048114096\nFOLD: 0, EPOCH: 2, train_loss: 0.023552772714882285, valid_loss: 0.019364771034036363\nFOLD: 0, EPOCH: 3, train_loss: 0.021624498421678674, valid_loss: 0.01816278855715479\nFOLD: 0, EPOCH: 4, train_loss: 0.021082206402678747, valid_loss: 0.018072728599820818\nFOLD: 0, EPOCH: 5, train_loss: 0.02086398171613345, valid_loss: 0.01801687479019165\nFOLD: 0, EPOCH: 6, train_loss: 0.02070078908188923, valid_loss: 0.01825589207666261\nFOLD: 0, EPOCH: 7, train_loss: 0.020502819604164845, valid_loss: 0.0180250738880464\nFOLD: 0, EPOCH: 8, train_loss: 0.020428504190734914, valid_loss: 0.01754347368010453\nFOLD: 0, EPOCH: 9, train_loss: 0.020244251735307073, valid_loss: 0.017769835889339447\nFOLD: 0, EPOCH: 10, train_loss: 0.020223072654492146, valid_loss: 0.0175035438899483\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 11, train_loss: 0.02012472423548634, valid_loss: 0.017478089513523237\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 12, train_loss: 0.019735569537088677, valid_loss: 0.017499073275497982\nFOLD: 0, EPOCH: 13, train_loss: 0.019567938521504402, valid_loss: 0.017276053449937274\nFOLD: 0, EPOCH: 14, train_loss: 0.01949921381231901, valid_loss: 0.017603218821542605\nFOLD: 0, EPOCH: 15, train_loss: 0.01943719794822706, valid_loss: 0.017401283606886864\nFOLD: 0, EPOCH: 16, train_loss: 0.019389999980056607, valid_loss: 0.01733237025993211\nFOLD: 0, EPOCH: 17, train_loss: 0.019381174163238424, valid_loss: 0.01733571956200259\nFOLD: 0, EPOCH: 18, train_loss: 0.019317515647491894, valid_loss: 0.017212514100330218\nFOLD: 0, EPOCH: 19, train_loss: 0.019269510851921263, valid_loss: 0.01724437198468617\nFOLD: 0, EPOCH: 20, train_loss: 0.01926537401772834, valid_loss: 0.017317426523991993\nFOLD: 0, EPOCH: 21, train_loss: 0.019236850577431755, valid_loss: 0.017279609239527156\nFOLD: 0, EPOCH: 22, train_loss: 0.019222356127323332, valid_loss: 0.01731150464287826\nFOLD: 0, EPOCH: 23, train_loss: 0.019196126962432992, valid_loss: 0.017307220292942866\nFOLD: 0, EPOCH: 24, train_loss: 0.01917093453576436, valid_loss: 0.01716991008392402\nFOLD: 0, EPOCH: 25, train_loss: 0.01917317323386669, valid_loss: 0.017163243144750595\nFOLD: 0, EPOCH: 26, train_loss: 0.0191616530253275, valid_loss: 0.017446318907397135\nFOLD: 0, EPOCH: 27, train_loss: 0.019116173825553945, valid_loss: 0.017162598935621127\nFOLD: 0, EPOCH: 28, train_loss: 0.01912140065955149, valid_loss: 0.017168565254126276\nFOLD: 0, EPOCH: 29, train_loss: 0.01912631597873327, valid_loss: 0.017319425408329283\nFOLD: 0, EPOCH: 30, train_loss: 0.019085852479612506, valid_loss: 0.017389004517878805\nFOLD: 0, EPOCH: 31, train_loss: 0.019084328181437543, valid_loss: 0.017334471589752605\nFOLD: 0, EPOCH: 32, train_loss: 0.019058253918145154, valid_loss: 0.017289486048477038\nFOLD: 0, EPOCH: 33, train_loss: 0.019022909068578, valid_loss: 0.017306575817721232\nFOLD: 0, EPOCH: 34, train_loss: 0.019045219731491966, valid_loss: 0.017266479453870227\nFOLD: 0, EPOCH: 35, train_loss: 0.019073470118078025, valid_loss: 0.017210578013743674\nFOLD: 0, EPOCH: 36, train_loss: 0.01900673088793819, valid_loss: 0.017358032720429555\nFOLD: 0, EPOCH: 37, train_loss: 0.01900148849833656, valid_loss: 0.017243410061512674\nFOLD: 0, EPOCH: 38, train_loss: 0.018972716369741672, valid_loss: 0.0172696126891034\nFOLD: 0, EPOCH: 39, train_loss: 0.018968583814598417, valid_loss: 0.01728088621582304\nFOLD: 0, EPOCH: 40, train_loss: 0.018936267223309825, valid_loss: 0.017257953328745707\nFOLD: 0, EPOCH: 41, train_loss: 0.018952098398192507, valid_loss: 0.01725216130060809\nFOLD: 0, EPOCH: 42, train_loss: 0.01891091591804414, valid_loss: 0.017235363434467996\nFOLD: 0, EPOCH: 43, train_loss: 0.01892392719919617, valid_loss: 0.017251274415424893\nFOLD: 0, EPOCH: 44, train_loss: 0.018875828214191103, valid_loss: 0.01721604061978204\nFOLD: 0, EPOCH: 45, train_loss: 0.018880088637406763, valid_loss: 0.017193844275815145\nFOLD: 0, EPOCH: 46, train_loss: 0.018853385555180343, valid_loss: 0.01720790485186236\nFOLD: 0, EPOCH: 47, train_loss: 0.018821534524495538, valid_loss: 0.01715466513165406\nFOLD: 0, EPOCH: 48, train_loss: 0.018808776714109087, valid_loss: 0.0171371546706983\nFOLD: 0, EPOCH: 49, train_loss: 0.018785315691619307, valid_loss: 0.017143047281673977\nFOLD: 0, EPOCH: 50, train_loss: 0.01877386500505177, valid_loss: 0.01716636280928339\nFOLD: 0, EPOCH: 51, train_loss: 0.018757794958514137, valid_loss: 0.017213943813528334\nFOLD: 0, EPOCH: 52, train_loss: 0.018747199125386572, valid_loss: 0.017151404970458577\nFOLD: 0, EPOCH: 53, train_loss: 0.018721371486380294, valid_loss: 0.017072066398603574\nFOLD: 0, EPOCH: 54, train_loss: 0.01868365246903252, valid_loss: 0.017151658024106706\nFOLD: 0, EPOCH: 55, train_loss: 0.018691083466684497, valid_loss: 0.01706381221967084\nFOLD: 0, EPOCH: 56, train_loss: 0.018671412121605228, valid_loss: 0.01716922808970724\nFOLD: 0, EPOCH: 57, train_loss: 0.0186540359580839, valid_loss: 0.01711142435669899\nFOLD: 0, EPOCH: 58, train_loss: 0.018648001893952087, valid_loss: 0.01712639443576336\nFOLD: 0, EPOCH: 59, train_loss: 0.018619812169187778, valid_loss: 0.01711987570992538\nFOLD: 0, EPOCH: 60, train_loss: 0.01859528898588709, valid_loss: 0.017112215980887413\nFOLD: 0, EPOCH: 61, train_loss: 0.018584535331339448, valid_loss: 0.0171149947813579\nFOLD: 0, EPOCH: 62, train_loss: 0.018590865052632383, valid_loss: 0.017104882480842725\nFOLD: 0, EPOCH: 63, train_loss: 0.018578995975690918, valid_loss: 0.017111865005322864\nFOLD: 0, EPOCH: 64, train_loss: 0.018554083298187, valid_loss: 0.017104117199778557\nFOLD: 0, EPOCH: 65, train_loss: 0.018543213605880737, valid_loss: 0.01711143074291093\nFOLD: 0, EPOCH: 66, train_loss: 0.018533740142309987, valid_loss: 0.01711442055446761\nFOLD: 0, EPOCH: 67, train_loss: 0.018539633601903915, valid_loss: 0.017106388562491963\nFOLD: 0, EPOCH: 68, train_loss: 0.018533487074278498, valid_loss: 0.017106967579041208\nFOLD: 0, EPOCH: 69, train_loss: 0.018545878346304636, valid_loss: 0.017104158444064006\nBeginning pretraining for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.7306522130966187, valid_loss: 0.6963111587933132\nFOLD: 1, EPOCH: 1, train_loss: 0.6378978321681151, valid_loss: 0.4200714443411146\nFOLD: 1, EPOCH: 2, train_loss: 0.1511429808530453, valid_loss: 0.02029214347047465\nFOLD: 1, EPOCH: 3, train_loss: 0.01388826417560513, valid_loss: 0.007203282189688512\nFOLD: 1, EPOCH: 4, train_loss: 0.009428393322269659, valid_loss: 0.005882680615676301\nFOLD: 1, EPOCH: 5, train_loss: 0.008705249106561815, valid_loss: 0.005644122471234628\nFOLD: 1, EPOCH: 6, train_loss: 0.008486535916155254, valid_loss: 0.005208442652864116\nFOLD: 1, EPOCH: 7, train_loss: 0.008480554119356581, valid_loss: 0.005003864278218576\nFOLD: 1, EPOCH: 8, train_loss: 0.008308472300602778, valid_loss: 0.005985953312899385\nFOLD: 1, EPOCH: 9, train_loss: 0.008270446425052109, valid_loss: 0.006370068128619876\nFOLD: 1, EPOCH: 10, train_loss: 0.008242275578448095, valid_loss: 0.005070288798638752\nFOLD: 1, EPOCH: 11, train_loss: 0.00825006135965924, valid_loss: 0.005426325635718448\nFOLD: 1, EPOCH: 12, train_loss: 0.008157298447111168, valid_loss: 0.004790162933724267\nFOLD: 1, EPOCH: 13, train_loss: 0.008142026632118065, valid_loss: 0.00546730835256832\nFOLD: 1, EPOCH: 14, train_loss: 0.008121695520507323, valid_loss: 0.0048368472738989764\nFOLD: 1, EPOCH: 15, train_loss: 0.008077781169197044, valid_loss: 0.0047694177898977485\nFOLD: 1, EPOCH: 16, train_loss: 0.008051736808910564, valid_loss: 0.00515092204191855\nFOLD: 1, EPOCH: 17, train_loss: 0.008041893396325208, valid_loss: 0.004798358306288719\nFOLD: 1, EPOCH: 18, train_loss: 0.008017550902189436, valid_loss: 0.004930281545966864\nFOLD: 1, EPOCH: 19, train_loss: 0.007999090477824211, valid_loss: 0.0048806801704423764\nFOLD: 1, EPOCH: 20, train_loss: 0.00800865377626709, valid_loss: 0.004989008219646556\nFOLD: 1, EPOCH: 21, train_loss: 0.00801628568788638, valid_loss: 0.005030838639608451\nFOLD: 1, EPOCH: 22, train_loss: 0.007992287392954569, valid_loss: 0.004913095917020526\nFOLD: 1, EPOCH: 23, train_loss: 0.007981722142446685, valid_loss: 0.0049451904903565136\nFOLD: 1, EPOCH: 24, train_loss: 0.008006560049850392, valid_loss: 0.0050293805875948495\nBeginning training for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.5537916498409735, valid_loss: 0.19417807246957505\nFOLD: 1, EPOCH: 1, train_loss: 0.06910108930959895, valid_loss: 0.021435953144516264\nFOLD: 1, EPOCH: 2, train_loss: 0.02382085472345352, valid_loss: 0.01987367043537753\nFOLD: 1, EPOCH: 3, train_loss: 0.02180653760159338, valid_loss: 0.018734913585441455\nFOLD: 1, EPOCH: 4, train_loss: 0.02118321946142493, valid_loss: 0.01791976818016597\nFOLD: 1, EPOCH: 5, train_loss: 0.020934398663607804, valid_loss: 0.01763870913003172\nFOLD: 1, EPOCH: 6, train_loss: 0.02072583617189446, valid_loss: 0.01781650206872395\nFOLD: 1, EPOCH: 7, train_loss: 0.02060963072486826, valid_loss: 0.018105132239205495\nFOLD: 1, EPOCH: 8, train_loss: 0.02047757154985054, valid_loss: 0.01835003441997937\nFOLD: 1, EPOCH: 9, train_loss: 0.020370534797375266, valid_loss: 0.0176144681338753\nFOLD: 1, EPOCH: 10, train_loss: 0.020297537885002187, valid_loss: 0.017378725111484528\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 11, train_loss: 0.020157206652534974, valid_loss: 0.017194687255791256\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 12, train_loss: 0.019769090655687695, valid_loss: 0.017169219042573656\nFOLD: 1, EPOCH: 13, train_loss: 0.019611834472901112, valid_loss: 0.01719668959932668\nFOLD: 1, EPOCH: 14, train_loss: 0.01950762144013031, valid_loss: 0.017155727105481283\nFOLD: 1, EPOCH: 15, train_loss: 0.01946546555169531, valid_loss: 0.0175769996962377\nFOLD: 1, EPOCH: 16, train_loss: 0.019428066607262637, valid_loss: 0.017114225242819105\nFOLD: 1, EPOCH: 17, train_loss: 0.019392105373176368, valid_loss: 0.01732736879161426\nFOLD: 1, EPOCH: 18, train_loss: 0.019382350599846325, valid_loss: 0.017368263698049953\nFOLD: 1, EPOCH: 19, train_loss: 0.019334065602035135, valid_loss: 0.017289577051997185\nFOLD: 1, EPOCH: 20, train_loss: 0.01930039582421651, valid_loss: 0.017288399061986377\nFOLD: 1, EPOCH: 21, train_loss: 0.01931765876911782, valid_loss: 0.017242666466959884\nFOLD: 1, EPOCH: 22, train_loss: 0.019329396504405384, valid_loss: 0.017584298870393207\nFOLD: 1, EPOCH: 23, train_loss: 0.019272346150230716, valid_loss: 0.017227301640169963\nFOLD: 1, EPOCH: 24, train_loss: 0.019248727507687902, valid_loss: 0.017173522817237035\nFOLD: 1, EPOCH: 25, train_loss: 0.01921790264345504, valid_loss: 0.01718414973999773\nFOLD: 1, EPOCH: 26, train_loss: 0.019175577757729066, valid_loss: 0.01715583088142531\nFOLD: 1, EPOCH: 27, train_loss: 0.01918228217274756, valid_loss: 0.01711361895182303\nFOLD: 1, EPOCH: 28, train_loss: 0.019119117962750228, valid_loss: 0.01706915189112936\nFOLD: 1, EPOCH: 29, train_loss: 0.019166824282021135, valid_loss: 0.017153630565319742\nFOLD: 1, EPOCH: 30, train_loss: 0.01913884191496952, valid_loss: 0.01724384751703058\nFOLD: 1, EPOCH: 31, train_loss: 0.01909768022596836, valid_loss: 0.017049719845610007\nFOLD: 1, EPOCH: 32, train_loss: 0.019091513410613343, valid_loss: 0.017192704869168147\nFOLD: 1, EPOCH: 33, train_loss: 0.019064396217062667, valid_loss: 0.017062397673726082\nFOLD: 1, EPOCH: 34, train_loss: 0.019044943959326356, valid_loss: 0.01713042746164969\nFOLD: 1, EPOCH: 35, train_loss: 0.01907263374006426, valid_loss: 0.017169177000011717\nFOLD: 1, EPOCH: 36, train_loss: 0.019059017904706904, valid_loss: 0.017250583374074528\nFOLD: 1, EPOCH: 37, train_loss: 0.01901163477954027, valid_loss: 0.01706396442438875\nFOLD: 1, EPOCH: 38, train_loss: 0.01899733892767816, valid_loss: 0.017112846353224347\nFOLD: 1, EPOCH: 39, train_loss: 0.019023001244342006, valid_loss: 0.017054492607712746\nFOLD: 1, EPOCH: 40, train_loss: 0.018991631078156265, valid_loss: 0.01699271345777171\nFOLD: 1, EPOCH: 41, train_loss: 0.01895314736946209, valid_loss: 0.017014069482684135\nFOLD: 1, EPOCH: 42, train_loss: 0.018932046670768712, valid_loss: 0.017024987377226353\nFOLD: 1, EPOCH: 43, train_loss: 0.01891322761169962, valid_loss: 0.01715108672423022\nFOLD: 1, EPOCH: 44, train_loss: 0.01887409419224069, valid_loss: 0.016973086100603853\nFOLD: 1, EPOCH: 45, train_loss: 0.01890236693056854, valid_loss: 0.01700145777847086\nFOLD: 1, EPOCH: 46, train_loss: 0.018865834820914914, valid_loss: 0.01700730182762657\nFOLD: 1, EPOCH: 47, train_loss: 0.018853644966273696, valid_loss: 0.016961603425443172\nFOLD: 1, EPOCH: 48, train_loss: 0.018839375996911847, valid_loss: 0.01699225338441985\nFOLD: 1, EPOCH: 49, train_loss: 0.018818983707476308, valid_loss: 0.016948222714875425\nFOLD: 1, EPOCH: 50, train_loss: 0.01878706828967945, valid_loss: 0.016972264408000877\nFOLD: 1, EPOCH: 51, train_loss: 0.018770471064222825, valid_loss: 0.016988927631505897\nFOLD: 1, EPOCH: 52, train_loss: 0.018759460314302832, valid_loss: 0.016945412515529563\nFOLD: 1, EPOCH: 53, train_loss: 0.018762302559775276, valid_loss: 0.016948607484144822\nFOLD: 1, EPOCH: 54, train_loss: 0.018715039147315798, valid_loss: 0.016993158496916294\nFOLD: 1, EPOCH: 55, train_loss: 0.018693706322763418, valid_loss: 0.016911513172090054\nFOLD: 1, EPOCH: 56, train_loss: 0.018707716837525368, valid_loss: 0.016915077742721354\nFOLD: 1, EPOCH: 57, train_loss: 0.01867677638860973, valid_loss: 0.016919374199850217\nFOLD: 1, EPOCH: 58, train_loss: 0.018645023322991422, valid_loss: 0.016929354784744128\nFOLD: 1, EPOCH: 59, train_loss: 0.018616463938677632, valid_loss: 0.01695846446922847\nFOLD: 1, EPOCH: 60, train_loss: 0.018608416204114218, valid_loss: 0.016922836591090475\nFOLD: 1, EPOCH: 61, train_loss: 0.01863858788400083, valid_loss: 0.01692779122718743\nFOLD: 1, EPOCH: 62, train_loss: 0.018597918940154282, valid_loss: 0.016906739079526494\nFOLD: 1, EPOCH: 63, train_loss: 0.018579202276226635, valid_loss: 0.01691799477807113\nFOLD: 1, EPOCH: 64, train_loss: 0.0186010221069729, valid_loss: 0.01690938988966601\nFOLD: 1, EPOCH: 65, train_loss: 0.018565891310572624, valid_loss: 0.01691703125834465\nFOLD: 1, EPOCH: 66, train_loss: 0.01854508657109093, valid_loss: 0.0169185892279659\nFOLD: 1, EPOCH: 67, train_loss: 0.018559711859435647, valid_loss: 0.01691497370068516\nFOLD: 1, EPOCH: 68, train_loss: 0.018557389594010404, valid_loss: 0.016910633338349208\nFOLD: 1, EPOCH: 69, train_loss: 0.018573730428879325, valid_loss: 0.016909458674490452\nBeginning pretraining for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.7307190733986932, valid_loss: 0.696650777544294\nFOLD: 2, EPOCH: 1, train_loss: 0.6391548973483008, valid_loss: 0.42532128947121756\nFOLD: 2, EPOCH: 2, train_loss: 0.15270846729745735, valid_loss: 0.019336172246507237\nFOLD: 2, EPOCH: 3, train_loss: 0.013852895942290087, valid_loss: 0.006972620795880046\nFOLD: 2, EPOCH: 4, train_loss: 0.00942558714666882, valid_loss: 0.005654333891080958\nFOLD: 2, EPOCH: 5, train_loss: 0.008784928107382479, valid_loss: 0.0050728691608778066\nFOLD: 2, EPOCH: 6, train_loss: 0.008622642707180333, valid_loss: 0.0047068968415260315\nFOLD: 2, EPOCH: 7, train_loss: 0.0084937712857248, valid_loss: 0.005023571396512645\nFOLD: 2, EPOCH: 8, train_loss: 0.008431767843462326, valid_loss: 0.0047271221743098325\nFOLD: 2, EPOCH: 9, train_loss: 0.0084154584721939, valid_loss: 0.005087587716323989\nFOLD: 2, EPOCH: 10, train_loss: 0.008353679277304862, valid_loss: 0.004891998999352966\nFOLD: 2, EPOCH: 11, train_loss: 0.008361080686586935, valid_loss: 0.0049480208461838105\nFOLD: 2, EPOCH: 12, train_loss: 0.008254813647048699, valid_loss: 0.005139773578516075\nFOLD: 2, EPOCH: 13, train_loss: 0.008238733129424823, valid_loss: 0.004783515485801867\nFOLD: 2, EPOCH: 14, train_loss: 0.008183013622623843, valid_loss: 0.005260972838316645\nFOLD: 2, EPOCH: 15, train_loss: 0.008148901806389159, valid_loss: 0.004811051368181195\nFOLD: 2, EPOCH: 16, train_loss: 0.008146939711091487, valid_loss: 0.0049186138702290395\nFOLD: 2, EPOCH: 17, train_loss: 0.008110130998633197, valid_loss: 0.004588298566107239\nFOLD: 2, EPOCH: 18, train_loss: 0.008096315230972864, valid_loss: 0.005053347043160882\nFOLD: 2, EPOCH: 19, train_loss: 0.008085543629587503, valid_loss: 0.004963137209415436\nFOLD: 2, EPOCH: 20, train_loss: 0.008051056830162133, valid_loss: 0.004758150982005256\nFOLD: 2, EPOCH: 21, train_loss: 0.008058912737446057, valid_loss: 0.004811466471957309\nFOLD: 2, EPOCH: 22, train_loss: 0.008049899661863173, valid_loss: 0.004769602990044015\nFOLD: 2, EPOCH: 23, train_loss: 0.008041306927397445, valid_loss: 0.004868509647037301\nFOLD: 2, EPOCH: 24, train_loss: 0.008034461794572102, valid_loss: 0.004755112874720778\nBeginning training for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.5528826854518942, valid_loss: 0.1953348389693669\nFOLD: 2, EPOCH: 1, train_loss: 0.06719862141117856, valid_loss: 0.02236134691962174\nFOLD: 2, EPOCH: 2, train_loss: 0.023561482997359457, valid_loss: 0.02026707333113466\nFOLD: 2, EPOCH: 3, train_loss: 0.02164746483636869, valid_loss: 0.01916046759911946\nFOLD: 2, EPOCH: 4, train_loss: 0.02109551671388987, valid_loss: 0.01859595812857151\nFOLD: 2, EPOCH: 5, train_loss: 0.02083257621003164, valid_loss: 0.01856194463159357\nFOLD: 2, EPOCH: 6, train_loss: 0.020698122730528987, valid_loss: 0.01846316616450037\nFOLD: 2, EPOCH: 7, train_loss: 0.02050125171002504, valid_loss: 0.018366561936480657\nFOLD: 2, EPOCH: 8, train_loss: 0.02040456683450454, valid_loss: 0.018060460154499327\nFOLD: 2, EPOCH: 9, train_loss: 0.020241676804584427, valid_loss: 0.017986728942820003\nFOLD: 2, EPOCH: 10, train_loss: 0.020189597145528405, valid_loss: 0.018159909173846245\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 11, train_loss: 0.020131662639008986, valid_loss: 0.018133882167083875\nFOLD: 2, EPOCH: 12, train_loss: 0.019769147239826822, valid_loss: 0.017829601785966327\nFOLD: 2, EPOCH: 13, train_loss: 0.019548232088217866, valid_loss: 0.017861603892275264\nFOLD: 2, EPOCH: 14, train_loss: 0.01951122027192567, valid_loss: 0.017883613439542905\nFOLD: 2, EPOCH: 15, train_loss: 0.01943930470057436, valid_loss: 0.017755139619112015\nFOLD: 2, EPOCH: 16, train_loss: 0.01937275520853094, valid_loss: 0.017733423305409297\nFOLD: 2, EPOCH: 17, train_loss: 0.01934385737655936, valid_loss: 0.017727276576416835\nFOLD: 2, EPOCH: 18, train_loss: 0.019307602549324166, valid_loss: 0.017671687262398855\nFOLD: 2, EPOCH: 19, train_loss: 0.01929380281551464, valid_loss: 0.017884319382054464\nFOLD: 2, EPOCH: 20, train_loss: 0.01929087906673148, valid_loss: 0.017768236409340585\nFOLD: 2, EPOCH: 21, train_loss: 0.019224575544531282, valid_loss: 0.017822701750057086\nFOLD: 2, EPOCH: 22, train_loss: 0.019229209000194394, valid_loss: 0.01785104801612241\nFOLD: 2, EPOCH: 23, train_loss: 0.019196105114108807, valid_loss: 0.017646543149437224\nFOLD: 2, EPOCH: 24, train_loss: 0.01917074273365575, valid_loss: 0.01786573444093977\nFOLD: 2, EPOCH: 25, train_loss: 0.019137449864600156, valid_loss: 0.01763038257403033\nFOLD: 2, EPOCH: 26, train_loss: 0.01916676351951586, valid_loss: 0.01761926178421293\nFOLD: 2, EPOCH: 27, train_loss: 0.019096374260009947, valid_loss: 0.01780722476541996\nFOLD: 2, EPOCH: 28, train_loss: 0.01910143471448808, valid_loss: 0.017731875713382448\nFOLD: 2, EPOCH: 29, train_loss: 0.019079045015009673, valid_loss: 0.017695828740085875\nFOLD: 2, EPOCH: 30, train_loss: 0.019075388843948778, valid_loss: 0.017794995701738765\nFOLD: 2, EPOCH: 31, train_loss: 0.01901582855026464, valid_loss: 0.01773976374949728\nFOLD: 2, EPOCH: 32, train_loss: 0.019044478751114896, valid_loss: 0.01780629956296512\nFOLD: 2, EPOCH: 33, train_loss: 0.01904846601993651, valid_loss: 0.017682861802833422\nFOLD: 2, EPOCH: 34, train_loss: 0.0190160450701778, valid_loss: 0.017688327069793428\nFOLD: 2, EPOCH: 35, train_loss: 0.018991792373157835, valid_loss: 0.017634600134832517\nFOLD: 2, EPOCH: 36, train_loss: 0.019007111421308003, valid_loss: 0.017742912151983807\nFOLD: 2, EPOCH: 37, train_loss: 0.01900779838497574, valid_loss: 0.017779074875371798\nFOLD: 2, EPOCH: 38, train_loss: 0.01893755989904339, valid_loss: 0.017760169293199266\nFOLD: 2, EPOCH: 39, train_loss: 0.018945590265699336, valid_loss: 0.017609656655362675\nFOLD: 2, EPOCH: 40, train_loss: 0.018919894723473368, valid_loss: 0.017757916024753025\nFOLD: 2, EPOCH: 41, train_loss: 0.01894838171633514, valid_loss: 0.0176223771912711\nFOLD: 2, EPOCH: 42, train_loss: 0.018919501352954556, valid_loss: 0.017608516184347018\nFOLD: 2, EPOCH: 43, train_loss: 0.018895118623166472, valid_loss: 0.01777268068066665\nFOLD: 2, EPOCH: 44, train_loss: 0.01884494535624981, valid_loss: 0.01766479866845267\nFOLD: 2, EPOCH: 45, train_loss: 0.01884499182169502, valid_loss: 0.01759599175836359\nFOLD: 2, EPOCH: 46, train_loss: 0.01883441587356297, valid_loss: 0.01761037962777274\nFOLD: 2, EPOCH: 47, train_loss: 0.018819426412920694, valid_loss: 0.01762970270855086\nFOLD: 2, EPOCH: 48, train_loss: 0.018819326333500242, valid_loss: 0.017639149512563432\nFOLD: 2, EPOCH: 49, train_loss: 0.01878980899582038, valid_loss: 0.017533647428665842\nFOLD: 2, EPOCH: 50, train_loss: 0.018754832446575165, valid_loss: 0.01759475895336696\nFOLD: 2, EPOCH: 51, train_loss: 0.018742262159247656, valid_loss: 0.017665748883570944\nFOLD: 2, EPOCH: 52, train_loss: 0.01871591282857431, valid_loss: 0.01759035007229873\nFOLD: 2, EPOCH: 53, train_loss: 0.018685956460398598, valid_loss: 0.017572141917688505\nFOLD: 2, EPOCH: 54, train_loss: 0.01867709807246118, valid_loss: 0.017564997875264714\nFOLD: 2, EPOCH: 55, train_loss: 0.018672147111312765, valid_loss: 0.01755437733871596\nFOLD: 2, EPOCH: 56, train_loss: 0.018661472090595477, valid_loss: 0.017576773783990314\nFOLD: 2, EPOCH: 57, train_loss: 0.018629232018783286, valid_loss: 0.017587479203939438\nFOLD: 2, EPOCH: 58, train_loss: 0.018628698698169476, valid_loss: 0.01753763748066766\nFOLD: 2, EPOCH: 59, train_loss: 0.018586123060133006, valid_loss: 0.017579064039247378\nFOLD: 2, EPOCH: 60, train_loss: 0.018566524056163995, valid_loss: 0.01757242903113365\nFOLD: 2, EPOCH: 61, train_loss: 0.018559575936681515, valid_loss: 0.017549516899245127\nFOLD: 2, EPOCH: 62, train_loss: 0.01855431104431281, valid_loss: 0.017532147467136383\nFOLD: 2, EPOCH: 63, train_loss: 0.01854222590053404, valid_loss: 0.017549890226551464\nFOLD: 2, EPOCH: 64, train_loss: 0.018532169932449185, valid_loss: 0.017549100198916028\nFOLD: 2, EPOCH: 65, train_loss: 0.018518681210037823, valid_loss: 0.01755391646708761\nFOLD: 2, EPOCH: 66, train_loss: 0.01854023860918509, valid_loss: 0.017548454393233572\nFOLD: 2, EPOCH: 67, train_loss: 0.018510044476873166, valid_loss: 0.017543995486838476\nFOLD: 2, EPOCH: 68, train_loss: 0.01853096198189903, valid_loss: 0.01754513116819518\nFOLD: 2, EPOCH: 69, train_loss: 0.01851947877455402, valid_loss: 0.017544598983866826\nBeginning pretraining for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.7306404838690886, valid_loss: 0.6967041833060128\nFOLD: 3, EPOCH: 1, train_loss: 0.6371101527600675, valid_loss: 0.41587108799389433\nFOLD: 3, EPOCH: 2, train_loss: 0.14992610088272676, valid_loss: 0.019283810364348546\nFOLD: 3, EPOCH: 3, train_loss: 0.013832246658165712, valid_loss: 0.007101666647940874\nFOLD: 3, EPOCH: 4, train_loss: 0.009442256990115385, valid_loss: 0.005819916725158691\nFOLD: 3, EPOCH: 5, train_loss: 0.008769747499074484, valid_loss: 0.005135437074516501\nFOLD: 3, EPOCH: 6, train_loss: 0.008591670089879551, valid_loss: 0.00522613971095\nFOLD: 3, EPOCH: 7, train_loss: 0.00852218781270691, valid_loss: 0.004937621232654367\nFOLD: 3, EPOCH: 8, train_loss: 0.00838940920358574, valid_loss: 0.005076178016939333\nFOLD: 3, EPOCH: 9, train_loss: 0.008458660978421167, valid_loss: 0.46320789200919016\nFOLD: 3, EPOCH: 10, train_loss: 0.008342990978948167, valid_loss: 0.0048431661645216605\nFOLD: 3, EPOCH: 11, train_loss: 0.00833381010169113, valid_loss: 0.007052788510918617\nFOLD: 3, EPOCH: 12, train_loss: 0.008288539694370451, valid_loss: 0.004795026034116745\nFOLD: 3, EPOCH: 13, train_loss: 0.008220321259687881, valid_loss: 0.006383805402687618\nFOLD: 3, EPOCH: 14, train_loss: 0.00818419042420951, valid_loss: 0.005081687588244677\nFOLD: 3, EPOCH: 15, train_loss: 0.008153443111459146, valid_loss: 0.00503141013905406\nFOLD: 3, EPOCH: 16, train_loss: 0.008134959844519963, valid_loss: 0.004868841663535152\nFOLD: 3, EPOCH: 17, train_loss: 0.008112122279566687, valid_loss: 0.004844251953597579\nFOLD: 3, EPOCH: 18, train_loss: 0.008067234645824175, valid_loss: 0.004908088129013777\nFOLD: 3, EPOCH: 19, train_loss: 0.0080643891029664, valid_loss: 0.004938670101442507\nFOLD: 3, EPOCH: 20, train_loss: 0.008041792675047307, valid_loss: 0.004864025727978775\nFOLD: 3, EPOCH: 21, train_loss: 0.008042256272322423, valid_loss: 0.005400926806032658\nFOLD: 3, EPOCH: 22, train_loss: 0.008031811955309397, valid_loss: 0.005020093771495989\nFOLD: 3, EPOCH: 23, train_loss: 0.008047064338382837, valid_loss: 0.00481116133076804\nFOLD: 3, EPOCH: 24, train_loss: 0.008033684643639906, valid_loss: 0.0051512269170156545\nBeginning training for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.5644668888401341, valid_loss: 0.221457377076149\nFOLD: 3, EPOCH: 1, train_loss: 0.0721708187279669, valid_loss: 0.022024696692824364\nFOLD: 3, EPOCH: 2, train_loss: 0.02398390504153999, valid_loss: 0.019424020977956907\nFOLD: 3, EPOCH: 3, train_loss: 0.02155931233554273, valid_loss: 0.018239470730934824\nFOLD: 3, EPOCH: 4, train_loss: 0.021029346395988722, valid_loss: 0.017793378393564905\nFOLD: 3, EPOCH: 5, train_loss: 0.020805571026898718, valid_loss: 0.01829590568585055\nFOLD: 3, EPOCH: 6, train_loss: 0.020561797745727205, valid_loss: 0.017974695190787315\nFOLD: 3, EPOCH: 7, train_loss: 0.020442761279441213, valid_loss: 0.01769688485988549\nFOLD: 3, EPOCH: 8, train_loss: 0.020317941904067993, valid_loss: 0.017421400706682886\nFOLD: 3, EPOCH: 9, train_loss: 0.020255910182321393, valid_loss: 0.017163862075124468\nFOLD: 3, EPOCH: 10, train_loss: 0.020222490099636285, valid_loss: 0.017283433250018528\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 11, train_loss: 0.02016698438170794, valid_loss: 0.017304730734654834\nFOLD: 3, EPOCH: 12, train_loss: 0.01987159372986974, valid_loss: 0.01697791474206107\nFOLD: 3, EPOCH: 13, train_loss: 0.01972854942888827, valid_loss: 0.016990521656615392\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 14, train_loss: 0.01961180487194577, valid_loss: 0.017153593844601085\nFOLD: 3, EPOCH: 15, train_loss: 0.01942384132259601, valid_loss: 0.016948768869042397\nFOLD: 3, EPOCH: 16, train_loss: 0.019382741956694705, valid_loss: 0.016933405239667212\nFOLD: 3, EPOCH: 17, train_loss: 0.019329546472510777, valid_loss: 0.017030826104538783\nFOLD: 3, EPOCH: 18, train_loss: 0.01931560059656968, valid_loss: 0.016876247844525745\nFOLD: 3, EPOCH: 19, train_loss: 0.01923916191869491, valid_loss: 0.017096973157354763\nFOLD: 3, EPOCH: 20, train_loss: 0.01922997837332455, valid_loss: 0.016875139836754118\nFOLD: 3, EPOCH: 21, train_loss: 0.01918390383188789, valid_loss: 0.016950023493596485\nFOLD: 3, EPOCH: 22, train_loss: 0.019181020356513357, valid_loss: 0.016839719244412014\nFOLD: 3, EPOCH: 23, train_loss: 0.01915272999856923, valid_loss: 0.016830259135791233\nFOLD: 3, EPOCH: 24, train_loss: 0.01910057900523817, valid_loss: 0.01684678186263357\nFOLD: 3, EPOCH: 25, train_loss: 0.019105506054050213, valid_loss: 0.016837415684546744\nFOLD: 3, EPOCH: 26, train_loss: 0.019081398290959565, valid_loss: 0.016896902716585567\nFOLD: 3, EPOCH: 27, train_loss: 0.019093114278606466, valid_loss: 0.01688557943063123\nFOLD: 3, EPOCH: 28, train_loss: 0.01904903508320048, valid_loss: 0.017031026471938406\nFOLD: 3, EPOCH: 29, train_loss: 0.019035336737697188, valid_loss: 0.016796914594514028\nFOLD: 3, EPOCH: 30, train_loss: 0.019003944723187265, valid_loss: 0.017019992428166524\nFOLD: 3, EPOCH: 31, train_loss: 0.01901799616580074, valid_loss: 0.017106800207069943\nFOLD: 3, EPOCH: 32, train_loss: 0.0190131864636331, valid_loss: 0.016985362927828516\nFOLD: 3, EPOCH: 33, train_loss: 0.01898588250214989, valid_loss: 0.01686575104083334\nFOLD: 3, EPOCH: 34, train_loss: 0.01895108028642229, valid_loss: 0.016768193138497218\nFOLD: 3, EPOCH: 35, train_loss: 0.018965276562281558, valid_loss: 0.016818096595151082\nFOLD: 3, EPOCH: 36, train_loss: 0.018938980442849366, valid_loss: 0.01691184804907867\nFOLD: 3, EPOCH: 37, train_loss: 0.018938417269571406, valid_loss: 0.016875465799655234\nFOLD: 3, EPOCH: 38, train_loss: 0.01889755204319954, valid_loss: 0.016897851867335185\nFOLD: 3, EPOCH: 39, train_loss: 0.018886557251617715, valid_loss: 0.01694498929594244\nFOLD: 3, EPOCH: 40, train_loss: 0.018891473527292948, valid_loss: 0.016839096322655678\nFOLD: 3, EPOCH: 41, train_loss: 0.01885025857670887, valid_loss: 0.016862110367843082\nFOLD: 3, EPOCH: 42, train_loss: 0.018870648298714612, valid_loss: 0.016717598640492985\nFOLD: 3, EPOCH: 43, train_loss: 0.018800557716875464, valid_loss: 0.016807740554213524\nFOLD: 3, EPOCH: 44, train_loss: 0.018771363875350437, valid_loss: 0.01680180350584643\nFOLD: 3, EPOCH: 45, train_loss: 0.018779903750967334, valid_loss: 0.016825132870248387\nFOLD: 3, EPOCH: 46, train_loss: 0.018749719233931723, valid_loss: 0.016734130148376738\nFOLD: 3, EPOCH: 47, train_loss: 0.018731607879335817, valid_loss: 0.016775932695184435\nFOLD: 3, EPOCH: 48, train_loss: 0.01869165781583335, valid_loss: 0.016770450930510248\nFOLD: 3, EPOCH: 49, train_loss: 0.01870130707283278, valid_loss: 0.016671232879161835\nFOLD: 3, EPOCH: 50, train_loss: 0.018671248007465054, valid_loss: 0.016721284815243313\nFOLD: 3, EPOCH: 51, train_loss: 0.018662737481094694, valid_loss: 0.01670683441417558\nFOLD: 3, EPOCH: 52, train_loss: 0.018657208697215932, valid_loss: 0.016705839628619806\nFOLD: 3, EPOCH: 53, train_loss: 0.01862025145139243, valid_loss: 0.01669392628329141\nFOLD: 3, EPOCH: 54, train_loss: 0.018612955369659373, valid_loss: 0.016740589269569943\nFOLD: 3, EPOCH: 55, train_loss: 0.018582286212492635, valid_loss: 0.016710236402494565\nFOLD: 3, EPOCH: 56, train_loss: 0.018584798820115423, valid_loss: 0.016691833202328\nFOLD: 3, EPOCH: 57, train_loss: 0.018566394728180523, valid_loss: 0.016688181619559015\nFOLD: 3, EPOCH: 58, train_loss: 0.018526193408949954, valid_loss: 0.016690846798675402\nFOLD: 3, EPOCH: 59, train_loss: 0.018528544721571175, valid_loss: 0.016662642093641416\nFOLD: 3, EPOCH: 60, train_loss: 0.018516915724486917, valid_loss: 0.01669162352170263\nFOLD: 3, EPOCH: 61, train_loss: 0.01848559730963127, valid_loss: 0.016693860558526858\nFOLD: 3, EPOCH: 62, train_loss: 0.018489638897212776, valid_loss: 0.016696858618940626\nFOLD: 3, EPOCH: 63, train_loss: 0.018470760132815386, valid_loss: 0.01667193536247526\nFOLD: 3, EPOCH: 64, train_loss: 0.01843560396416767, valid_loss: 0.01668199896812439\nFOLD: 3, EPOCH: 65, train_loss: 0.0184538581886807, valid_loss: 0.0166734800274883\nFOLD: 3, EPOCH: 66, train_loss: 0.01842427017116869, valid_loss: 0.01667579103793417\nFOLD: 3, EPOCH: 67, train_loss: 0.018458976385158463, valid_loss: 0.016677440410213813\nFOLD: 3, EPOCH: 68, train_loss: 0.018437887768487673, valid_loss: 0.016684369051030705\nFOLD: 3, EPOCH: 69, train_loss: 0.018428057683883486, valid_loss: 0.016673307067581584\nBeginning pretraining for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.7307223951494372, valid_loss: 0.6968967914581299\nFOLD: 4, EPOCH: 1, train_loss: 0.6393705606460571, valid_loss: 0.42246292744364056\nFOLD: 4, EPOCH: 2, train_loss: 0.15218871957748323, valid_loss: 0.020183521988136426\nFOLD: 4, EPOCH: 3, train_loss: 0.014039580030618486, valid_loss: 0.0069618031515606815\nFOLD: 4, EPOCH: 4, train_loss: 0.009440007896439449, valid_loss: 0.005579450101192508\nFOLD: 4, EPOCH: 5, train_loss: 0.008737957457432876, valid_loss: 0.005179103264319045\nFOLD: 4, EPOCH: 6, train_loss: 0.008545542236518216, valid_loss: 0.004989943733172757\nFOLD: 4, EPOCH: 7, train_loss: 0.008524867580146403, valid_loss: 0.004898151515849999\nFOLD: 4, EPOCH: 8, train_loss: 0.008409394970717462, valid_loss: 0.004836212910179581\nFOLD: 4, EPOCH: 9, train_loss: 0.008339336871899463, valid_loss: 0.00477205915376544\nFOLD: 4, EPOCH: 10, train_loss: 0.008269330411142594, valid_loss: 0.0056748817940907815\nFOLD: 4, EPOCH: 11, train_loss: 0.008348067643473277, valid_loss: 0.006099445678825889\nFOLD: 4, EPOCH: 12, train_loss: 0.008229088264744024, valid_loss: 0.004732778096305472\nFOLD: 4, EPOCH: 13, train_loss: 0.008221800376132533, valid_loss: 0.004920535587838718\nFOLD: 4, EPOCH: 14, train_loss: 0.008125961393218589, valid_loss: 0.005220570534999881\nFOLD: 4, EPOCH: 15, train_loss: 0.008089806834185446, valid_loss: 0.004936480029885258\nFOLD: 4, EPOCH: 16, train_loss: 0.008084613615898666, valid_loss: 0.004777189610259873\nFOLD: 4, EPOCH: 17, train_loss: 0.00804572946366829, valid_loss: 0.004683077069265502\nFOLD: 4, EPOCH: 18, train_loss: 0.00803470819232029, valid_loss: 0.004629537330142089\nFOLD: 4, EPOCH: 19, train_loss: 0.008010564862775642, valid_loss: 0.004719042485313756\nFOLD: 4, EPOCH: 20, train_loss: 0.007991242776247295, valid_loss: 0.004783865197428635\nFOLD: 4, EPOCH: 21, train_loss: 0.007979003070677454, valid_loss: 0.004761314152606896\nFOLD: 4, EPOCH: 22, train_loss: 0.007988526246736984, valid_loss: 0.00488853028842381\nFOLD: 4, EPOCH: 23, train_loss: 0.007990569052462643, valid_loss: 0.004869962709822825\nFOLD: 4, EPOCH: 24, train_loss: 0.007980474094684059, valid_loss: 0.004683619697711298\nBeginning training for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.5476753457977965, valid_loss: 0.19083947156156814\nFOLD: 4, EPOCH: 1, train_loss: 0.06617964779001635, valid_loss: 0.021482847364885465\nFOLD: 4, EPOCH: 2, train_loss: 0.0235198384301888, valid_loss: 0.01913293357938528\nFOLD: 4, EPOCH: 3, train_loss: 0.021681523907023506, valid_loss: 0.018288483577115194\nFOLD: 4, EPOCH: 4, train_loss: 0.021178145112620818, valid_loss: 0.018105425472770418\nFOLD: 4, EPOCH: 5, train_loss: 0.020943513492474686, valid_loss: 0.018039315805903504\nFOLD: 4, EPOCH: 6, train_loss: 0.020714567219083373, valid_loss: 0.01787287475807326\nFOLD: 4, EPOCH: 7, train_loss: 0.020549143286975654, valid_loss: 0.01793988381645509\nFOLD: 4, EPOCH: 8, train_loss: 0.0204611772016899, valid_loss: 0.017672167691801275\nFOLD: 4, EPOCH: 9, train_loss: 0.02040540472277113, valid_loss: 0.017421290677573\nFOLD: 4, EPOCH: 10, train_loss: 0.020231416549634288, valid_loss: 0.017888465630156652\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 11, train_loss: 0.0202043711434345, valid_loss: 0.017442788928747177\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 12, train_loss: 0.019890919529102945, valid_loss: 0.01736475101539067\nFOLD: 4, EPOCH: 13, train_loss: 0.019681639113538974, valid_loss: 0.01746147924235889\nFOLD: 4, EPOCH: 14, train_loss: 0.019594999231599474, valid_loss: 0.01755242488746132\nFOLD: 4, EPOCH: 15, train_loss: 0.01952972570182504, valid_loss: 0.017271260331783975\nFOLD: 4, EPOCH: 16, train_loss: 0.019493642237943573, valid_loss: 0.017293971297996386\nFOLD: 4, EPOCH: 17, train_loss: 0.019463295620438213, valid_loss: 0.017266751267015934\nFOLD: 4, EPOCH: 18, train_loss: 0.019460063075294364, valid_loss: 0.017416013537773063\nFOLD: 4, EPOCH: 19, train_loss: 0.019368380962594134, valid_loss: 0.017316479102841446\nFOLD: 4, EPOCH: 20, train_loss: 0.019374823801823565, valid_loss: 0.01721322150634868\nFOLD: 4, EPOCH: 21, train_loss: 0.019366720187905674, valid_loss: 0.017178876059395925\nFOLD: 4, EPOCH: 22, train_loss: 0.019345328111100842, valid_loss: 0.01725487251366888\nFOLD: 4, EPOCH: 23, train_loss: 0.019314411725546862, valid_loss: 0.017286817410162518\nFOLD: 4, EPOCH: 24, train_loss: 0.019274636297612578, valid_loss: 0.017460390260177\nFOLD: 4, EPOCH: 25, train_loss: 0.019293316312738368, valid_loss: 0.017379749566316605\nFOLD: 4, EPOCH: 26, train_loss: 0.019258090672460763, valid_loss: 0.017184716250215257\nFOLD: 4, EPOCH: 27, train_loss: 0.019239226708541047, valid_loss: 0.017260512204042504\nFOLD: 4, EPOCH: 28, train_loss: 0.01923020984474066, valid_loss: 0.017258117773703167\nFOLD: 4, EPOCH: 29, train_loss: 0.01921790203935391, valid_loss: 0.017181800279234136\nFOLD: 4, EPOCH: 30, train_loss: 0.019223649759550352, valid_loss: 0.017553689357425486\nFOLD: 4, EPOCH: 31, train_loss: 0.019187471300766274, valid_loss: 0.01732900898371424\nFOLD: 4, EPOCH: 32, train_loss: 0.019166978579517956, valid_loss: 0.01728217982287918\nFOLD: 4, EPOCH: 33, train_loss: 0.019147895729622326, valid_loss: 0.017381453354443823\nFOLD: 4, EPOCH: 34, train_loss: 0.01915260197947154, valid_loss: 0.017281758998121535\nFOLD: 4, EPOCH: 35, train_loss: 0.019113364251884254, valid_loss: 0.01720720383205584\nFOLD: 4, EPOCH: 36, train_loss: 0.019116450201820682, valid_loss: 0.017139641834156855\nFOLD: 4, EPOCH: 37, train_loss: 0.019106225639178947, valid_loss: 0.017165937327912877\nFOLD: 4, EPOCH: 38, train_loss: 0.01906803814140526, valid_loss: 0.01728133245238236\nFOLD: 4, EPOCH: 39, train_loss: 0.01908217919235294, valid_loss: 0.017116421168403968\nFOLD: 4, EPOCH: 40, train_loss: 0.019053365330438356, valid_loss: 0.017228574625083377\nFOLD: 4, EPOCH: 41, train_loss: 0.019045161083340645, valid_loss: 0.0172448427017246\nFOLD: 4, EPOCH: 42, train_loss: 0.01904808755057889, valid_loss: 0.01709786882357938\nFOLD: 4, EPOCH: 43, train_loss: 0.01904281651651537, valid_loss: 0.017313171178102493\nFOLD: 4, EPOCH: 44, train_loss: 0.01898832035225791, valid_loss: 0.01715887151658535\nFOLD: 4, EPOCH: 45, train_loss: 0.018933017864017875, valid_loss: 0.017081604472228458\nFOLD: 4, EPOCH: 46, train_loss: 0.018974045945985896, valid_loss: 0.017197767937822\nFOLD: 4, EPOCH: 47, train_loss: 0.018919152383868758, valid_loss: 0.017136732249387672\nFOLD: 4, EPOCH: 48, train_loss: 0.01893488151600232, valid_loss: 0.017119507970554487\nFOLD: 4, EPOCH: 49, train_loss: 0.018906783815976734, valid_loss: 0.017115195015711442\nFOLD: 4, EPOCH: 50, train_loss: 0.018908450329625928, valid_loss: 0.01710207693810974\nFOLD: 4, EPOCH: 51, train_loss: 0.018899685829072387, valid_loss: 0.01714625781668084\nFOLD: 4, EPOCH: 52, train_loss: 0.01887971273547894, valid_loss: 0.017055168614855835\nFOLD: 4, EPOCH: 53, train_loss: 0.018823841032949654, valid_loss: 0.017038896679878235\nFOLD: 4, EPOCH: 54, train_loss: 0.018828684162046458, valid_loss: 0.017106206954589913\nFOLD: 4, EPOCH: 55, train_loss: 0.018808004521840328, valid_loss: 0.01703357044607401\nFOLD: 4, EPOCH: 56, train_loss: 0.01877383726674157, valid_loss: 0.01706949355346816\nFOLD: 4, EPOCH: 57, train_loss: 0.018761829649274413, valid_loss: 0.01706627051212958\nFOLD: 4, EPOCH: 58, train_loss: 0.01875470266551585, valid_loss: 0.017069277752723013\nFOLD: 4, EPOCH: 59, train_loss: 0.018740584822119894, valid_loss: 0.017063972806291922\nFOLD: 4, EPOCH: 60, train_loss: 0.018722516660754744, valid_loss: 0.017047706193157604\nFOLD: 4, EPOCH: 61, train_loss: 0.018696671150423384, valid_loss: 0.017036196642688343\nFOLD: 4, EPOCH: 62, train_loss: 0.018699235811426834, valid_loss: 0.01701888721436262\nFOLD: 4, EPOCH: 63, train_loss: 0.01868972373572556, valid_loss: 0.017018643207848072\nFOLD: 4, EPOCH: 64, train_loss: 0.018687298471057736, valid_loss: 0.017036622257104943\nFOLD: 4, EPOCH: 65, train_loss: 0.0186669995152467, valid_loss: 0.017030037673456327\nFOLD: 4, EPOCH: 66, train_loss: 0.018656072181624336, valid_loss: 0.017028810057256902\nFOLD: 4, EPOCH: 67, train_loss: 0.018669300133714806, valid_loss: 0.017021194100379944\nFOLD: 4, EPOCH: 68, train_loss: 0.018669233380539996, valid_loss: 0.01702924245702369\nFOLD: 4, EPOCH: 69, train_loss: 0.018654463863050617, valid_loss: 0.017029347563428537\nBeginning pretraining for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.7306304332372304, valid_loss: 0.6970859851155963\nFOLD: 5, EPOCH: 1, train_loss: 0.6375316462001285, valid_loss: 0.41936857785497395\nFOLD: 5, EPOCH: 2, train_loss: 0.1504876480714695, valid_loss: 0.019941735746605054\nFOLD: 5, EPOCH: 3, train_loss: 0.013756069430225604, valid_loss: 0.007045279123953411\nFOLD: 5, EPOCH: 4, train_loss: 0.009331827857405753, valid_loss: 0.0058498744453702655\nFOLD: 5, EPOCH: 5, train_loss: 0.008680926056931148, valid_loss: 0.005250869518412011\nFOLD: 5, EPOCH: 6, train_loss: 0.008469264375398288, valid_loss: 0.005272511459354844\nFOLD: 5, EPOCH: 7, train_loss: 0.008395865546992502, valid_loss: 0.004877929708787373\nFOLD: 5, EPOCH: 8, train_loss: 0.008289300106667183, valid_loss: 0.005432587848710162\nFOLD: 5, EPOCH: 9, train_loss: 0.008251110419026902, valid_loss: 0.0053899349378688\nFOLD: 5, EPOCH: 10, train_loss: 0.008240895194781793, valid_loss: 0.005315674668444055\nFOLD: 5, EPOCH: 11, train_loss: 0.00819906616281416, valid_loss: 0.004905051817851407\nFOLD: 5, EPOCH: 12, train_loss: 0.008128583519341977, valid_loss: 0.005781515900577817\nFOLD: 5, EPOCH: 13, train_loss: 0.008088156266288983, valid_loss: 0.005531372302877051\nFOLD: 5, EPOCH: 14, train_loss: 0.008060125407536287, valid_loss: 0.005080580844410828\nFOLD: 5, EPOCH: 15, train_loss: 0.008027235360665096, valid_loss: 0.004881938653332847\nFOLD: 5, EPOCH: 16, train_loss: 0.00800876976368395, valid_loss: 0.004821667647255319\nFOLD: 5, EPOCH: 17, train_loss: 0.007995800203266176, valid_loss: 0.0054052359025393215\nFOLD: 5, EPOCH: 18, train_loss: 0.007976760231965297, valid_loss: 0.00569186858566744\nFOLD: 5, EPOCH: 19, train_loss: 0.007970706081470928, valid_loss: 0.004976580518164805\nFOLD: 5, EPOCH: 20, train_loss: 0.007966655004467513, valid_loss: 0.00498150202578732\nFOLD: 5, EPOCH: 21, train_loss: 0.007955278899218585, valid_loss: 0.005100589112511703\nFOLD: 5, EPOCH: 22, train_loss: 0.00796263906601313, valid_loss: 0.005462748929858208\nFOLD: 5, EPOCH: 23, train_loss: 0.00797289628787218, valid_loss: 0.0048508197734398505\nFOLD: 5, EPOCH: 24, train_loss: 0.007965513152649274, valid_loss: 0.004970535436379058\nBeginning training for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.5435304947801538, valid_loss: 0.1972882811512266\nFOLD: 5, EPOCH: 1, train_loss: 0.06706511903856252, valid_loss: 0.022845779146466936\nFOLD: 5, EPOCH: 2, train_loss: 0.02374231634107796, valid_loss: 0.019580213353037834\nFOLD: 5, EPOCH: 3, train_loss: 0.02173439267317991, valid_loss: 0.019279697643859044\nFOLD: 5, EPOCH: 4, train_loss: 0.021181886008865124, valid_loss: 0.018401475889342173\nFOLD: 5, EPOCH: 5, train_loss: 0.020907088812138583, valid_loss: 0.018291250137346133\nFOLD: 5, EPOCH: 6, train_loss: 0.020675217128686002, valid_loss: 0.017905141093901226\nFOLD: 5, EPOCH: 7, train_loss: 0.02059316509277434, valid_loss: 0.01789622301501887\nFOLD: 5, EPOCH: 8, train_loss: 0.020413719520375535, valid_loss: 0.017715066937463626\nFOLD: 5, EPOCH: 9, train_loss: 0.020272284242752438, valid_loss: 0.01791131030768156\nFOLD: 5, EPOCH: 10, train_loss: 0.020258848932949273, valid_loss: 0.01812046207487583\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 11, train_loss: 0.020135493395296303, valid_loss: 0.01767436574612345\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 12, train_loss: 0.01986850526284527, valid_loss: 0.01760154403746128\nFOLD: 5, EPOCH: 13, train_loss: 0.019596891679071093, valid_loss: 0.017454421945980618\nFOLD: 5, EPOCH: 14, train_loss: 0.019513062629345303, valid_loss: 0.01748853855367218\nFOLD: 5, EPOCH: 15, train_loss: 0.019462290345817, valid_loss: 0.017440615354904106\nFOLD: 5, EPOCH: 16, train_loss: 0.019387938132559932, valid_loss: 0.01748120691627264\nFOLD: 5, EPOCH: 17, train_loss: 0.019321539867165928, valid_loss: 0.01756476185151509\nFOLD: 5, EPOCH: 18, train_loss: 0.019293871330651077, valid_loss: 0.01756697560527495\nFOLD: 5, EPOCH: 19, train_loss: 0.019264333000456966, valid_loss: 0.017617395014635155\nFOLD: 5, EPOCH: 20, train_loss: 0.019247713624625594, valid_loss: 0.01749368091779096\nFOLD: 5, EPOCH: 21, train_loss: 0.01919532094050098, valid_loss: 0.017466578233454908\nFOLD: 5, EPOCH: 22, train_loss: 0.019159882354575233, valid_loss: 0.017537373384194716\nFOLD: 5, EPOCH: 23, train_loss: 0.019194081173958006, valid_loss: 0.017568767203816345\nFOLD: 5, EPOCH: 24, train_loss: 0.019192647440610704, valid_loss: 0.017619257792830467\nFOLD: 5, EPOCH: 25, train_loss: 0.01913539949502494, valid_loss: 0.017441296417798315\nFOLD: 5, EPOCH: 26, train_loss: 0.01910777244012098, valid_loss: 0.017412221591387476\nFOLD: 5, EPOCH: 27, train_loss: 0.019088722362711624, valid_loss: 0.01762756492410387\nFOLD: 5, EPOCH: 28, train_loss: 0.019105280573303636, valid_loss: 0.01743477662759168\nFOLD: 5, EPOCH: 29, train_loss: 0.01906649224661492, valid_loss: 0.017530102947992936\nFOLD: 5, EPOCH: 30, train_loss: 0.01905769175170241, valid_loss: 0.01748074684292078\nFOLD: 5, EPOCH: 31, train_loss: 0.019010963069426047, valid_loss: 0.01743857961680208\nFOLD: 5, EPOCH: 32, train_loss: 0.019006145312576682, valid_loss: 0.017364573531917164\nFOLD: 5, EPOCH: 33, train_loss: 0.019007257311730773, valid_loss: 0.017399747855961323\nFOLD: 5, EPOCH: 34, train_loss: 0.01897276983269163, valid_loss: 0.01735521759837866\nFOLD: 5, EPOCH: 35, train_loss: 0.018980238888714765, valid_loss: 0.01751200681818383\nFOLD: 5, EPOCH: 36, train_loss: 0.018980199219407263, valid_loss: 0.01752082604382719\nFOLD: 5, EPOCH: 37, train_loss: 0.018958145853232692, valid_loss: 0.01758345974875348\nFOLD: 5, EPOCH: 38, train_loss: 0.018950863112066244, valid_loss: 0.017493460593479022\nFOLD: 5, EPOCH: 39, train_loss: 0.018922965721906843, valid_loss: 0.01746450604072639\nFOLD: 5, EPOCH: 40, train_loss: 0.018918381148093456, valid_loss: 0.01745029698525156\nFOLD: 5, EPOCH: 41, train_loss: 0.018918310367577785, valid_loss: 0.017450297517435893\nFOLD: 5, EPOCH: 42, train_loss: 0.018852682683516194, valid_loss: 0.01741394147809063\nFOLD: 5, EPOCH: 43, train_loss: 0.018822402164742753, valid_loss: 0.017399017566016743\nFOLD: 5, EPOCH: 44, train_loss: 0.018811999291584298, valid_loss: 0.017348079276936396\nFOLD: 5, EPOCH: 45, train_loss: 0.0188031050603132, valid_loss: 0.01738022959658078\nFOLD: 5, EPOCH: 46, train_loss: 0.01877979888907961, valid_loss: 0.017291462714118615\nFOLD: 5, EPOCH: 47, train_loss: 0.018775331963961188, valid_loss: 0.017403232865035534\nFOLD: 5, EPOCH: 48, train_loss: 0.01878765255615518, valid_loss: 0.017413702926465442\nFOLD: 5, EPOCH: 49, train_loss: 0.01874562962031042, valid_loss: 0.017305643696870123\nFOLD: 5, EPOCH: 50, train_loss: 0.018678147345781326, valid_loss: 0.01734042473669563\nFOLD: 5, EPOCH: 51, train_loss: 0.018678833554322656, valid_loss: 0.01738480957491057\nFOLD: 5, EPOCH: 52, train_loss: 0.018664452272492485, valid_loss: 0.017401660260345255\nFOLD: 5, EPOCH: 53, train_loss: 0.01865806571535162, valid_loss: 0.017351592491780008\nFOLD: 5, EPOCH: 54, train_loss: 0.018632210891794513, valid_loss: 0.017371104364948615\nFOLD: 5, EPOCH: 55, train_loss: 0.018619645840010128, valid_loss: 0.017353537625500133\nFOLD: 5, EPOCH: 56, train_loss: 0.018620147646681684, valid_loss: 0.017322277384144918\nFOLD: 5, EPOCH: 57, train_loss: 0.018591878130226523, valid_loss: 0.01732186840048858\nFOLD: 5, EPOCH: 58, train_loss: 0.01855770609266049, valid_loss: 0.017329515224056586\nFOLD: 5, EPOCH: 59, train_loss: 0.018554047907929163, valid_loss: 0.017349518170314177\nFOLD: 5, EPOCH: 60, train_loss: 0.018533544111493473, valid_loss: 0.017324054879801615\nFOLD: 5, EPOCH: 61, train_loss: 0.01854122173343156, valid_loss: 0.017308249936572144\nFOLD: 5, EPOCH: 62, train_loss: 0.0185114116584127, valid_loss: 0.017320945725909302\nFOLD: 5, EPOCH: 63, train_loss: 0.018508361652493477, valid_loss: 0.01732073710965259\nFOLD: 5, EPOCH: 64, train_loss: 0.01847448300670933, valid_loss: 0.01731315374906574\nFOLD: 5, EPOCH: 65, train_loss: 0.018491594070518338, valid_loss: 0.01731967566800969\nFOLD: 5, EPOCH: 66, train_loss: 0.018493112226998485, valid_loss: 0.0173119403687971\nFOLD: 5, EPOCH: 67, train_loss: 0.018458255189093383, valid_loss: 0.01731323849942003\nFOLD: 5, EPOCH: 68, train_loss: 0.018496214386981888, valid_loss: 0.017316000536084175\nFOLD: 5, EPOCH: 69, train_loss: 0.0185083311957282, valid_loss: 0.017314051144889424\nBeginning pretraining for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.730578987985044, valid_loss: 0.6962139010429382\nFOLD: 6, EPOCH: 1, train_loss: 0.6380995254258852, valid_loss: 0.4186732258115496\nFOLD: 6, EPOCH: 2, train_loss: 0.15065181099281116, valid_loss: 0.020236455968448093\nFOLD: 6, EPOCH: 3, train_loss: 0.013848719111568219, valid_loss: 0.007035249511578253\nFOLD: 6, EPOCH: 4, train_loss: 0.009349625960395142, valid_loss: 0.00572579846318279\nFOLD: 6, EPOCH: 5, train_loss: 0.008696523268480558, valid_loss: 0.004987924293215785\nFOLD: 6, EPOCH: 6, train_loss: 0.008458551699043932, valid_loss: 0.0048788378148206645\nFOLD: 6, EPOCH: 7, train_loss: 0.00837036593489953, valid_loss: 0.004754134986017432\nFOLD: 6, EPOCH: 8, train_loss: 0.00835958164739045, valid_loss: 0.0049552542290517265\nFOLD: 6, EPOCH: 9, train_loss: 0.008251461288514169, valid_loss: 0.005024834203400782\nFOLD: 6, EPOCH: 10, train_loss: 0.008208534277572826, valid_loss: 0.005553180684468576\nFOLD: 6, EPOCH: 11, train_loss: 0.008183948733415958, valid_loss: 0.00649790299524154\nFOLD: 6, EPOCH: 12, train_loss: 0.008133450359408115, valid_loss: 0.005094746526862893\nFOLD: 6, EPOCH: 13, train_loss: 0.008082928589066944, valid_loss: 0.004947261086532048\nFOLD: 6, EPOCH: 14, train_loss: 0.008058768785181077, valid_loss: 0.004700821824371815\nFOLD: 6, EPOCH: 15, train_loss: 0.008023186007866988, valid_loss: 0.004580173308828047\nFOLD: 6, EPOCH: 16, train_loss: 0.007992305465646693, valid_loss: 0.005139034840145281\nFOLD: 6, EPOCH: 17, train_loss: 0.00796998177680212, valid_loss: 0.005166101003331798\nFOLD: 6, EPOCH: 18, train_loss: 0.007968418828740314, valid_loss: 0.004790676292032003\nFOLD: 6, EPOCH: 19, train_loss: 0.007960738400249062, valid_loss: 0.004949224846703666\nFOLD: 6, EPOCH: 20, train_loss: 0.007961933110916131, valid_loss: 0.004959889288459506\nFOLD: 6, EPOCH: 21, train_loss: 0.007953830101099369, valid_loss: 0.0051536583341658115\nFOLD: 6, EPOCH: 22, train_loss: 0.007963088844474909, valid_loss: 0.005207039349313293\nFOLD: 6, EPOCH: 23, train_loss: 0.00796041835250484, valid_loss: 0.004684052962277617\nFOLD: 6, EPOCH: 24, train_loss: 0.007966048021272227, valid_loss: 0.004613372430737529\nBeginning training for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.547527999894039, valid_loss: 0.20052235467093332\nFOLD: 6, EPOCH: 1, train_loss: 0.06838117378789026, valid_loss: 0.021920636562364443\nFOLD: 6, EPOCH: 2, train_loss: 0.02377564540585956, valid_loss: 0.019251023286155293\nFOLD: 6, EPOCH: 3, train_loss: 0.021781349000898568, valid_loss: 0.017739393881389072\nFOLD: 6, EPOCH: 4, train_loss: 0.021173580625169986, valid_loss: 0.01709870794521911\nFOLD: 6, EPOCH: 5, train_loss: 0.020983209581793966, valid_loss: 0.017385430500975678\nFOLD: 6, EPOCH: 6, train_loss: 0.02082221829206557, valid_loss: 0.017756584898701737\nFOLD: 6, EPOCH: 7, train_loss: 0.020664551672903268, valid_loss: 0.017080115021339486\nFOLD: 6, EPOCH: 8, train_loss: 0.020482323610702076, valid_loss: 0.016756855882704258\nFOLD: 6, EPOCH: 9, train_loss: 0.02040286488025575, valid_loss: 0.01685564486043794\nFOLD: 6, EPOCH: 10, train_loss: 0.020316787466809556, valid_loss: 0.01675047685525247\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 11, train_loss: 0.020220475623736512, valid_loss: 0.01659566749419485\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 12, train_loss: 0.01990312257328549, valid_loss: 0.016385269750441824\nFOLD: 6, EPOCH: 13, train_loss: 0.019663218509506534, valid_loss: 0.016382972576788495\nFOLD: 6, EPOCH: 14, train_loss: 0.019554567397446244, valid_loss: 0.016328631235020503\nFOLD: 6, EPOCH: 15, train_loss: 0.019536131539860287, valid_loss: 0.01639531472963946\nFOLD: 6, EPOCH: 16, train_loss: 0.01946910037784963, valid_loss: 0.016285739040800502\nFOLD: 6, EPOCH: 17, train_loss: 0.019402287799764325, valid_loss: 0.01636985463223287\nFOLD: 6, EPOCH: 18, train_loss: 0.01936023838415339, valid_loss: 0.016409153917006085\nFOLD: 6, EPOCH: 19, train_loss: 0.01930725307682076, valid_loss: 0.016363126491861685\nFOLD: 6, EPOCH: 20, train_loss: 0.019301896864497983, valid_loss: 0.01651063242128917\nFOLD: 6, EPOCH: 21, train_loss: 0.019286808482295758, valid_loss: 0.01639007923326322\nFOLD: 6, EPOCH: 22, train_loss: 0.019274615305098327, valid_loss: 0.01623923810464995\nFOLD: 6, EPOCH: 23, train_loss: 0.019269150354572245, valid_loss: 0.016415096021124294\nFOLD: 6, EPOCH: 24, train_loss: 0.019257383119012858, valid_loss: 0.016354782905961786\nFOLD: 6, EPOCH: 25, train_loss: 0.019244623345297737, valid_loss: 0.016433240845799446\nFOLD: 6, EPOCH: 26, train_loss: 0.01918825159805852, valid_loss: 0.01649589743465185\nFOLD: 6, EPOCH: 27, train_loss: 0.019177835434675217, valid_loss: 0.01630120471652065\nFOLD: 6, EPOCH: 28, train_loss: 0.01918927997954794, valid_loss: 0.016232121868857315\nFOLD: 6, EPOCH: 29, train_loss: 0.01915349091428357, valid_loss: 0.016217577005071298\nFOLD: 6, EPOCH: 30, train_loss: 0.01913933954327493, valid_loss: 0.016316302785915986\nFOLD: 6, EPOCH: 31, train_loss: 0.019105086958891637, valid_loss: 0.01625824094350849\nFOLD: 6, EPOCH: 32, train_loss: 0.019118075938643637, valid_loss: 0.016389132743435248\nFOLD: 6, EPOCH: 33, train_loss: 0.01908733761189757, valid_loss: 0.016334712372294495\nFOLD: 6, EPOCH: 34, train_loss: 0.01908564391369755, valid_loss: 0.016081847782645906\nFOLD: 6, EPOCH: 35, train_loss: 0.019079835279970557, valid_loss: 0.016159741873187677\nFOLD: 6, EPOCH: 36, train_loss: 0.019072920839126047, valid_loss: 0.016372190788388252\nFOLD: 6, EPOCH: 37, train_loss: 0.019051981687143043, valid_loss: 0.016213557815977504\nFOLD: 6, EPOCH: 38, train_loss: 0.01901312716103889, valid_loss: 0.01624760710235153\nFOLD: 6, EPOCH: 39, train_loss: 0.01896420831011759, valid_loss: 0.016225308712039675\nFOLD: 6, EPOCH: 40, train_loss: 0.01898948017608475, valid_loss: 0.016232899922345365\nFOLD: 6, EPOCH: 41, train_loss: 0.018945500506339846, valid_loss: 0.016142796458942548\nFOLD: 6, EPOCH: 42, train_loss: 0.018964786938316113, valid_loss: 0.016316128495548452\nFOLD: 6, EPOCH: 43, train_loss: 0.018934086015498317, valid_loss: 0.016113697019006525\nFOLD: 6, EPOCH: 44, train_loss: 0.018930558971053845, valid_loss: 0.016288051514753273\nFOLD: 6, EPOCH: 45, train_loss: 0.01885440039473611, valid_loss: 0.01622081734240055\nFOLD: 6, EPOCH: 46, train_loss: 0.018877342242646863, valid_loss: 0.0160671810486487\nFOLD: 6, EPOCH: 47, train_loss: 0.018870844077822323, valid_loss: 0.0161552241604243\nFOLD: 6, EPOCH: 48, train_loss: 0.01882374060113688, valid_loss: 0.01613421006394284\nFOLD: 6, EPOCH: 49, train_loss: 0.018819271712689788, valid_loss: 0.016081930670355047\nFOLD: 6, EPOCH: 50, train_loss: 0.018789602242208815, valid_loss: 0.016148039805037633\nFOLD: 6, EPOCH: 51, train_loss: 0.018765638506895787, valid_loss: 0.016164470996175493\nFOLD: 6, EPOCH: 52, train_loss: 0.018734719906304334, valid_loss: 0.01607424220336335\nFOLD: 6, EPOCH: 53, train_loss: 0.018742498211764002, valid_loss: 0.016121655303452696\nFOLD: 6, EPOCH: 54, train_loss: 0.01871029247303267, valid_loss: 0.01611531206539699\nFOLD: 6, EPOCH: 55, train_loss: 0.01869260081769647, valid_loss: 0.016124038824013302\nFOLD: 6, EPOCH: 56, train_loss: 0.018677955342305673, valid_loss: 0.01602554214852197\nFOLD: 6, EPOCH: 57, train_loss: 0.018633472708028717, valid_loss: 0.016079930987741266\nFOLD: 6, EPOCH: 58, train_loss: 0.018640125120008313, valid_loss: 0.01606010233185121\nFOLD: 6, EPOCH: 59, train_loss: 0.01860305163505915, valid_loss: 0.01607740337827376\nFOLD: 6, EPOCH: 60, train_loss: 0.01860338283350339, valid_loss: 0.01605961924152715\nFOLD: 6, EPOCH: 61, train_loss: 0.01858359162469168, valid_loss: 0.016042237702224935\nFOLD: 6, EPOCH: 62, train_loss: 0.018566322739462595, valid_loss: 0.01606488986206906\nFOLD: 6, EPOCH: 63, train_loss: 0.018540658107077754, valid_loss: 0.01605370281530278\nFOLD: 6, EPOCH: 64, train_loss: 0.018546915537602193, valid_loss: 0.01605098055941718\nFOLD: 6, EPOCH: 65, train_loss: 0.01853935209077758, valid_loss: 0.016045073978602886\nFOLD: 6, EPOCH: 66, train_loss: 0.018559599546967325, valid_loss: 0.016055420573268617\nFOLD: 6, EPOCH: 67, train_loss: 0.01853884439412001, valid_loss: 0.01604776962527207\nFOLD: 6, EPOCH: 68, train_loss: 0.01852074728624241, valid_loss: 0.016043617124004022\nFOLD: 6, EPOCH: 69, train_loss: 0.018504144523192097, valid_loss: 0.01604647162769522\n"
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "# SEED = [0,1,2,3,4,5,6] #<-- Update\n",
    "SEED = [0]\n",
    "oof = np.zeros((len(train), len(target_scored_cols)))\n",
    "predictions = np.zeros((len(test), len(target_scored_cols)))\n",
    "\n",
    "# mean_scored = np.mean(train[target_scored_cols].values,axis=0)\n",
    "# mean_nscored = np.mean(train[target_nscored_cols].values,axis=0)\n",
    "# pos_scored_rate = np.log(np.where(mean_scored==0, 1e-8, mean_scored))\n",
    "# pos_nscored_rate = np.log(np.where(mean_nscored==0, 1e-8, mean_nscored))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_scored_cols] = oof\n",
    "test[target_scored_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV log_loss:  0.017167792999419937\n"
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_scored_cols).merge(train[['sig_id']+target_scored_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "y_true = train_targets_scored[target_scored_cols].values\n",
    "y_pred = valid_results[target_scored_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_scored_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target_scored.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6338543106534481\n"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV log_loss:  0.017167805053221446 - 0.6321265691914774\n",
    "\n",
    "#[1300,1200] CV log_loss:  0.017198893749125664 - 0.629916963086414\n",
    "# CV log_loss:  0.01717605112196575 - 0.6353968367997714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV log_loss:  0.017210720953361913 - 0.6277702181452575\n",
    "#ep 60 -> 70 - CV log_loss:  0.017182289539647747 - 0.6292206349965699\n",
    "#hidden_sizes = [1300,1000] CV log_loss:  0.017171263482886407 - 0.6299124736515431\n",
    "#hidden_sizes = [1300,1000] CV log_loss:  0.017170478147632742 - 0.6308188233446383\n",
    "\n",
    "#CV log_loss:  0.017182484639766352 - 0.6307064056953464\n",
    "\n",
    "# hidden_sizes = [1300,1000]\n",
    "# dropout_rates = [0.24,0.25]\n",
    "#CV log_loss:  0.017167792999419937 - 0.6338543106534481\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline - CV log_loss:  0.01592192106436285 - 0.6481409205142853\n",
    "#CV log_loss:  0.01592196808239733 - 0.6467970096134422\n",
    "#CV log_loss:  0.015910805622295764 - 0.6536475888580155 epoch 40\n",
    "#CV log_loss:  0.01589769241561799 - 0.6557716465832143 epoch 45\n",
    "#CV log_loss:  0.015891434988748915 - 0.6606290483745609 epoch 50\n",
    "#CV log_loss:  0.01588715803509738 - 0.6583277267355544 epoch 55\n",
    "#CV log_loss:  0.01588569318599064 - 0.6628654461934201 epoch 60\n",
    "#CV log_loss:  0.01590611275187904 - 0.6557353505850225 epoch 65\n",
    "\n",
    "\n",
    "#CV log_loss:  0.01586760697874445 - 0.6593208836774367 epochj 60  [1200,900]\n",
    "#CV log_loss:  0.01585970108953215 - 0.6563924422632783 epoch 60 [1300,900]\n",
    "\n",
    "\n",
    "#CV log_loss:  0.01589648288882382 - 0.6508185055047814 (qnatile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_csv = train_features[[\"sig_id\"]].merge(train[train_targets_scored.columns], on='sig_id', how='left')\n",
    "oof_csv.to_csv(\"model_1b_oof.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = sample_submission[[\"sig_id\"]].merge(test[train_targets_scored.columns], on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('submission_model1b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}