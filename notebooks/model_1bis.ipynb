{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1604926155665",
   "display_name": "Python 3.7.9 64-bit ('moa_kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['train_targets_scored.csv',\n 'sample_submission.csv',\n '.gitkeep',\n 'train_drug.csv',\n 'train_features.csv',\n 'test_features.csv',\n 'train_targets_nonscored.csv']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "data_dir = '../data/01_raw'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "no_ctl = True\n",
    "ncompo_genes = 600\n",
    "ncompo_cells = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(data_dir+'/train_features.csv')\n",
    "train_targets_scored = pd.read_csv(data_dir+'/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(data_dir+'/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv(data_dir+'/test_features.csv')\n",
    "sample_submission = pd.read_csv(data_dir+'/sample_submission.csv')\n",
    "drug = pd.read_csv(data_dir+'/train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_scored = train_targets_scored.columns[1:]\n",
    "scored = train_targets_scored.merge(drug, on='sig_id', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "not_ctl\n"
    }
   ],
   "source": [
    "if no_ctl:\n",
    "    # cp_type == ctl_vehicle\n",
    "    print(\"not_ctl\")\n",
    "    train_features = train_features[train_features[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
    "    test_features = test_features[test_features[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
    "    train_targets_scored = train_targets_scored.iloc[train_features.index]\n",
    "    train_targets_nonscored = train_targets_nonscored.iloc[train_features.index]\n",
    "    train_features.reset_index(drop = True, inplace = True)\n",
    "    train_features.reset_index(drop = True, inplace = True)\n",
    "    train_targets_scored.reset_index(drop = True, inplace = True)\n",
    "    train_targets_nonscored.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Indiquer si valeur dans le range max, min\n",
    "\n",
    "# import seaborn as sns\n",
    "# data = pd.concat([train_features,test_features],axis=0)\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"ctl_vehicle\"][\"c-4\"],label=\"normal\")\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"trt_cp\"][\"c-4\"],label=\"treated\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets = train_targets_scored[[c for c in train_targets_scored.columns if (c != \"sig_id\")]].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_inhibitor\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_activator\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_agonist\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_antagonist\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# train_features[GENES].apply(lambda x : stats.moment(x,moment=5),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RankGauss\n",
    "\n",
    "for col in (GENES + CELLS):\n",
    "    transformer = QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n",
    "    vec_len = len(train_features[col].values)\n",
    "    vec_len_test = len(test_features[col].values)\n",
    "    raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENES\n",
    "n_comp = ncompo_genes \n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELLS\n",
    "n_comp = ncompo_cells\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(21948, 1526)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = GENES\n",
    "    features_c = CELLS\n",
    "    \n",
    "    for df in train, test:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1) ## <==\n",
    "        # df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1) ## <==\n",
    "        # df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        #df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        # df['g_q25'] = df[features_g].quantile(q=.25,axis = 1)\n",
    "        # df['g_q50'] = df[features_g].quantile(q=.5,axis = 1)\n",
    "        # df['g_q75'] = df[features_g].quantile(q=.75,axis = 1)\n",
    "        #df['g_var'] = df[features_g].apply(axis=1,func=stats.variation)\n",
    "        # df['g_mad'] = df[features_g].mad(axis = 1)\n",
    "\n",
    "\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1) ## <==\n",
    "        # df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1) ## <==\n",
    "        # df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        #df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        # df['c_q25'] = df[features_c].quantile(q=.25,axis = 1)\n",
    "        # df['c_q50'] = df[features_c].quantile(q=.5,axis = 1)\n",
    "        # df['c_q75'] = df[features_c].quantile(q=.75,axis = 1)\n",
    "        # df['c_var'] = df[features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['c_mad'] = df[features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1) ## <==\n",
    "        # df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        # df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        # df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        # df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        # df['gc_q25'] = df[features_g + features_c].quantile(q=.25,axis = 1)\n",
    "        # df['gc_q50'] = df[features_g + features_c].quantile(q=.5,axis = 1)\n",
    "        # df['gc_q75'] = df[features_g + features_c].quantile(q=.75,axis = 1)\n",
    "        # df['gc_var'] = df[features_g + features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['gc_mad'] = df[features_g + features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "train_features,test_features=fe_stats(train_features,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features_gc = train_features[[\"sig_id\"]+GENES+CELLS].copy()\n",
    "# test_features_gc = test_features[[\"sig_id\"]+GENES+CELLS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(21948, 1047)"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "var_thresh = VarianceThreshold(0.8)  #<-- Update\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# def fe_cluster2(train, test, n_clusters = 3, SEED = 42):\n",
    "    \n",
    "\n",
    "#     def create_cluster(train, test, n_clusters = n_clusters):\n",
    "#         train_ = train.copy()\n",
    "#         test_ = test.copy()\n",
    "#         data = pd.concat([train_, test_], axis = 0)\n",
    "#         kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data[[c for c in data.columns if c not in [\"sig_id\",\"cp_type\",\"cp_dose\",\"cp_time\"]]])\n",
    "#         train['cluster'] = kmeans.labels_[:train.shape[0]]\n",
    "#         test['cluster'] = kmeans.labels_[train.shape[0]:]\n",
    "#         train = pd.get_dummies(train, columns = ['cluster'])\n",
    "#         test = pd.get_dummies(test, columns = ['cluster'])\n",
    "#         return train, test\n",
    "    \n",
    "#     train, test = create_cluster(train, test, n_clusters = n_clusters)\n",
    "#     return train, test\n",
    "\n",
    "\n",
    "\n",
    "# train_features,test_features=fe_cluster2(train_features,test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([train1, test1], axis = 0)\n",
    "\n",
    "# distortion = []\n",
    "# for k in range(1,10):\n",
    "#     kmeans = KMeans(n_clusters = k, random_state = 42).fit(data)\n",
    "#     distortion += [kmeans.inertia_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(1,10),distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = train_features.merge(train_features_gc.loc[:,[col for col in train_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")\n",
    "# test_features = test_features.merge(test_features_gc.loc[:,[col for col in test_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_targets_nonscored, on='sig_id')\n",
    "# train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "# test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target_scored = train[train_targets_scored.columns]\n",
    "target_nscored = train[train_targets_nonscored.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test_features.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scored_cols = target_scored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "target_nscored_cols = target_nscored.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds = train.copy()\n",
    "\n",
    "# mskf = MultilabelStratifiedKFold(n_splits=7)\n",
    "\n",
    "# for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target_scored)):\n",
    "#     folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "# folds['kfold'] = folds['kfold'].astype(int)\n",
    "# folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float),          \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, \n",
    "                 num_targets, \n",
    "                 hidden_sizes,\n",
    "                 dropout_rates):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_sizes[0]))\n",
    "        self.activation1 = torch.nn.PReLU(num_parameters = hidden_sizes[0], init = 1.0)\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_sizes[0])\n",
    "        self.dropout2 = nn.Dropout(dropout_rates[0])\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_sizes[0], hidden_sizes[1]))\n",
    "        self.activation2 = torch.nn.PReLU(num_parameters = hidden_sizes[1], init = 1.0)\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_sizes[1])\n",
    "        self.dropout3 = nn.Dropout(dropout_rates[1])\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_sizes[1], num_targets))\n",
    "\n",
    "    def init_bias(self,pos_scored_rate,pos_nscored_rate):\n",
    "        self.dense3.bias.data = nn.Parameter(torch.tensor(pos_scored_rate, dtype=torch.float))\n",
    "    \n",
    "    def recalibrate_layer(self, layer):\n",
    "        if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "            print ('recalibrate layer.weight_v')\n",
    "            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "        if(torch.isnan(layer.weight).sum() > 0):\n",
    "            print ('recalibrate layer.weight')\n",
    "            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "            layer.weight += 1e-7\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        self.recalibrate_layer(self.dense1)\n",
    "        x = self.activation1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        self.recalibrate_layer(self.dense2)\n",
    "        x = self.activation2(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        self.recalibrate_layer(self.dense3)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PREEPOCHS = 25\n",
    "EPOCHS = 35\n",
    "#EPOCHS = 300 #200\n",
    "PATIENCE=40\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7           \n",
    "EARLY_STOPPING_STEPS = PATIENCE+5\n",
    "EARLY_STOP = False\n",
    "\n",
    "#hidden_size=1500\n",
    "hidden_sizes = [1100,900]\n",
    "dropout_rates = [0.25,0.25]\n",
    "#dropout_rate = 0.2619422201258426\n",
    "#dropout_rate = 0.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCATE DRUGS\n",
    "vc = scored.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc<=18].index.sort_values()\n",
    "vc2 = vc.loc[vc>18].index.sort_values()\n",
    "\n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}; dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = scored.groupby('drug_id')[targets_scored].mean().loc[vc1]\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_scored])):\n",
    "    dd = {k:fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "\n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_scored])):\n",
    "    dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "# ASSIGN FOLDS\n",
    "folds = train.merge(drug,on=\"sig_id\")\n",
    "folds['fold'] = folds.drug_id.map(dct1)\n",
    "folds.loc[folds.fold.isna(),'fold'] =\\\n",
    "    folds.loc[folds.fold.isna(),'sig_id'].map(dct2)\n",
    "folds.fold = folds.fold.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1048"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "feature_cols = [c for c in process_data(train).columns if c not in (target_scored_cols + target_nscored_cols)]\n",
    "feature_cols = [c for c in feature_cols if c not in ['fold','sig_id']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=len(feature_cols)\n",
    "num_targets_scored=len(target_scored_cols)\n",
    "num_targets_nscored=len(target_nscored_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(20228, 1654)\n(20228, 1656)\n(3624, 1046)\n(20228, 207)\n(20228, 403)\n(3982, 207)\n"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target_scored.shape)\n",
    "print(target_nscored.shape)\n",
    "print(sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, seed, preTrain=True):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "\n",
    "    \n",
    "    trn_idx = train[train['fold'] != fold].index\n",
    "    val_idx = train[train['fold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['fold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    x_train, y_scored_train  = train_df[feature_cols].values, train_df[target_scored_cols].values\n",
    "    x_valid, y_scored_valid  =  valid_df[feature_cols].values, valid_df[target_scored_cols].values\n",
    "\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "\n",
    "\n",
    "    \n",
    "    if preTrain:\n",
    "        print(f\"Beginning pretraining for fold {fold}\")\n",
    "        y_nscored_train = train_df[target_nscored_cols].values\n",
    "        y_nscored_valid = valid_df[target_nscored_cols].values\n",
    "\n",
    "        train_dataset = TrainDataset(x_train, y_nscored_train)\n",
    "        valid_dataset = TrainDataset(x_valid, y_nscored_train)\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        num_targets=len(target_nscored_cols)\n",
    "\n",
    "\n",
    "\n",
    "        model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "        )\n",
    "\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,pct_start=0.1, div_factor=1e4, final_div_factor=1e5,\n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "\n",
    "\n",
    "        early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "        early_step = 0\n",
    "        best_loss = np.inf\n",
    "    \n",
    "        #Main pretrain loop\n",
    "        for epoch in range(PREEPOCHS):\n",
    "        \n",
    "            train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "            print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "            #scheduler.step(valid_loss)\n",
    "        \n",
    "            if valid_loss < best_loss:\n",
    "            \n",
    "                best_loss = valid_loss\n",
    "                torch.save(model.state_dict(), f\"preFOLD{fold}_.pth\")\n",
    "        \n",
    "            elif(EARLY_STOP == True):\n",
    "            \n",
    "                early_step += 1\n",
    "                if (early_step >= early_stopping_steps):\n",
    "                    break\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    if preTrain:\n",
    "        model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=len(target_nscored_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(torch.load(f\"preFOLD{fold}_.pth\"))\n",
    "        model.dense3 = nn.utils.weight_norm(nn.Linear(hidden_sizes[1], len(target_scored_cols)))\n",
    "    else:\n",
    "        model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=len(target_scored_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "        )\n",
    "\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,pct_start=0.1, div_factor=1e4, final_div_factor=1e5,\n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "\n",
    "    train_dataset = TrainDataset(x_train, y_scored_train)\n",
    "    valid_dataset = TrainDataset(x_valid, y_scored_valid)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "   \n",
    "    oof = np.zeros((len(train), target_scored.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    #Main train loop\n",
    "    print(f\"Beginning training for fold {fold}\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "        #scheduler.step(valid_loss)\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=len(target_scored_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target_scored.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_scored_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_scored_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Beginning pretraining for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.718624612864326, valid_loss: 0.6696476638317108\nFOLD: 0, EPOCH: 1, train_loss: 0.34174346704693404, valid_loss: 0.026073926128447056\nFOLD: 0, EPOCH: 2, train_loss: 0.013414711499696268, valid_loss: 0.005990099472304185\nFOLD: 0, EPOCH: 3, train_loss: 0.008961626301136087, valid_loss: 0.00535932354008158\nFOLD: 0, EPOCH: 4, train_loss: 0.008592991021407, valid_loss: 0.005093668354675174\nFOLD: 0, EPOCH: 5, train_loss: 0.008424927864004584, valid_loss: 0.00509935609685878\nFOLD: 0, EPOCH: 6, train_loss: 0.008383225485244217, valid_loss: 0.004941950164114435\nFOLD: 0, EPOCH: 7, train_loss: 0.008333493654123125, valid_loss: 0.00505666349393626\nFOLD: 0, EPOCH: 8, train_loss: 0.008293899022700155, valid_loss: 0.004588379058986902\nFOLD: 0, EPOCH: 9, train_loss: 0.008297105675891918, valid_loss: 0.005674307156975071\nFOLD: 0, EPOCH: 10, train_loss: 0.008241923895719297, valid_loss: 0.005482877604663372\nFOLD: 0, EPOCH: 11, train_loss: 0.008202162105590105, valid_loss: 0.004604998937187095\nFOLD: 0, EPOCH: 12, train_loss: 0.008146800169282976, valid_loss: 0.005239397442589204\nFOLD: 0, EPOCH: 13, train_loss: 0.008100096533513245, valid_loss: 0.005059613613411784\nFOLD: 0, EPOCH: 14, train_loss: 0.008082457215470426, valid_loss: 0.004731062295225759\nFOLD: 0, EPOCH: 15, train_loss: 0.008029658703462166, valid_loss: 0.004630983923561871\nFOLD: 0, EPOCH: 16, train_loss: 0.007995219832724509, valid_loss: 0.004954844790821274\nFOLD: 0, EPOCH: 17, train_loss: 0.00795571248540107, valid_loss: 0.005025858369966348\nFOLD: 0, EPOCH: 18, train_loss: 0.007933890679851174, valid_loss: 0.004764123043666284\nFOLD: 0, EPOCH: 19, train_loss: 0.007883567705421764, valid_loss: 0.004791256195555131\nFOLD: 0, EPOCH: 20, train_loss: 0.007846709066892372, valid_loss: 0.004933894689505299\nFOLD: 0, EPOCH: 21, train_loss: 0.007821061470381478, valid_loss: 0.0048851122458775835\nFOLD: 0, EPOCH: 22, train_loss: 0.007773864973226891, valid_loss: 0.004873245023190975\nFOLD: 0, EPOCH: 23, train_loss: 0.007730168002821943, valid_loss: 0.004912754520773888\nFOLD: 0, EPOCH: 24, train_loss: 0.007670861600405153, valid_loss: 0.004846521109963457\nBeginning training for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.43203949687235493, valid_loss: 0.044183459132909775\nFOLD: 0, EPOCH: 1, train_loss: 0.028024406610604596, valid_loss: 0.021004641739030678\nFOLD: 0, EPOCH: 2, train_loss: 0.022174821837859994, valid_loss: 0.02063326444476843\nFOLD: 0, EPOCH: 3, train_loss: 0.02129039869588964, valid_loss: 0.018209847000737984\nFOLD: 0, EPOCH: 4, train_loss: 0.02095312811434269, valid_loss: 0.018242554739117622\nFOLD: 0, EPOCH: 5, train_loss: 0.020782731583013254, valid_loss: 0.018746564785639446\nFOLD: 0, EPOCH: 6, train_loss: 0.020621954353854936, valid_loss: 0.017928509041666985\nFOLD: 0, EPOCH: 7, train_loss: 0.020467893932672107, valid_loss: 0.017651247791945934\nFOLD: 0, EPOCH: 8, train_loss: 0.02038670183323762, valid_loss: 0.017755480172733467\nFOLD: 0, EPOCH: 9, train_loss: 0.020300753140712485, valid_loss: 0.01768636330962181\nFOLD: 0, EPOCH: 10, train_loss: 0.020229904090656954, valid_loss: 0.01762637651214997\nFOLD: 0, EPOCH: 11, train_loss: 0.020106428686310265, valid_loss: 0.018013780005276203\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 12, train_loss: 0.019821039181860053, valid_loss: 0.01744891858349244\nFOLD: 0, EPOCH: 13, train_loss: 0.019652369377367637, valid_loss: 0.01749128382652998\nFOLD: 0, EPOCH: 14, train_loss: 0.019601152168915552, valid_loss: 0.017742278675238293\nFOLD: 0, EPOCH: 15, train_loss: 0.01954743392108118, valid_loss: 0.0175491056094567\nFOLD: 0, EPOCH: 16, train_loss: 0.01944349563735373, valid_loss: 0.01781406719237566\nFOLD: 0, EPOCH: 17, train_loss: 0.019404131152174053, valid_loss: 0.017377341476579506\nFOLD: 0, EPOCH: 18, train_loss: 0.01934611556284568, valid_loss: 0.01731292065232992\nFOLD: 0, EPOCH: 19, train_loss: 0.01930057936731507, valid_loss: 0.01736471615731716\nFOLD: 0, EPOCH: 20, train_loss: 0.01924444521393846, valid_loss: 0.017394633653263252\nFOLD: 0, EPOCH: 21, train_loss: 0.019150150172850666, valid_loss: 0.01745148127277692\nFOLD: 0, EPOCH: 22, train_loss: 0.019086931613, valid_loss: 0.017361396302779514\nFOLD: 0, EPOCH: 23, train_loss: 0.01900765738066505, valid_loss: 0.01735083945095539\nFOLD: 0, EPOCH: 24, train_loss: 0.01896298353505485, valid_loss: 0.01737457575897376\nFOLD: 0, EPOCH: 25, train_loss: 0.018888908109682447, valid_loss: 0.017355115773777168\nFOLD: 0, EPOCH: 26, train_loss: 0.018802563331144696, valid_loss: 0.0173401761179169\nFOLD: 0, EPOCH: 27, train_loss: 0.018753599539837417, valid_loss: 0.017255553975701332\nFOLD: 0, EPOCH: 28, train_loss: 0.018691657713669187, valid_loss: 0.017242866878708202\nFOLD: 0, EPOCH: 29, train_loss: 0.018638632095911923, valid_loss: 0.017269990717371304\nFOLD: 0, EPOCH: 30, train_loss: 0.01859302939299275, valid_loss: 0.01723239105194807\nFOLD: 0, EPOCH: 31, train_loss: 0.018538074293995604, valid_loss: 0.017228346318006516\nFOLD: 0, EPOCH: 32, train_loss: 0.01851442511029103, valid_loss: 0.017249534217019875\nFOLD: 0, EPOCH: 33, train_loss: 0.01849176931907149, valid_loss: 0.017244204878807068\nFOLD: 0, EPOCH: 34, train_loss: 0.01849940063103157, valid_loss: 0.01723945451279481\nBeginning pretraining for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.7185809980420506, valid_loss: 0.6683434347311655\nFOLD: 1, EPOCH: 1, train_loss: 0.3432049692115363, valid_loss: 0.026300738876064617\nFOLD: 1, EPOCH: 2, train_loss: 0.013562744949012995, valid_loss: 0.005991023577128847\nFOLD: 1, EPOCH: 3, train_loss: 0.008832781548228334, valid_loss: 0.005417578155174851\nFOLD: 1, EPOCH: 4, train_loss: 0.008475985556073925, valid_loss: 0.005179834474499027\nFOLD: 1, EPOCH: 5, train_loss: 0.008369806071962504, valid_loss: 0.004863137689729531\nFOLD: 1, EPOCH: 6, train_loss: 0.008333422421642086, valid_loss: 0.005053864714379112\nFOLD: 1, EPOCH: 7, train_loss: 0.008297748288468403, valid_loss: 0.00494870023491482\nFOLD: 1, EPOCH: 8, train_loss: 0.00841314616777441, valid_loss: 0.0048168582531313104\nFOLD: 1, EPOCH: 9, train_loss: 0.008218595525249839, valid_loss: 0.005218915408477187\nFOLD: 1, EPOCH: 10, train_loss: 0.00820843652164673, valid_loss: 0.004782478092238307\nFOLD: 1, EPOCH: 11, train_loss: 0.008169205256682984, valid_loss: 0.00488282631461819\nFOLD: 1, EPOCH: 12, train_loss: 0.008131399750709534, valid_loss: 0.005290040746331215\nFOLD: 1, EPOCH: 13, train_loss: 0.008084674721912426, valid_loss: 0.004796377150341868\nFOLD: 1, EPOCH: 14, train_loss: 0.008057575357858749, valid_loss: 0.005041406179467837\nFOLD: 1, EPOCH: 15, train_loss: 0.008035226793521467, valid_loss: 0.0052849180065095425\nFOLD: 1, EPOCH: 16, train_loss: 0.00800750104655676, valid_loss: 0.004852745371560256\nFOLD: 1, EPOCH: 17, train_loss: 0.00796576403081417, valid_loss: 0.004912918666377664\nFOLD: 1, EPOCH: 18, train_loss: 0.007946057272527148, valid_loss: 0.0051118348104258375\nFOLD: 1, EPOCH: 19, train_loss: 0.007903263972633901, valid_loss: 0.004981331760063767\nFOLD: 1, EPOCH: 20, train_loss: 0.007865598507444648, valid_loss: 0.004900384461507201\nFOLD: 1, EPOCH: 21, train_loss: 0.007829332272248232, valid_loss: 0.005206162032360832\nFOLD: 1, EPOCH: 22, train_loss: 0.007798653164440218, valid_loss: 0.005041354413454731\nFOLD: 1, EPOCH: 23, train_loss: 0.007758603102582342, valid_loss: 0.005049549431229631\nFOLD: 1, EPOCH: 24, train_loss: 0.007708070128608276, valid_loss: 0.004968702870731552\nBeginning training for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.4202786217279294, valid_loss: 0.03844453270236651\nFOLD: 1, EPOCH: 1, train_loss: 0.027641913360532594, valid_loss: 0.020428241230547428\nFOLD: 1, EPOCH: 2, train_loss: 0.022321168278508326, valid_loss: 0.018840112102528412\nFOLD: 1, EPOCH: 3, train_loss: 0.02141647396937889, valid_loss: 0.018335414119064808\nFOLD: 1, EPOCH: 4, train_loss: 0.02102830441778197, valid_loss: 0.01821311532209317\nFOLD: 1, EPOCH: 5, train_loss: 0.02085466606213766, valid_loss: 0.017962502936522167\nFOLD: 1, EPOCH: 6, train_loss: 0.020626172478146413, valid_loss: 0.017994068873425324\nFOLD: 1, EPOCH: 7, train_loss: 0.02056340210358886, valid_loss: 0.017712075263261795\nFOLD: 1, EPOCH: 8, train_loss: 0.02044356444521862, valid_loss: 0.017646650783717632\nFOLD: 1, EPOCH: 9, train_loss: 0.0203428715467453, valid_loss: 0.017571681179106236\nFOLD: 1, EPOCH: 10, train_loss: 0.02028273560983293, valid_loss: 0.01773276769866546\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 11, train_loss: 0.020120687673197073, valid_loss: 0.01745518824706475\nFOLD: 1, EPOCH: 12, train_loss: 0.019890922286054668, valid_loss: 0.017436533855895203\nFOLD: 1, EPOCH: 13, train_loss: 0.019816865267999032, valid_loss: 0.017724385485053062\nFOLD: 1, EPOCH: 14, train_loss: 0.01974206916330492, valid_loss: 0.017399924186368782\nFOLD: 1, EPOCH: 15, train_loss: 0.019684830570922178, valid_loss: 0.017399456662436325\nFOLD: 1, EPOCH: 16, train_loss: 0.019594441551496002, valid_loss: 0.017416175765295822\nFOLD: 1, EPOCH: 17, train_loss: 0.019498636124326903, valid_loss: 0.017435080992678802\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 18, train_loss: 0.01949085180154618, valid_loss: 0.017310114266971748\nFOLD: 1, EPOCH: 19, train_loss: 0.019349012688240585, valid_loss: 0.017407207128902275\nFOLD: 1, EPOCH: 20, train_loss: 0.019295895088683155, valid_loss: 0.017308408704896767\nFOLD: 1, EPOCH: 21, train_loss: 0.019216572811060092, valid_loss: 0.017287245330711205\nFOLD: 1, EPOCH: 22, train_loss: 0.01918702413711478, valid_loss: 0.017296024598181248\nFOLD: 1, EPOCH: 23, train_loss: 0.019143719390472946, valid_loss: 0.0172630629191796\nFOLD: 1, EPOCH: 24, train_loss: 0.019109721185968202, valid_loss: 0.017279773329695065\nFOLD: 1, EPOCH: 25, train_loss: 0.01906367188648266, valid_loss: 0.017238525363306206\nFOLD: 1, EPOCH: 26, train_loss: 0.019055791637476754, valid_loss: 0.0172805596763889\nFOLD: 1, EPOCH: 27, train_loss: 0.019042627283317203, valid_loss: 0.017249154547850292\nFOLD: 1, EPOCH: 28, train_loss: 0.01900170568157645, valid_loss: 0.01723735003421704\nFOLD: 1, EPOCH: 29, train_loss: 0.018988640814581337, valid_loss: 0.01724266664435466\nFOLD: 1, EPOCH: 30, train_loss: 0.018982378711156985, valid_loss: 0.017259625097115833\nFOLD: 1, EPOCH: 31, train_loss: 0.018972407752538428, valid_loss: 0.017238297189275425\nFOLD: 1, EPOCH: 32, train_loss: 0.01896547054981484, valid_loss: 0.0172457043081522\nFOLD: 1, EPOCH: 33, train_loss: 0.01893932783209226, valid_loss: 0.017241209434966247\nFOLD: 1, EPOCH: 34, train_loss: 0.018979191451388246, valid_loss: 0.01724386556694905\nBeginning pretraining for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.7187546579276815, valid_loss: 0.6717158456643423\nFOLD: 2, EPOCH: 1, train_loss: 0.34253782592713833, valid_loss: 0.026234267900387447\nFOLD: 2, EPOCH: 2, train_loss: 0.01336347550044165, valid_loss: 0.005879474803805351\nFOLD: 2, EPOCH: 3, train_loss: 0.008879027082858718, valid_loss: 0.005159982635329167\nFOLD: 2, EPOCH: 4, train_loss: 0.008588108124540132, valid_loss: 0.004933898725236456\nFOLD: 2, EPOCH: 5, train_loss: 0.008565454442492303, valid_loss: 0.004766345373354852\nFOLD: 2, EPOCH: 6, train_loss: 0.008394137572716264, valid_loss: 0.005092942078287403\nFOLD: 2, EPOCH: 7, train_loss: 0.008353078477632473, valid_loss: 0.004650463350117207\nFOLD: 2, EPOCH: 8, train_loss: 0.008323241025209427, valid_loss: 0.005278262232119839\nFOLD: 2, EPOCH: 9, train_loss: 0.008313597864745295, valid_loss: 0.004948085717236002\nFOLD: 2, EPOCH: 10, train_loss: 0.00830915679826456, valid_loss: 0.005377411764735977\nFOLD: 2, EPOCH: 11, train_loss: 0.008244942983283716, valid_loss: 0.004533026056985061\nFOLD: 2, EPOCH: 12, train_loss: 0.008191488779095165, valid_loss: 0.004624872817657888\nFOLD: 2, EPOCH: 13, train_loss: 0.008179570808458854, valid_loss: 0.004844221208865444\nFOLD: 2, EPOCH: 14, train_loss: 0.008151419693604112, valid_loss: 0.004831866128370166\nFOLD: 2, EPOCH: 15, train_loss: 0.008116932421484414, valid_loss: 0.005072045139968395\nFOLD: 2, EPOCH: 16, train_loss: 0.008085908984546275, valid_loss: 0.005240958649665117\nFOLD: 2, EPOCH: 17, train_loss: 0.008048199938938898, valid_loss: 0.004728945903480053\nFOLD: 2, EPOCH: 18, train_loss: 0.008009707842789152, valid_loss: 0.004520121379755437\nFOLD: 2, EPOCH: 19, train_loss: 0.007965959485291559, valid_loss: 0.004597017347502212\nFOLD: 2, EPOCH: 20, train_loss: 0.007934412371148081, valid_loss: 0.004854173709948857\nFOLD: 2, EPOCH: 21, train_loss: 0.007887213929172824, valid_loss: 0.0048286648622403545\nFOLD: 2, EPOCH: 22, train_loss: 0.007850630304721348, valid_loss: 0.004696793194549779\nFOLD: 2, EPOCH: 23, train_loss: 0.007808917996418827, valid_loss: 0.00478567520622164\nFOLD: 2, EPOCH: 24, train_loss: 0.007769889800864107, valid_loss: 0.004773453770515819\nBeginning training for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.4046605497817783, valid_loss: 0.03323620247344176\nFOLD: 2, EPOCH: 1, train_loss: 0.02677681329934036, valid_loss: 0.024415924524267513\nFOLD: 2, EPOCH: 2, train_loss: 0.022469491095227355, valid_loss: 0.01954738919933637\nFOLD: 2, EPOCH: 3, train_loss: 0.02155748360297259, valid_loss: 0.019161175626019638\nFOLD: 2, EPOCH: 4, train_loss: 0.02125617039992529, valid_loss: 0.01897447369992733\nFOLD: 2, EPOCH: 5, train_loss: 0.020963562597685, valid_loss: 0.018773383771379788\nFOLD: 2, EPOCH: 6, train_loss: 0.02082994541920283, valid_loss: 0.01862648657212655\nFOLD: 2, EPOCH: 7, train_loss: 0.020666799453251502, valid_loss: 0.018640488386154175\nFOLD: 2, EPOCH: 8, train_loss: 0.02055486390257583, valid_loss: 0.018654913641512394\nFOLD: 2, EPOCH: 9, train_loss: 0.02042796550428166, valid_loss: 0.018922326775888603\nFOLD: 2, EPOCH: 10, train_loss: 0.0203569934648626, valid_loss: 0.01817660437275966\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 11, train_loss: 0.020228120498359203, valid_loss: 0.018313860520720482\nFOLD: 2, EPOCH: 12, train_loss: 0.019927781896994394, valid_loss: 0.018057694658637047\nFOLD: 2, EPOCH: 13, train_loss: 0.019770157852155322, valid_loss: 0.018062797375023365\nFOLD: 2, EPOCH: 14, train_loss: 0.019711391507264447, valid_loss: 0.018100795956949394\nFOLD: 2, EPOCH: 15, train_loss: 0.019643350285204017, valid_loss: 0.018074672669172287\nFOLD: 2, EPOCH: 16, train_loss: 0.01961238112519769, valid_loss: 0.018083570214609306\nFOLD: 2, EPOCH: 17, train_loss: 0.01955980429535403, valid_loss: 0.018035337949792545\nFOLD: 2, EPOCH: 18, train_loss: 0.01951565549654119, valid_loss: 0.01803587997953097\nFOLD: 2, EPOCH: 19, train_loss: 0.01948238180621582, valid_loss: 0.018221009833117325\nFOLD: 2, EPOCH: 20, train_loss: 0.01946431792834226, valid_loss: 0.017967337742447853\nFOLD: 2, EPOCH: 21, train_loss: 0.019402589101125214, valid_loss: 0.017983846987287205\nFOLD: 2, EPOCH: 22, train_loss: 0.01937931474736508, valid_loss: 0.017914988100528717\nFOLD: 2, EPOCH: 23, train_loss: 0.019345324103008297, valid_loss: 0.017937952652573586\nFOLD: 2, EPOCH: 24, train_loss: 0.01933398366193561, valid_loss: 0.017888310675819714\nFOLD: 2, EPOCH: 25, train_loss: 0.01929421290097868, valid_loss: 0.01791848987340927\nFOLD: 2, EPOCH: 26, train_loss: 0.019278681179618135, valid_loss: 0.01792333461344242\nFOLD: 2, EPOCH: 27, train_loss: 0.019265737274990362, valid_loss: 0.017960338853299618\nFOLD: 2, EPOCH: 28, train_loss: 0.019242965944987887, valid_loss: 0.017908444938560326\nFOLD: 2, EPOCH: 29, train_loss: 0.019204982611186364, valid_loss: 0.017925051661829155\nFOLD: 2, EPOCH: 30, train_loss: 0.019218420993317577, valid_loss: 0.01792355626821518\nFOLD: 2, EPOCH: 31, train_loss: 0.019206388305653545, valid_loss: 0.017925760708749294\nFOLD: 2, EPOCH: 32, train_loss: 0.01919030025601387, valid_loss: 0.01792268641293049\nFOLD: 2, EPOCH: 33, train_loss: 0.019186828340239385, valid_loss: 0.017925456476708252\nFOLD: 2, EPOCH: 34, train_loss: 0.019186699817724088, valid_loss: 0.01792520998666684\nBeginning pretraining for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.7184127534137053, valid_loss: 0.6691794097423553\nFOLD: 3, EPOCH: 1, train_loss: 0.34086785410695214, valid_loss: 0.025853544163207214\nFOLD: 3, EPOCH: 2, train_loss: 0.013446407447404721, valid_loss: 0.00597560793782274\nFOLD: 3, EPOCH: 3, train_loss: 0.008849303895498024, valid_loss: 0.0053621523547917604\nFOLD: 3, EPOCH: 4, train_loss: 0.008596236077959047, valid_loss: 0.0048668296076357365\nFOLD: 3, EPOCH: 5, train_loss: 0.010270484586191527, valid_loss: 0.004750184870014588\nFOLD: 3, EPOCH: 6, train_loss: 0.008332639288924196, valid_loss: 0.004858395628010233\nFOLD: 3, EPOCH: 7, train_loss: 0.008289263145450284, valid_loss: 0.004724588866035144\nFOLD: 3, EPOCH: 8, train_loss: 0.008276509381282856, valid_loss: 0.00474441796541214\nFOLD: 3, EPOCH: 9, train_loss: 0.00825692484538783, valid_loss: 0.004768667509779334\nFOLD: 3, EPOCH: 10, train_loss: 0.008229772971176049, valid_loss: 0.004940279371415575\nFOLD: 3, EPOCH: 11, train_loss: 0.00820596151820877, valid_loss: 0.0048042344860732555\nFOLD: 3, EPOCH: 12, train_loss: 0.008163507575826609, valid_loss: 0.004933932252849142\nFOLD: 3, EPOCH: 13, train_loss: 0.008133739931508899, valid_loss: 0.0047925959806889296\nFOLD: 3, EPOCH: 14, train_loss: 0.008096775286557042, valid_loss: 0.004910054461409648\nFOLD: 3, EPOCH: 15, train_loss: 0.008072019540978706, valid_loss: 0.005066208075731993\nFOLD: 3, EPOCH: 16, train_loss: 0.008034000666264226, valid_loss: 0.0048158844001591206\nFOLD: 3, EPOCH: 17, train_loss: 0.00798706233720569, valid_loss: 0.004994701826944947\nFOLD: 3, EPOCH: 18, train_loss: 0.007950257030589616, valid_loss: 0.004966354463249445\nFOLD: 3, EPOCH: 19, train_loss: 0.007894113714642385, valid_loss: 0.004857391584664583\nFOLD: 3, EPOCH: 20, train_loss: 0.007835411631009159, valid_loss: 0.004749921150505543\nFOLD: 3, EPOCH: 21, train_loss: 0.007796542294433012, valid_loss: 0.004757577631001671\nFOLD: 3, EPOCH: 22, train_loss: 0.007719250135671566, valid_loss: 0.0047854109822462005\nFOLD: 3, EPOCH: 23, train_loss: 0.007663417785592815, valid_loss: 0.004867425498863061\nFOLD: 3, EPOCH: 24, train_loss: 0.007588125765323639, valid_loss: 0.005074665416032076\nBeginning training for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.42324419177192096, valid_loss: 0.04247472435235977\nFOLD: 3, EPOCH: 1, train_loss: 0.02808949170524583, valid_loss: 0.023048656682173412\nFOLD: 3, EPOCH: 2, train_loss: 0.023649319294182694, valid_loss: 0.01961469246695439\nFOLD: 3, EPOCH: 3, train_loss: 0.0218826582764878, valid_loss: 0.019009863336881\nFOLD: 3, EPOCH: 4, train_loss: 0.021354904343538424, valid_loss: 0.01935388209919135\nFOLD: 3, EPOCH: 5, train_loss: 0.021108296425903544, valid_loss: 0.018196859086553257\nFOLD: 3, EPOCH: 6, train_loss: 0.020896394408362752, valid_loss: 0.017973686568439007\nFOLD: 3, EPOCH: 7, train_loss: 0.020786970634670818, valid_loss: 0.01808408461511135\nFOLD: 3, EPOCH: 8, train_loss: 0.020692249157411212, valid_loss: 0.018130233821769554\nFOLD: 3, EPOCH: 9, train_loss: 0.020564167650745195, valid_loss: 0.018007438940306503\nFOLD: 3, EPOCH: 10, train_loss: 0.02045092934413868, valid_loss: 0.01775479627152284\nFOLD: 3, EPOCH: 11, train_loss: 0.02039167914977845, valid_loss: 0.017672854165236156\nFOLD: 3, EPOCH: 12, train_loss: 0.02033004674183972, valid_loss: 0.017708384431898594\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 13, train_loss: 0.020224989282295984, valid_loss: 0.01771366409957409\nFOLD: 3, EPOCH: 14, train_loss: 0.019940126796855646, valid_loss: 0.017416761877636116\nFOLD: 3, EPOCH: 15, train_loss: 0.01985741132760749, valid_loss: 0.01736489466081063\nFOLD: 3, EPOCH: 16, train_loss: 0.019752346746185246, valid_loss: 0.017350342124700546\nFOLD: 3, EPOCH: 17, train_loss: 0.01968771279515589, valid_loss: 0.017350303009152412\nFOLD: 3, EPOCH: 18, train_loss: 0.019646933302283287, valid_loss: 0.017428566701710224\nFOLD: 3, EPOCH: 19, train_loss: 0.01959647988790975, valid_loss: 0.017419263410071533\nFOLD: 3, EPOCH: 20, train_loss: 0.019511787987807217, valid_loss: 0.017471451001862686\nFOLD: 3, EPOCH: 21, train_loss: 0.019439272582530975, valid_loss: 0.017420692990223568\nFOLD: 3, EPOCH: 22, train_loss: 0.01936939380624715, valid_loss: 0.017192441038787365\nFOLD: 3, EPOCH: 23, train_loss: 0.01931465422625051, valid_loss: 0.01717237972964843\nFOLD: 3, EPOCH: 24, train_loss: 0.019244179732221013, valid_loss: 0.01718697976320982\nFOLD: 3, EPOCH: 25, train_loss: 0.019214402665110195, valid_loss: 0.01718031211445729\nFOLD: 3, EPOCH: 26, train_loss: 0.01913845172042356, valid_loss: 0.01717178151011467\nFOLD: 3, EPOCH: 27, train_loss: 0.019079980604788837, valid_loss: 0.017099248555799324\nFOLD: 3, EPOCH: 28, train_loss: 0.018991535474710605, valid_loss: 0.017074891986946266\nFOLD: 3, EPOCH: 29, train_loss: 0.01893531607792658, valid_loss: 0.017044329705337685\nFOLD: 3, EPOCH: 30, train_loss: 0.01890316074166228, valid_loss: 0.01708612373719613\nFOLD: 3, EPOCH: 31, train_loss: 0.01884630519677611, valid_loss: 0.017072654329240322\nFOLD: 3, EPOCH: 32, train_loss: 0.01881017776973107, valid_loss: 0.01705884871383508\nFOLD: 3, EPOCH: 33, train_loss: 0.018819949589669704, valid_loss: 0.017068874090909958\nFOLD: 3, EPOCH: 34, train_loss: 0.018809912397580987, valid_loss: 0.017053102763990562\nBeginning pretraining for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.7186756940448985, valid_loss: 0.6700522502263387\nFOLD: 4, EPOCH: 1, train_loss: 0.3421949477975859, valid_loss: 0.026343553327023983\nFOLD: 4, EPOCH: 2, train_loss: 0.013459520222728743, valid_loss: 0.005880567400405805\nFOLD: 4, EPOCH: 3, train_loss: 0.008801377460579662, valid_loss: 0.005265887438630064\nFOLD: 4, EPOCH: 4, train_loss: 0.008511916871237405, valid_loss: 0.004812948949014147\nFOLD: 4, EPOCH: 5, train_loss: 0.00839019099743489, valid_loss: 0.004951356289287408\nFOLD: 4, EPOCH: 6, train_loss: 0.00831635034752681, valid_loss: 0.004840834764763713\nFOLD: 4, EPOCH: 7, train_loss: 0.008271985194262336, valid_loss: 0.004637982230633497\nFOLD: 4, EPOCH: 8, train_loss: 0.008234506034675767, valid_loss: 0.004615319970374306\nFOLD: 4, EPOCH: 9, train_loss: 0.008183493717190097, valid_loss: 0.004971573905398448\nFOLD: 4, EPOCH: 10, train_loss: 0.008163493400549187, valid_loss: 0.00596502252543966\nFOLD: 4, EPOCH: 11, train_loss: 0.008127280825968175, valid_loss: 0.005055028169105451\nFOLD: 4, EPOCH: 12, train_loss: 0.00807096777648172, valid_loss: 0.005260184019183119\nFOLD: 4, EPOCH: 13, train_loss: 0.008036054835161743, valid_loss: 0.0049636206434418755\nFOLD: 4, EPOCH: 14, train_loss: 0.008009272750795764, valid_loss: 0.005019846061865489\nFOLD: 4, EPOCH: 15, train_loss: 0.007968428298173583, valid_loss: 0.004945281504963835\nFOLD: 4, EPOCH: 16, train_loss: 0.007949062595691751, valid_loss: 0.004952665806437532\nFOLD: 4, EPOCH: 17, train_loss: 0.007917049573734403, valid_loss: 0.00475418094235162\nFOLD: 4, EPOCH: 18, train_loss: 0.007889976711286342, valid_loss: 0.00491890109454592\nFOLD: 4, EPOCH: 19, train_loss: 0.007861542310017873, valid_loss: 0.00484263935747246\nFOLD: 4, EPOCH: 20, train_loss: 0.007834353223990868, valid_loss: 0.0048097662317256136\nFOLD: 4, EPOCH: 21, train_loss: 0.00778542276855339, valid_loss: 0.0047473494584361715\nFOLD: 4, EPOCH: 22, train_loss: 0.007755452000042971, valid_loss: 0.004937548811237018\nFOLD: 4, EPOCH: 23, train_loss: 0.0077248697497827165, valid_loss: 0.0047281937052806216\nFOLD: 4, EPOCH: 24, train_loss: 0.007674729054355446, valid_loss: 0.004722713337590297\nBeginning training for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.4252541101373294, valid_loss: 0.04353565784792105\nFOLD: 4, EPOCH: 1, train_loss: 0.0277227816884132, valid_loss: 0.020789381116628647\nFOLD: 4, EPOCH: 2, train_loss: 0.022163493558764458, valid_loss: 0.01969960207740466\nFOLD: 4, EPOCH: 3, train_loss: 0.021221115473000443, valid_loss: 0.018946880164245766\nFOLD: 4, EPOCH: 4, train_loss: 0.020934569923316732, valid_loss: 0.018335909272233646\nFOLD: 4, EPOCH: 5, train_loss: 0.020686873180024764, valid_loss: 0.01840008795261383\nFOLD: 4, EPOCH: 6, train_loss: 0.020597688360687566, valid_loss: 0.018211583296457928\nFOLD: 4, EPOCH: 7, train_loss: 0.02042848342920051, valid_loss: 0.018087386153638363\nFOLD: 4, EPOCH: 8, train_loss: 0.020322908373440012, valid_loss: 0.01812446117401123\nFOLD: 4, EPOCH: 9, train_loss: 0.02028739090789767, valid_loss: 0.018173943273723125\nFOLD: 4, EPOCH: 10, train_loss: 0.020189625649329496, valid_loss: 0.018070086215933163\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 11, train_loss: 0.020131822018062368, valid_loss: 0.018024813073376816\nFOLD: 4, EPOCH: 12, train_loss: 0.019839086569845676, valid_loss: 0.017771433107554913\nFOLD: 4, EPOCH: 13, train_loss: 0.019745437757057303, valid_loss: 0.017841611057519913\nFOLD: 4, EPOCH: 14, train_loss: 0.01965578902951058, valid_loss: 0.017728858937819798\nFOLD: 4, EPOCH: 15, train_loss: 0.019576636855216586, valid_loss: 0.017758887882033985\nFOLD: 4, EPOCH: 16, train_loss: 0.019506906268789488, valid_loss: 0.017756026859084766\nFOLD: 4, EPOCH: 17, train_loss: 0.019468739135738683, valid_loss: 0.01779870254298051\nFOLD: 4, EPOCH: 18, train_loss: 0.01936742597643067, valid_loss: 0.017807564698159695\nFOLD: 4, EPOCH: 19, train_loss: 0.019394810256712577, valid_loss: 0.017669970480104286\nFOLD: 4, EPOCH: 20, train_loss: 0.019287272794720006, valid_loss: 0.0177629254758358\nFOLD: 4, EPOCH: 21, train_loss: 0.0192486914970419, valid_loss: 0.017631271854043007\nFOLD: 4, EPOCH: 22, train_loss: 0.019189747653025037, valid_loss: 0.01769686583429575\nFOLD: 4, EPOCH: 23, train_loss: 0.01909998815287562, valid_loss: 0.017569640030463535\nFOLD: 4, EPOCH: 24, train_loss: 0.01907135217505343, valid_loss: 0.017658849246799946\nFOLD: 4, EPOCH: 25, train_loss: 0.018994205631315708, valid_loss: 0.01770140944669644\nFOLD: 4, EPOCH: 26, train_loss: 0.018944810856791103, valid_loss: 0.017649398185312748\nFOLD: 4, EPOCH: 27, train_loss: 0.01887368060210172, valid_loss: 0.017553630595405895\nFOLD: 4, EPOCH: 28, train_loss: 0.018779141718850416, valid_loss: 0.017598239394525688\nFOLD: 4, EPOCH: 29, train_loss: 0.018731825875447077, valid_loss: 0.017516189875702064\nFOLD: 4, EPOCH: 30, train_loss: 0.01870050812687944, valid_loss: 0.017530225527783234\nFOLD: 4, EPOCH: 31, train_loss: 0.018680237891042933, valid_loss: 0.017521430117388565\nFOLD: 4, EPOCH: 32, train_loss: 0.01864262516884243, valid_loss: 0.017527433422704537\nFOLD: 4, EPOCH: 33, train_loss: 0.01862109490834615, valid_loss: 0.01752677746117115\nFOLD: 4, EPOCH: 34, train_loss: 0.018589163034716073, valid_loss: 0.017529336425165336\nBeginning pretraining for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.7187535271925085, valid_loss: 0.6721392869949341\nFOLD: 5, EPOCH: 1, train_loss: 0.3446126595577773, valid_loss: 0.02649024222046137\nFOLD: 5, EPOCH: 2, train_loss: 0.013313035836772007, valid_loss: 0.005919980816543102\nFOLD: 5, EPOCH: 3, train_loss: 0.008791657712529688, valid_loss: 0.005292222835123539\nFOLD: 5, EPOCH: 4, train_loss: 0.008467068402644466, valid_loss: 0.005163679442678888\nFOLD: 5, EPOCH: 5, train_loss: 0.00836286915685324, valid_loss: 0.004794929875060916\nFOLD: 5, EPOCH: 6, train_loss: 0.00837013651342953, valid_loss: 0.005937544861808419\nFOLD: 5, EPOCH: 7, train_loss: 0.008334443096400183, valid_loss: 0.004899486356104414\nFOLD: 5, EPOCH: 8, train_loss: 0.008225729373995872, valid_loss: 0.005387066397815943\nFOLD: 5, EPOCH: 9, train_loss: 0.008188594885937431, valid_loss: 0.005352496014287074\nFOLD: 5, EPOCH: 10, train_loss: 0.00819776368820492, valid_loss: 0.005251008629178007\nFOLD: 5, EPOCH: 11, train_loss: 0.008158713428522734, valid_loss: 0.004839301652585466\nFOLD: 5, EPOCH: 12, train_loss: 0.008111325599362744, valid_loss: 0.005228679860010743\nFOLD: 5, EPOCH: 13, train_loss: 0.008075962733367787, valid_loss: 0.00571432220749557\nFOLD: 5, EPOCH: 14, train_loss: 0.00803820724489496, valid_loss: 0.004774026184653242\nFOLD: 5, EPOCH: 15, train_loss: 0.008010391638997723, valid_loss: 0.004733839460338156\nFOLD: 5, EPOCH: 16, train_loss: 0.007969116819474627, valid_loss: 0.004882105315725009\nFOLD: 5, EPOCH: 17, train_loss: 0.007924163563396125, valid_loss: 0.004794753156602383\nFOLD: 5, EPOCH: 18, train_loss: 0.007896154803936095, valid_loss: 0.004719279396037261\nFOLD: 5, EPOCH: 19, train_loss: 0.00785130334963255, valid_loss: 0.004747727420181036\nFOLD: 5, EPOCH: 20, train_loss: 0.007815138477941646, valid_loss: 0.004930351162329316\nFOLD: 5, EPOCH: 21, train_loss: 0.007782648714697536, valid_loss: 0.004749767715111375\nFOLD: 5, EPOCH: 22, train_loss: 0.007730986594277269, valid_loss: 0.00502691666285197\nFOLD: 5, EPOCH: 23, train_loss: 0.007672623334014241, valid_loss: 0.004870885595058401\nFOLD: 5, EPOCH: 24, train_loss: 0.007623245747869506, valid_loss: 0.004995181690901518\nBeginning training for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.4016806244411889, valid_loss: 0.031685084414978824\nFOLD: 5, EPOCH: 1, train_loss: 0.026585532735814068, valid_loss: 0.02059793844819069\nFOLD: 5, EPOCH: 2, train_loss: 0.022305031361825326, valid_loss: 0.019206659868359566\nFOLD: 5, EPOCH: 3, train_loss: 0.021493154742261943, valid_loss: 0.018776968742410343\nFOLD: 5, EPOCH: 4, train_loss: 0.021203455734340584, valid_loss: 0.01870423958947261\nFOLD: 5, EPOCH: 5, train_loss: 0.020933295107063127, valid_loss: 0.018382991664111614\nFOLD: 5, EPOCH: 6, train_loss: 0.02078072600724066, valid_loss: 0.018293272083004315\nFOLD: 5, EPOCH: 7, train_loss: 0.020642752266105485, valid_loss: 0.01822978009780248\nFOLD: 5, EPOCH: 8, train_loss: 0.020503013668691412, valid_loss: 0.01852578731874625\nFOLD: 5, EPOCH: 9, train_loss: 0.020412109737448832, valid_loss: 0.018279582262039185\nFOLD: 5, EPOCH: 10, train_loss: 0.020303068792118746, valid_loss: 0.018095257071157295\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 11, train_loss: 0.020195152884458795, valid_loss: 0.017929979289571445\nFOLD: 5, EPOCH: 12, train_loss: 0.019838913617765203, valid_loss: 0.01792000016818444\nFOLD: 5, EPOCH: 13, train_loss: 0.019721582970198465, valid_loss: 0.01794474261502425\nFOLD: 5, EPOCH: 14, train_loss: 0.019652863087899545, valid_loss: 0.017929671953121822\nFOLD: 5, EPOCH: 15, train_loss: 0.019569981131045258, valid_loss: 0.017948675900697708\nFOLD: 5, EPOCH: 16, train_loss: 0.019524342161329353, valid_loss: 0.01800705896069606\nFOLD: 5, EPOCH: 17, train_loss: 0.019496068248853964, valid_loss: 0.017839438592394192\nFOLD: 5, EPOCH: 18, train_loss: 0.01946305494536372, valid_loss: 0.017917160565654438\nFOLD: 5, EPOCH: 19, train_loss: 0.019416772924801883, valid_loss: 0.017856731700400513\nFOLD: 5, EPOCH: 20, train_loss: 0.019373137777780786, valid_loss: 0.017729894258081913\nFOLD: 5, EPOCH: 21, train_loss: 0.019362858332255307, valid_loss: 0.01779873886456092\nFOLD: 5, EPOCH: 22, train_loss: 0.019319679314161047, valid_loss: 0.017748091369867325\nFOLD: 5, EPOCH: 23, train_loss: 0.019271672648542067, valid_loss: 0.017773070683081944\nFOLD: 5, EPOCH: 24, train_loss: 0.019237335442620164, valid_loss: 0.017816674585143726\nFOLD: 5, EPOCH: 25, train_loss: 0.019221200607717037, valid_loss: 0.017773407821853954\nFOLD: 5, EPOCH: 26, train_loss: 0.019199613098274258, valid_loss: 0.017714338687558968\nFOLD: 5, EPOCH: 27, train_loss: 0.019177858936874306, valid_loss: 0.017751300397018593\nFOLD: 5, EPOCH: 28, train_loss: 0.019187596955281848, valid_loss: 0.01772629873206218\nFOLD: 5, EPOCH: 29, train_loss: 0.019151733037741744, valid_loss: 0.017716143590708573\nFOLD: 5, EPOCH: 30, train_loss: 0.019144254791385987, valid_loss: 0.017733321525156498\nFOLD: 5, EPOCH: 31, train_loss: 0.01912175606497947, valid_loss: 0.017725632525980473\nFOLD: 5, EPOCH: 32, train_loss: 0.019111505435670122, valid_loss: 0.017726446501910686\nFOLD: 5, EPOCH: 33, train_loss: 0.01911596060894868, valid_loss: 0.01772990357130766\nFOLD: 5, EPOCH: 34, train_loss: 0.01910055023344124, valid_loss: 0.017727224466701347\nBeginning pretraining for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.7184056481894325, valid_loss: 0.6714448630809784\nFOLD: 6, EPOCH: 1, train_loss: 0.3433530092896784, valid_loss: 0.025858217229445774\nFOLD: 6, EPOCH: 2, train_loss: 0.013330132974421276, valid_loss: 0.005781887952859203\nFOLD: 6, EPOCH: 3, train_loss: 0.008841721834067036, valid_loss: 0.00513689514870445\nFOLD: 6, EPOCH: 4, train_loss: 0.008453430854441488, valid_loss: 0.004781051073223352\nFOLD: 6, EPOCH: 5, train_loss: 0.008347287417992073, valid_loss: 0.004839565216874083\nFOLD: 6, EPOCH: 6, train_loss: 0.008265695493558751, valid_loss: 0.0048762729081014795\nFOLD: 6, EPOCH: 7, train_loss: 0.008264930028577937, valid_loss: 0.0054558551249404745\nFOLD: 6, EPOCH: 8, train_loss: 0.008216427337816534, valid_loss: 0.004722291448463996\nFOLD: 6, EPOCH: 9, train_loss: 0.008156552404055701, valid_loss: 0.004995360582446058\nFOLD: 6, EPOCH: 10, train_loss: 0.008150678196483675, valid_loss: 0.005124166064585249\nFOLD: 6, EPOCH: 11, train_loss: 0.008102020385729916, valid_loss: 0.004420352013160785\nFOLD: 6, EPOCH: 12, train_loss: 0.008085199686534265, valid_loss: 0.0050814275940259295\nFOLD: 6, EPOCH: 13, train_loss: 0.00803880371591624, valid_loss: 0.0048319226286063595\nFOLD: 6, EPOCH: 14, train_loss: 0.007998203694382134, valid_loss: 0.00523666253623863\nFOLD: 6, EPOCH: 15, train_loss: 0.007964200011509307, valid_loss: 0.004563309795533617\nFOLD: 6, EPOCH: 16, train_loss: 0.007938487331985551, valid_loss: 0.0045961434952914715\nFOLD: 6, EPOCH: 17, train_loss: 0.007898321553297779, valid_loss: 0.004638529382646084\nFOLD: 6, EPOCH: 18, train_loss: 0.007862717885633601, valid_loss: 0.004523906468724211\nFOLD: 6, EPOCH: 19, train_loss: 0.007841446039759937, valid_loss: 0.004627564378703634\nFOLD: 6, EPOCH: 20, train_loss: 0.00779247798902147, valid_loss: 0.00484335096552968\nFOLD: 6, EPOCH: 21, train_loss: 0.007751153425916153, valid_loss: 0.004631799878552556\nFOLD: 6, EPOCH: 22, train_loss: 0.007717199705760269, valid_loss: 0.004603060893714428\nFOLD: 6, EPOCH: 23, train_loss: 0.007664422107422177, valid_loss: 0.004678627398485939\nFOLD: 6, EPOCH: 24, train_loss: 0.007632470785585397, valid_loss: 0.0047012098754445715\nBeginning training for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.41343445582863164, valid_loss: 0.03514642578860124\nFOLD: 6, EPOCH: 1, train_loss: 0.02732157679822515, valid_loss: 0.021459287653366726\nFOLD: 6, EPOCH: 2, train_loss: 0.022256337161011556, valid_loss: 0.018583550428350765\nFOLD: 6, EPOCH: 3, train_loss: 0.021435793434434077, valid_loss: 0.018030333643158276\nFOLD: 6, EPOCH: 4, train_loss: 0.02103853866677074, valid_loss: 0.01785802251348893\nFOLD: 6, EPOCH: 5, train_loss: 0.020843957769958413, valid_loss: 0.017655006609857082\nFOLD: 6, EPOCH: 6, train_loss: 0.020693838760695037, valid_loss: 0.017662515863776207\nFOLD: 6, EPOCH: 7, train_loss: 0.02057122132357429, valid_loss: 0.017544087953865528\nFOLD: 6, EPOCH: 8, train_loss: 0.020462169585859075, valid_loss: 0.01754559235026439\nFOLD: 6, EPOCH: 9, train_loss: 0.02040225760463406, valid_loss: 0.01767299883067608\nFOLD: 6, EPOCH: 10, train_loss: 0.02030415865866577, valid_loss: 0.017366239180167515\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 11, train_loss: 0.020089141805382335, valid_loss: 0.017142072630425293\nFOLD: 6, EPOCH: 12, train_loss: 0.01989263274213847, valid_loss: 0.017264958160618942\nFOLD: 6, EPOCH: 13, train_loss: 0.01977847270010149, valid_loss: 0.017168651955823105\nFOLD: 6, EPOCH: 14, train_loss: 0.01967929435126922, valid_loss: 0.017133464726308983\nFOLD: 6, EPOCH: 15, train_loss: 0.019640672458883596, valid_loss: 0.0171219181890289\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 16, train_loss: 0.019526619464159012, valid_loss: 0.017139170008401077\nFOLD: 6, EPOCH: 17, train_loss: 0.019402272451449844, valid_loss: 0.017087151917318504\nFOLD: 6, EPOCH: 18, train_loss: 0.01936264012885444, valid_loss: 0.01705406854550044\nFOLD: 6, EPOCH: 19, train_loss: 0.019326649496660513, valid_loss: 0.0169945377856493\nFOLD: 6, EPOCH: 20, train_loss: 0.019276170498308015, valid_loss: 0.017024670417110126\nFOLD: 6, EPOCH: 21, train_loss: 0.019228134468636093, valid_loss: 0.016972404283781845\nFOLD: 6, EPOCH: 22, train_loss: 0.019201033858253676, valid_loss: 0.017036097434659798\nFOLD: 6, EPOCH: 23, train_loss: 0.019155503415009555, valid_loss: 0.016933312950034935\nFOLD: 6, EPOCH: 24, train_loss: 0.01910506221739685, valid_loss: 0.016965586381653946\nFOLD: 6, EPOCH: 25, train_loss: 0.019103926606476307, valid_loss: 0.016935911029577255\nFOLD: 6, EPOCH: 26, train_loss: 0.019062263617182478, valid_loss: 0.016921815151969593\nFOLD: 6, EPOCH: 27, train_loss: 0.019044086117954814, valid_loss: 0.016949946681658428\nFOLD: 6, EPOCH: 28, train_loss: 0.01904005781911752, valid_loss: 0.016896365210413933\nFOLD: 6, EPOCH: 29, train_loss: 0.019024548668633488, valid_loss: 0.016882969066500664\nFOLD: 6, EPOCH: 30, train_loss: 0.01902324311873492, valid_loss: 0.0169267018015186\nFOLD: 6, EPOCH: 31, train_loss: 0.018964452504673424, valid_loss: 0.016914318936566513\nFOLD: 6, EPOCH: 32, train_loss: 0.018986942520474687, valid_loss: 0.016911761835217476\nFOLD: 6, EPOCH: 33, train_loss: 0.01897474696092746, valid_loss: 0.016917775695522625\nFOLD: 6, EPOCH: 34, train_loss: 0.018981685478459385, valid_loss: 0.016901240684092045\nBeginning pretraining for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.7192230189547819, valid_loss: 0.6702582041422526\nFOLD: 0, EPOCH: 1, train_loss: 0.34143926926395474, valid_loss: 0.02573994640260935\nFOLD: 0, EPOCH: 2, train_loss: 0.013405173332156503, valid_loss: 0.0060028797791649895\nFOLD: 0, EPOCH: 3, train_loss: 0.008870444178362103, valid_loss: 0.005293138325214386\nFOLD: 0, EPOCH: 4, train_loss: 0.008555239860844962, valid_loss: 0.004822538156683247\nFOLD: 0, EPOCH: 5, train_loss: 0.008449550183928189, valid_loss: 0.004910684345910947\nFOLD: 0, EPOCH: 6, train_loss: 0.008423865581040873, valid_loss: 0.004775530115390818\nFOLD: 0, EPOCH: 7, train_loss: 0.008319331839790238, valid_loss: 0.004676920633452634\nFOLD: 0, EPOCH: 8, train_loss: 0.008281728252768517, valid_loss: 0.005169326361889641\nFOLD: 0, EPOCH: 9, train_loss: 0.00826498694882235, valid_loss: 0.005904587761809428\nFOLD: 0, EPOCH: 10, train_loss: 0.008208595188882421, valid_loss: 0.005244219520439704\nFOLD: 0, EPOCH: 11, train_loss: 0.008172147701877882, valid_loss: 0.007216181373223662\nFOLD: 0, EPOCH: 12, train_loss: 0.008146754014031851, valid_loss: 0.004907640473296245\nFOLD: 0, EPOCH: 13, train_loss: 0.008104500278611393, valid_loss: 0.004844112244124214\nFOLD: 0, EPOCH: 14, train_loss: 0.008082151180133224, valid_loss: 0.004923220258206129\nFOLD: 0, EPOCH: 15, train_loss: 0.008045389152625027, valid_loss: 0.004732312285341322\nFOLD: 0, EPOCH: 16, train_loss: 0.008006951689501019, valid_loss: 0.0048346245506157475\nFOLD: 0, EPOCH: 17, train_loss: 0.007988497792907497, valid_loss: 0.004876078261683385\nFOLD: 0, EPOCH: 18, train_loss: 0.007949429386131027, valid_loss: 0.004822883599748214\nFOLD: 0, EPOCH: 19, train_loss: 0.007908429731340968, valid_loss: 0.005161668872460723\nFOLD: 0, EPOCH: 20, train_loss: 0.007872010526411673, valid_loss: 0.004782556633775433\nFOLD: 0, EPOCH: 21, train_loss: 0.007836795658530557, valid_loss: 0.00502805116896828\nFOLD: 0, EPOCH: 22, train_loss: 0.007804323302801041, valid_loss: 0.0048551352228969336\nFOLD: 0, EPOCH: 23, train_loss: 0.007757354322273065, valid_loss: 0.004781054488072793\nFOLD: 0, EPOCH: 24, train_loss: 0.007739114109426737, valid_loss: 0.004845010815188289\nBeginning training for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.42441378106527466, valid_loss: 0.042476837212840714\nFOLD: 0, EPOCH: 1, train_loss: 0.02819308791966999, valid_loss: 0.020880289375782013\nFOLD: 0, EPOCH: 2, train_loss: 0.022534182076068485, valid_loss: 0.026174264959990978\nFOLD: 0, EPOCH: 3, train_loss: 0.021443148088805816, valid_loss: 0.01841560968508323\nFOLD: 0, EPOCH: 4, train_loss: 0.021183626726269722, valid_loss: 0.01869141620894273\nFOLD: 0, EPOCH: 5, train_loss: 0.020899708711487407, valid_loss: 0.017966749146580696\nFOLD: 0, EPOCH: 6, train_loss: 0.020781506784260273, valid_loss: 0.0183104258030653\nFOLD: 0, EPOCH: 7, train_loss: 0.02057105374029454, valid_loss: 0.017906909498075645\nFOLD: 0, EPOCH: 8, train_loss: 0.020484678556813914, valid_loss: 0.01776852427671353\nFOLD: 0, EPOCH: 9, train_loss: 0.020355739411624038, valid_loss: 0.01773600559681654\nFOLD: 0, EPOCH: 10, train_loss: 0.020260904258226648, valid_loss: 0.01787949912250042\nFOLD: 0, EPOCH: 11, train_loss: 0.02023544754175579, valid_loss: 0.017677439687152702\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 12, train_loss: 0.019917504916734555, valid_loss: 0.01757588206479947\nFOLD: 0, EPOCH: 13, train_loss: 0.01979315861621324, valid_loss: 0.017413930346568424\nFOLD: 0, EPOCH: 14, train_loss: 0.019710142329773483, valid_loss: 0.01764657876143853\nFOLD: 0, EPOCH: 15, train_loss: 0.019620849203099224, valid_loss: 0.017582468688488007\nFOLD: 0, EPOCH: 16, train_loss: 0.019539788365364075, valid_loss: 0.017511331165830295\nFOLD: 0, EPOCH: 17, train_loss: 0.01951989290468833, valid_loss: 0.0175357557212313\nFOLD: 0, EPOCH: 18, train_loss: 0.01945721248493475, valid_loss: 0.01739118279268344\nFOLD: 0, EPOCH: 19, train_loss: 0.01936355950858663, valid_loss: 0.01748753587404887\nFOLD: 0, EPOCH: 20, train_loss: 0.019305990241906223, valid_loss: 0.017335300023357075\nFOLD: 0, EPOCH: 21, train_loss: 0.019231886219452408, valid_loss: 0.017307733185589314\nFOLD: 0, EPOCH: 22, train_loss: 0.019185308640932337, valid_loss: 0.017316829102734726\nFOLD: 0, EPOCH: 23, train_loss: 0.019107525456039345, valid_loss: 0.017273529743154842\nFOLD: 0, EPOCH: 24, train_loss: 0.01907359145800857, valid_loss: 0.017323120186726253\nFOLD: 0, EPOCH: 25, train_loss: 0.018983386950019526, valid_loss: 0.01726920499155919\nFOLD: 0, EPOCH: 26, train_loss: 0.018898293540319976, valid_loss: 0.017205753674109776\nFOLD: 0, EPOCH: 27, train_loss: 0.01883823950501049, valid_loss: 0.017246372376879055\nFOLD: 0, EPOCH: 28, train_loss: 0.018769736292169374, valid_loss: 0.01721599077184995\nFOLD: 0, EPOCH: 29, train_loss: 0.01868867567356895, valid_loss: 0.017202898859977722\nFOLD: 0, EPOCH: 30, train_loss: 0.018652478123412412, valid_loss: 0.017218324976662796\nFOLD: 0, EPOCH: 31, train_loss: 0.018586587544311497, valid_loss: 0.017237657370666664\nFOLD: 0, EPOCH: 32, train_loss: 0.018580450566814226, valid_loss: 0.01722324515382449\nFOLD: 0, EPOCH: 33, train_loss: 0.01854730846689028, valid_loss: 0.017225082342823345\nFOLD: 0, EPOCH: 34, train_loss: 0.018526051248259404, valid_loss: 0.017222775767246883\nBeginning pretraining for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.7192021029836991, valid_loss: 0.6677590409914652\nFOLD: 1, EPOCH: 1, train_loss: 0.34142231371472864, valid_loss: 0.027006235904991627\nFOLD: 1, EPOCH: 2, train_loss: 0.013686230707475367, valid_loss: 0.0060880939320971566\nFOLD: 1, EPOCH: 3, train_loss: 0.008809141075129019, valid_loss: 0.005484848748892546\nFOLD: 1, EPOCH: 4, train_loss: 0.0084750473170596, valid_loss: 0.005197294832517703\nFOLD: 1, EPOCH: 5, train_loss: 0.008366360246916027, valid_loss: 0.00492028232353429\nFOLD: 1, EPOCH: 6, train_loss: 0.00833129696547985, valid_loss: 0.0049043637700378895\nFOLD: 1, EPOCH: 7, train_loss: 0.008283398059361121, valid_loss: 0.0052201775057862205\nFOLD: 1, EPOCH: 8, train_loss: 0.008215120719636189, valid_loss: 0.0049331048503518105\nFOLD: 1, EPOCH: 9, train_loss: 0.008185757173444419, valid_loss: 0.005213963178296884\nFOLD: 1, EPOCH: 10, train_loss: 0.008160535684403251, valid_loss: 0.004936004678408305\nFOLD: 1, EPOCH: 11, train_loss: 0.008120776167796814, valid_loss: 0.004730910373230775\nFOLD: 1, EPOCH: 12, train_loss: 0.00810234303421834, valid_loss: 0.0053710066713392735\nFOLD: 1, EPOCH: 13, train_loss: 0.008052500841372153, valid_loss: 0.0048299965759118395\nFOLD: 1, EPOCH: 14, train_loss: 0.008022386594401562, valid_loss: 0.00478282175026834\nFOLD: 1, EPOCH: 15, train_loss: 0.007989193504566656, valid_loss: 0.004824399249628186\nFOLD: 1, EPOCH: 16, train_loss: 0.007976076319156325, valid_loss: 0.00497530276576678\nFOLD: 1, EPOCH: 17, train_loss: 0.00793761171071845, valid_loss: 0.004921822498242061\nFOLD: 1, EPOCH: 18, train_loss: 0.007902970181449372, valid_loss: 0.004957211281483372\nFOLD: 1, EPOCH: 19, train_loss: 0.00787332808763227, valid_loss: 0.005002876355623205\nFOLD: 1, EPOCH: 20, train_loss: 0.007855770383578013, valid_loss: 0.005148947549362977\nFOLD: 1, EPOCH: 21, train_loss: 0.007793290157090215, valid_loss: 0.005343526524181168\nFOLD: 1, EPOCH: 22, train_loss: 0.007778885130606154, valid_loss: 0.0048411579336971045\nFOLD: 1, EPOCH: 23, train_loss: 0.007719566182726447, valid_loss: 0.005016568660115202\nFOLD: 1, EPOCH: 24, train_loss: 0.007683942984680042, valid_loss: 0.004911267043401797\nBeginning training for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.41428705173380237, valid_loss: 0.03791178266207377\nFOLD: 1, EPOCH: 1, train_loss: 0.027578454026404547, valid_loss: 0.020231506476799648\nFOLD: 1, EPOCH: 2, train_loss: 0.022477399722179946, valid_loss: 0.018399714181820553\nFOLD: 1, EPOCH: 3, train_loss: 0.02148043320459478, valid_loss: 0.01815899151066939\nFOLD: 1, EPOCH: 4, train_loss: 0.021087400669998983, valid_loss: 0.01819904490063588\nFOLD: 1, EPOCH: 5, train_loss: 0.020838082658455652, valid_loss: 0.01786086366822322\nFOLD: 1, EPOCH: 6, train_loss: 0.020690711215138435, valid_loss: 0.018032291283210117\nFOLD: 1, EPOCH: 7, train_loss: 0.020584842847550616, valid_loss: 0.01798337573806445\nFOLD: 1, EPOCH: 8, train_loss: 0.02046066736254622, valid_loss: 0.01778096364190181\nFOLD: 1, EPOCH: 9, train_loss: 0.020382035030599904, valid_loss: 0.01779829493413369\nFOLD: 1, EPOCH: 10, train_loss: 0.02031189082738231, valid_loss: 0.017688672679166\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 11, train_loss: 0.020171950954724762, valid_loss: 0.01748860285927852\nFOLD: 1, EPOCH: 12, train_loss: 0.019866772160372314, valid_loss: 0.01756215530137221\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 13, train_loss: 0.019724028842414126, valid_loss: 0.01753442920744419\nFOLD: 1, EPOCH: 14, train_loss: 0.019616773954647428, valid_loss: 0.017411349962155025\nFOLD: 1, EPOCH: 15, train_loss: 0.019499521373825913, valid_loss: 0.01749680284410715\nFOLD: 1, EPOCH: 16, train_loss: 0.0194666310596992, valid_loss: 0.017425224805871647\nFOLD: 1, EPOCH: 17, train_loss: 0.019414656247724506, valid_loss: 0.017338978747526806\nFOLD: 1, EPOCH: 18, train_loss: 0.019388129715533817, valid_loss: 0.017561778115729492\nFOLD: 1, EPOCH: 19, train_loss: 0.019340970334323013, valid_loss: 0.017328296477595966\nFOLD: 1, EPOCH: 20, train_loss: 0.019319786690175533, valid_loss: 0.01735158668210109\nFOLD: 1, EPOCH: 21, train_loss: 0.019284598803257242, valid_loss: 0.017329602191845577\nFOLD: 1, EPOCH: 22, train_loss: 0.019240546861992162, valid_loss: 0.017355539525548618\nFOLD: 1, EPOCH: 23, train_loss: 0.019227506483302396, valid_loss: 0.01729735266417265\nFOLD: 1, EPOCH: 24, train_loss: 0.019221146426656666, valid_loss: 0.017329385814567406\nFOLD: 1, EPOCH: 25, train_loss: 0.01916727051138878, valid_loss: 0.017326117493212223\nFOLD: 1, EPOCH: 26, train_loss: 0.019127251854275957, valid_loss: 0.017309686169028282\nFOLD: 1, EPOCH: 27, train_loss: 0.019104281549944598, valid_loss: 0.017320132193466026\nFOLD: 1, EPOCH: 28, train_loss: 0.019103883053450024, valid_loss: 0.017292036054035027\nFOLD: 1, EPOCH: 29, train_loss: 0.019077645286041146, valid_loss: 0.017308115338285763\nFOLD: 1, EPOCH: 30, train_loss: 0.019082156393457863, valid_loss: 0.017284837861855824\nFOLD: 1, EPOCH: 31, train_loss: 0.01905881575144389, valid_loss: 0.01731394821157058\nFOLD: 1, EPOCH: 32, train_loss: 0.019050112980253556, valid_loss: 0.017304985783994198\nFOLD: 1, EPOCH: 33, train_loss: 0.01904479244395214, valid_loss: 0.017296535583833855\nFOLD: 1, EPOCH: 34, train_loss: 0.019067920963553822, valid_loss: 0.017298993344108265\nBeginning pretraining for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.7193162336068994, valid_loss: 0.67090771595637\nFOLD: 2, EPOCH: 1, train_loss: 0.34152807503500404, valid_loss: 0.027207938333352406\nFOLD: 2, EPOCH: 2, train_loss: 0.013466354870401761, valid_loss: 0.005925654045616587\nFOLD: 2, EPOCH: 3, train_loss: 0.008894155733287334, valid_loss: 0.005304038679848115\nFOLD: 2, EPOCH: 4, train_loss: 0.008570906651370665, valid_loss: 0.004927960379670064\nFOLD: 2, EPOCH: 5, train_loss: 0.008476075866971822, valid_loss: 0.0046619882341474295\nFOLD: 2, EPOCH: 6, train_loss: 0.008447247762780856, valid_loss: 0.004863068539028366\nFOLD: 2, EPOCH: 7, train_loss: 0.00833551336408538, valid_loss: 0.004746388372344275\nFOLD: 2, EPOCH: 8, train_loss: 0.008308050551396958, valid_loss: 0.0050257365219295025\nFOLD: 2, EPOCH: 9, train_loss: 0.008279471439035499, valid_loss: 0.0048159523867070675\nFOLD: 2, EPOCH: 10, train_loss: 0.008252234800773509, valid_loss: 0.0057063948673506575\nFOLD: 2, EPOCH: 11, train_loss: 0.008220267879283604, valid_loss: 0.0056863777960340185\nFOLD: 2, EPOCH: 12, train_loss: 0.008178806630894542, valid_loss: 0.004841207293793559\nFOLD: 2, EPOCH: 13, train_loss: 0.008143680827582584, valid_loss: 0.004689938117129107\nFOLD: 2, EPOCH: 14, train_loss: 0.00812404669876046, valid_loss: 0.0047956714406609535\nFOLD: 2, EPOCH: 15, train_loss: 0.008088039754725555, valid_loss: 0.004944992406914632\nFOLD: 2, EPOCH: 16, train_loss: 0.008037358084145714, valid_loss: 0.004933409470443924\nFOLD: 2, EPOCH: 17, train_loss: 0.007989825228886568, valid_loss: 0.004699584639941652\nFOLD: 2, EPOCH: 18, train_loss: 0.007970337810761789, valid_loss: 0.004740385105833411\nFOLD: 2, EPOCH: 19, train_loss: 0.007937398930901991, valid_loss: 0.004769719361017148\nFOLD: 2, EPOCH: 20, train_loss: 0.007892316522295861, valid_loss: 0.004765607571850221\nFOLD: 2, EPOCH: 21, train_loss: 0.00786671694368124, valid_loss: 0.004861152886102597\nFOLD: 2, EPOCH: 22, train_loss: 0.00784439325113507, valid_loss: 0.004777995985932648\nFOLD: 2, EPOCH: 23, train_loss: 0.007804215022855822, valid_loss: 0.004762200211795668\nFOLD: 2, EPOCH: 24, train_loss: 0.0077538250616806395, valid_loss: 0.004748909772994618\nBeginning training for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.4348981344305417, valid_loss: 0.048099117974440254\nFOLD: 2, EPOCH: 1, train_loss: 0.028755286544123116, valid_loss: 0.02226537000387907\nFOLD: 2, EPOCH: 2, train_loss: 0.02238909067476497, valid_loss: 0.019808631700774033\nFOLD: 2, EPOCH: 3, train_loss: 0.021388552535106155, valid_loss: 0.019048881406585377\nFOLD: 2, EPOCH: 4, train_loss: 0.021270871381549275, valid_loss: 0.019132354917625587\nFOLD: 2, EPOCH: 5, train_loss: 0.020799077992491862, valid_loss: 0.018669053601721924\nFOLD: 2, EPOCH: 6, train_loss: 0.020654094843741727, valid_loss: 0.01830615196377039\nFOLD: 2, EPOCH: 7, train_loss: 0.02042439881273929, valid_loss: 0.018441779538989067\nFOLD: 2, EPOCH: 8, train_loss: 0.02030762767090517, valid_loss: 0.018269631390770275\nFOLD: 2, EPOCH: 9, train_loss: 0.020208863760618603, valid_loss: 0.01860563674320777\nFOLD: 2, EPOCH: 10, train_loss: 0.02016244165818481, valid_loss: 0.018484074311951797\nFOLD: 2, EPOCH: 11, train_loss: 0.020039311445811215, valid_loss: 0.01836346120884021\nFOLD: 2, EPOCH: 12, train_loss: 0.01993238865671789, valid_loss: 0.018353831643859547\nFOLD: 2, EPOCH: 13, train_loss: 0.019918865414665025, valid_loss: 0.018463164878388245\nFOLD: 2, EPOCH: 14, train_loss: 0.019795518916319397, valid_loss: 0.018041829578578472\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 15, train_loss: 0.0195876105743296, valid_loss: 0.018097767295936745\nFOLD: 2, EPOCH: 16, train_loss: 0.019339052357656115, valid_loss: 0.018071494686106842\nFOLD: 2, EPOCH: 17, train_loss: 0.019222925033639458, valid_loss: 0.018073988767961662\nFOLD: 2, EPOCH: 18, train_loss: 0.019149282344562167, valid_loss: 0.017953332513570786\nFOLD: 2, EPOCH: 19, train_loss: 0.019056381329017526, valid_loss: 0.01834929237763087\nFOLD: 2, EPOCH: 20, train_loss: 0.01898275184280732, valid_loss: 0.018084336693088215\nFOLD: 2, EPOCH: 21, train_loss: 0.018902376787189173, valid_loss: 0.018039342015981674\nFOLD: 2, EPOCH: 22, train_loss: 0.018866099417209625, valid_loss: 0.018100311358769734\nFOLD: 2, EPOCH: 23, train_loss: 0.0187921362435993, valid_loss: 0.01806633857389291\nFOLD: 2, EPOCH: 24, train_loss: 0.01869034838369664, valid_loss: 0.01807582409431537\nFOLD: 2, EPOCH: 25, train_loss: 0.018628846908755162, valid_loss: 0.01800154522061348\nFOLD: 2, EPOCH: 26, train_loss: 0.018523148261010647, valid_loss: 0.01796279971798261\nFOLD: 2, EPOCH: 27, train_loss: 0.018423920223379835, valid_loss: 0.01797314950575431\nFOLD: 2, EPOCH: 28, train_loss: 0.01839701152023147, valid_loss: 0.017940109906097252\nFOLD: 2, EPOCH: 29, train_loss: 0.018329596804345354, valid_loss: 0.01796835536758105\nFOLD: 2, EPOCH: 30, train_loss: 0.01828738707391655, valid_loss: 0.017914843435088795\nFOLD: 2, EPOCH: 31, train_loss: 0.018207446398103937, valid_loss: 0.017943100072443485\nFOLD: 2, EPOCH: 32, train_loss: 0.01820476249079494, valid_loss: 0.017948149393002193\nFOLD: 2, EPOCH: 33, train_loss: 0.018149492111714446, valid_loss: 0.017924888680378597\nFOLD: 2, EPOCH: 34, train_loss: 0.01815570781336111, valid_loss: 0.017927915789186954\nBeginning pretraining for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.7192603682770449, valid_loss: 0.670280396938324\nFOLD: 3, EPOCH: 1, train_loss: 0.3414389433010536, valid_loss: 0.02586760340879361\nFOLD: 3, EPOCH: 2, train_loss: 0.013829959200366455, valid_loss: 0.0059627743903547525\nFOLD: 3, EPOCH: 3, train_loss: 0.008828326594084501, valid_loss: 0.00519881312114497\nFOLD: 3, EPOCH: 4, train_loss: 0.008532866150798166, valid_loss: 0.00505832660322388\nFOLD: 3, EPOCH: 5, train_loss: 0.008463597582543598, valid_loss: 0.005079216634233792\nFOLD: 3, EPOCH: 6, train_loss: 0.008379522642558989, valid_loss: 0.004936549657334884\nFOLD: 3, EPOCH: 7, train_loss: 0.008345202378490391, valid_loss: 0.005129475379362702\nFOLD: 3, EPOCH: 8, train_loss: 0.008289943147888956, valid_loss: 0.00565717873784403\nFOLD: 3, EPOCH: 9, train_loss: 0.00828287929899114, valid_loss: 0.004615840424473087\nFOLD: 3, EPOCH: 10, train_loss: 0.008237330544301692, valid_loss: 0.0052556738567849\nFOLD: 3, EPOCH: 11, train_loss: 0.008224563741618219, valid_loss: 0.005062824891259273\nFOLD: 3, EPOCH: 12, train_loss: 0.008188865380361676, valid_loss: 0.005129173320407669\nFOLD: 3, EPOCH: 13, train_loss: 0.008143523050581707, valid_loss: 0.0047885402260969085\nFOLD: 3, EPOCH: 14, train_loss: 0.008108873440719703, valid_loss: 0.00483407755382359\nFOLD: 3, EPOCH: 15, train_loss: 0.008087509078904986, valid_loss: 0.004871754751851161\nFOLD: 3, EPOCH: 16, train_loss: 0.008054978720953359, valid_loss: 0.004718210625772675\nFOLD: 3, EPOCH: 17, train_loss: 0.008005060337703018, valid_loss: 0.004965497646480799\nFOLD: 3, EPOCH: 18, train_loss: 0.007978660136680393, valid_loss: 0.004637640745689471\nFOLD: 3, EPOCH: 19, train_loss: 0.007934461114928126, valid_loss: 0.0049123189722498255\nFOLD: 3, EPOCH: 20, train_loss: 0.00791022741674062, valid_loss: 0.004892703223352631\nFOLD: 3, EPOCH: 21, train_loss: 0.007876669960644315, valid_loss: 0.004807396791875362\nFOLD: 3, EPOCH: 22, train_loss: 0.007831825148862074, valid_loss: 0.004875655906895797\nFOLD: 3, EPOCH: 23, train_loss: 0.007790796020451714, valid_loss: 0.004876015242189169\nFOLD: 3, EPOCH: 24, train_loss: 0.00775096209391075, valid_loss: 0.004841964536656936\nBeginning training for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.4219968592857613, valid_loss: 0.04126574471592903\nFOLD: 3, EPOCH: 1, train_loss: 0.027562796631280112, valid_loss: 0.020678793390591938\nFOLD: 3, EPOCH: 2, train_loss: 0.022169349386411553, valid_loss: 0.019661633918682735\nFOLD: 3, EPOCH: 3, train_loss: 0.021262847489732152, valid_loss: 0.01855343673378229\nFOLD: 3, EPOCH: 4, train_loss: 0.021004499374505353, valid_loss: 0.01824114875247081\nFOLD: 3, EPOCH: 5, train_loss: 0.02085821226458339, valid_loss: 0.01813728113969167\nFOLD: 3, EPOCH: 6, train_loss: 0.020672411712653497, valid_loss: 0.01783735180894534\nFOLD: 3, EPOCH: 7, train_loss: 0.020600578035501874, valid_loss: 0.01775995052109162\nFOLD: 3, EPOCH: 8, train_loss: 0.020479446989210212, valid_loss: 0.01782241618881623\nFOLD: 3, EPOCH: 9, train_loss: 0.02031889245571459, valid_loss: 0.017471894311408203\nFOLD: 3, EPOCH: 10, train_loss: 0.020318240475128677, valid_loss: 0.01769830472767353\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 11, train_loss: 0.02025461010634899, valid_loss: 0.01760254210482041\nFOLD: 3, EPOCH: 12, train_loss: 0.019974238294012406, valid_loss: 0.01742695861806472\nFOLD: 3, EPOCH: 13, train_loss: 0.01984123239184127, valid_loss: 0.01752208701024453\nFOLD: 3, EPOCH: 14, train_loss: 0.019747139557319528, valid_loss: 0.01742146008958419\nFOLD: 3, EPOCH: 15, train_loss: 0.019731199697536582, valid_loss: 0.017437056948741276\nFOLD: 3, EPOCH: 16, train_loss: 0.019643095102818572, valid_loss: 0.017229977995157242\nFOLD: 3, EPOCH: 17, train_loss: 0.019582299022551847, valid_loss: 0.017268433546026547\nFOLD: 3, EPOCH: 18, train_loss: 0.01952907716964974, valid_loss: 0.017234578107794125\nFOLD: 3, EPOCH: 19, train_loss: 0.019458285368540707, valid_loss: 0.01726308837532997\nFOLD: 3, EPOCH: 20, train_loss: 0.01944329370470608, valid_loss: 0.01729530810068051\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 21, train_loss: 0.019401103696402383, valid_loss: 0.017228884312013786\nFOLD: 3, EPOCH: 22, train_loss: 0.01923510306240881, valid_loss: 0.017217277238766353\nFOLD: 3, EPOCH: 23, train_loss: 0.019177874057170224, valid_loss: 0.017311896507938702\nFOLD: 3, EPOCH: 24, train_loss: 0.019120062591836733, valid_loss: 0.01720162418981393\nFOLD: 3, EPOCH: 25, train_loss: 0.019090365234981563, valid_loss: 0.01712424587458372\nFOLD: 3, EPOCH: 26, train_loss: 0.019071216311524895, valid_loss: 0.017152067894736927\nFOLD: 3, EPOCH: 27, train_loss: 0.019028323921648896, valid_loss: 0.01710346279044946\nFOLD: 3, EPOCH: 28, train_loss: 0.018996326198034427, valid_loss: 0.017139654606580734\nFOLD: 3, EPOCH: 29, train_loss: 0.01897279600448468, valid_loss: 0.01712468235443036\nFOLD: 3, EPOCH: 30, train_loss: 0.018957949243485928, valid_loss: 0.017121779111524422\nFOLD: 3, EPOCH: 31, train_loss: 0.01894374202717753, valid_loss: 0.01713746382544438\nFOLD: 3, EPOCH: 32, train_loss: 0.018955473514164194, valid_loss: 0.017123628097275894\nFOLD: 3, EPOCH: 33, train_loss: 0.01894956870990641, valid_loss: 0.017121933711071808\nFOLD: 3, EPOCH: 34, train_loss: 0.01894740667194128, valid_loss: 0.017127329483628273\nBeginning pretraining for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.7193888601134805, valid_loss: 0.669641395409902\nFOLD: 4, EPOCH: 1, train_loss: 0.34077969368766337, valid_loss: 0.027414413789908092\nFOLD: 4, EPOCH: 2, train_loss: 0.013463857200215845, valid_loss: 0.005916270272185405\nFOLD: 4, EPOCH: 3, train_loss: 0.008817836258779554, valid_loss: 0.0052376076734314365\nFOLD: 4, EPOCH: 4, train_loss: 0.0085200055173653, valid_loss: 0.004934621586774786\nFOLD: 4, EPOCH: 5, train_loss: 0.008391783709692605, valid_loss: 0.004810448425511519\nFOLD: 4, EPOCH: 6, train_loss: 0.008359246106599183, valid_loss: 0.004900780816872914\nFOLD: 4, EPOCH: 7, train_loss: 0.008334039190017125, valid_loss: 0.0046639520054062205\nFOLD: 4, EPOCH: 8, train_loss: 0.008212019853732166, valid_loss: 0.004915726759160559\nFOLD: 4, EPOCH: 9, train_loss: 0.00820101979736458, valid_loss: 0.004777217051014304\nFOLD: 4, EPOCH: 10, train_loss: 0.008162890615708688, valid_loss: 0.0058120843799163895\nFOLD: 4, EPOCH: 11, train_loss: 0.008121270234422648, valid_loss: 0.005194603155056636\nFOLD: 4, EPOCH: 12, train_loss: 0.008090480545754819, valid_loss: 0.005289666121825576\nFOLD: 4, EPOCH: 13, train_loss: 0.008071446832378997, valid_loss: 0.005312814610078931\nFOLD: 4, EPOCH: 14, train_loss: 0.008021667887292364, valid_loss: 0.004937811754643917\nFOLD: 4, EPOCH: 15, train_loss: 0.00797995092237697, valid_loss: 0.004819512988130252\nFOLD: 4, EPOCH: 16, train_loss: 0.007959601387162419, valid_loss: 0.004719210633387168\nFOLD: 4, EPOCH: 17, train_loss: 0.007922800038667285, valid_loss: 0.00473653773466746\nFOLD: 4, EPOCH: 18, train_loss: 0.007882273755967617, valid_loss: 0.004953945831706126\nFOLD: 4, EPOCH: 19, train_loss: 0.007865309208521949, valid_loss: 0.004949480295181274\nFOLD: 4, EPOCH: 20, train_loss: 0.007821174105629325, valid_loss: 0.004799436777830124\nFOLD: 4, EPOCH: 21, train_loss: 0.007796323282972854, valid_loss: 0.004733251174911857\nFOLD: 4, EPOCH: 22, train_loss: 0.007751443382243023, valid_loss: 0.0047748153253148\nFOLD: 4, EPOCH: 23, train_loss: 0.007715663886355127, valid_loss: 0.004770615758995215\nFOLD: 4, EPOCH: 24, train_loss: 0.007665914551847998, valid_loss: 0.0048218836697439356\nBeginning training for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.42437739067656155, valid_loss: 0.0441963542252779\nFOLD: 4, EPOCH: 1, train_loss: 0.02806110154179966, valid_loss: 0.021335801109671593\nFOLD: 4, EPOCH: 2, train_loss: 0.022478883154690266, valid_loss: 0.018795857205986977\nFOLD: 4, EPOCH: 3, train_loss: 0.021453564767451846, valid_loss: 0.019398498038450878\nFOLD: 4, EPOCH: 4, train_loss: 0.021002387025338763, valid_loss: 0.01838928957780202\nFOLD: 4, EPOCH: 5, train_loss: 0.020811664324034664, valid_loss: 0.0185988104591767\nFOLD: 4, EPOCH: 6, train_loss: 0.020667501013068593, valid_loss: 0.018254967406392097\nFOLD: 4, EPOCH: 7, train_loss: 0.020464392598061, valid_loss: 0.01819483811656634\nFOLD: 4, EPOCH: 8, train_loss: 0.02041245164240108, valid_loss: 0.018012816707293194\nFOLD: 4, EPOCH: 9, train_loss: 0.02028834474656512, valid_loss: 0.01817046043773492\nFOLD: 4, EPOCH: 10, train_loss: 0.020219269537312144, valid_loss: 0.017970296554267406\nFOLD: 4, EPOCH: 11, train_loss: 0.020125501570017898, valid_loss: 0.017953377837936085\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 12, train_loss: 0.019955435329500365, valid_loss: 0.017805336664120357\nFOLD: 4, EPOCH: 13, train_loss: 0.019768801079515147, valid_loss: 0.01783668560286363\nFOLD: 4, EPOCH: 14, train_loss: 0.0196901387808954, valid_loss: 0.017783803554872673\nFOLD: 4, EPOCH: 15, train_loss: 0.019591800758943838, valid_loss: 0.01789902150630951\nFOLD: 4, EPOCH: 16, train_loss: 0.019537830122691745, valid_loss: 0.017752664474149544\nFOLD: 4, EPOCH: 17, train_loss: 0.01948947092408643, valid_loss: 0.017740128251413505\nFOLD: 4, EPOCH: 18, train_loss: 0.019409187028513235, valid_loss: 0.01763042900711298\nFOLD: 4, EPOCH: 19, train_loss: 0.01936873492291745, valid_loss: 0.01771666668355465\nFOLD: 4, EPOCH: 20, train_loss: 0.019302672542193356, valid_loss: 0.01770393457263708\nFOLD: 4, EPOCH: 21, train_loss: 0.019240663112962946, valid_loss: 0.017656441467503708\nFOLD: 4, EPOCH: 22, train_loss: 0.019198890008470592, valid_loss: 0.0176644679158926\nFOLD: 4, EPOCH: 23, train_loss: 0.01913868559195715, valid_loss: 0.017668123977879684\nFOLD: 4, EPOCH: 24, train_loss: 0.019096432911122545, valid_loss: 0.017695740486184757\nFOLD: 4, EPOCH: 25, train_loss: 0.01904528642840245, valid_loss: 0.017651112129290897\nFOLD: 4, EPOCH: 26, train_loss: 0.01892374861328041, valid_loss: 0.017607466938594978\nFOLD: 4, EPOCH: 27, train_loss: 0.018898767693077818, valid_loss: 0.01754883366326491\nFOLD: 4, EPOCH: 28, train_loss: 0.0188292552552679, valid_loss: 0.017539674416184425\nFOLD: 4, EPOCH: 29, train_loss: 0.018760608673533973, valid_loss: 0.01753008148322503\nFOLD: 4, EPOCH: 30, train_loss: 0.018698693472234643, valid_loss: 0.017523284380634625\nFOLD: 4, EPOCH: 31, train_loss: 0.018680566045291284, valid_loss: 0.017521371133625507\nFOLD: 4, EPOCH: 32, train_loss: 0.018641825107967153, valid_loss: 0.01752828558286031\nFOLD: 4, EPOCH: 33, train_loss: 0.018626989303704572, valid_loss: 0.017523241539796192\nFOLD: 4, EPOCH: 34, train_loss: 0.0186339916442247, valid_loss: 0.017520544740060966\nBeginning pretraining for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.7193080064128426, valid_loss: 0.6713186303774515\nFOLD: 5, EPOCH: 1, train_loss: 0.34153449360062094, valid_loss: 0.02605308871716261\nFOLD: 5, EPOCH: 2, train_loss: 0.013426769908298464, valid_loss: 0.0059723355031261844\nFOLD: 5, EPOCH: 3, train_loss: 0.008834797341157408, valid_loss: 0.005311162599051992\nFOLD: 5, EPOCH: 4, train_loss: 0.008470034760916057, valid_loss: 0.005075283348560333\nFOLD: 5, EPOCH: 5, train_loss: 0.008357134291573483, valid_loss: 0.004801647815232475\nFOLD: 5, EPOCH: 6, train_loss: 0.008318744572427343, valid_loss: 0.005078556947410107\nFOLD: 5, EPOCH: 7, train_loss: 0.008264343774713138, valid_loss: 0.005442423125108083\nFOLD: 5, EPOCH: 8, train_loss: 0.00821986174046555, valid_loss: 0.004716893502821525\nFOLD: 5, EPOCH: 9, train_loss: 0.008186248391318847, valid_loss: 0.004853868080923955\nFOLD: 5, EPOCH: 10, train_loss: 0.008172102245118688, valid_loss: 0.005078276386484504\nFOLD: 5, EPOCH: 11, train_loss: 0.00813921053400811, valid_loss: 0.005367451502631108\nFOLD: 5, EPOCH: 12, train_loss: 0.008077340063584201, valid_loss: 0.0049276757054030895\nFOLD: 5, EPOCH: 13, train_loss: 0.008062702877556576, valid_loss: 0.005023023967320721\nFOLD: 5, EPOCH: 14, train_loss: 0.008024844258804531, valid_loss: 0.005196588191514214\nFOLD: 5, EPOCH: 15, train_loss: 0.008006890907007106, valid_loss: 0.0049184684952100115\nFOLD: 5, EPOCH: 16, train_loss: 0.007973298005869283, valid_loss: 0.005144383292645216\nFOLD: 5, EPOCH: 17, train_loss: 0.007927674444063622, valid_loss: 0.004918765199060242\nFOLD: 5, EPOCH: 18, train_loss: 0.00789283517309848, valid_loss: 0.004692797859509786\nFOLD: 5, EPOCH: 19, train_loss: 0.007849005215308246, valid_loss: 0.004822670326878627\nFOLD: 5, EPOCH: 20, train_loss: 0.007818262120160986, valid_loss: 0.005065178032964468\nFOLD: 5, EPOCH: 21, train_loss: 0.0077727964860113226, valid_loss: 0.004823888341585795\nFOLD: 5, EPOCH: 22, train_loss: 0.007737035958972924, valid_loss: 0.004836117848753929\nFOLD: 5, EPOCH: 23, train_loss: 0.007694595287937452, valid_loss: 0.004787804636483391\nFOLD: 5, EPOCH: 24, train_loss: 0.007644496734856683, valid_loss: 0.0048169590688000126\nBeginning training for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.4057144456926514, valid_loss: 0.03562195971608162\nFOLD: 5, EPOCH: 1, train_loss: 0.026859797646894175, valid_loss: 0.020724991336464882\nFOLD: 5, EPOCH: 2, train_loss: 0.022559513885746982, valid_loss: 0.019299123746653397\nFOLD: 5, EPOCH: 3, train_loss: 0.02166126564364223, valid_loss: 0.018813027689854305\nFOLD: 5, EPOCH: 4, train_loss: 0.021217781940803808, valid_loss: 0.01867553840080897\nFOLD: 5, EPOCH: 5, train_loss: 0.021018964348032194, valid_loss: 0.01837878643224637\nFOLD: 5, EPOCH: 6, train_loss: 0.020853332901263937, valid_loss: 0.018910216478010018\nFOLD: 5, EPOCH: 7, train_loss: 0.02074724720681415, valid_loss: 0.018460604672630627\nFOLD: 5, EPOCH: 8, train_loss: 0.020558118327137304, valid_loss: 0.018308893777430058\nFOLD: 5, EPOCH: 9, train_loss: 0.020492510705748024, valid_loss: 0.018163979053497314\nFOLD: 5, EPOCH: 10, train_loss: 0.020366415162296855, valid_loss: 0.018091972296436627\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 11, train_loss: 0.02023448460899732, valid_loss: 0.017820284701883793\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 12, train_loss: 0.01993996074751896, valid_loss: 0.017886308021843433\nFOLD: 5, EPOCH: 13, train_loss: 0.019763793029329357, valid_loss: 0.017979543035229046\nFOLD: 5, EPOCH: 14, train_loss: 0.019679496722186312, valid_loss: 0.01803465684254964\nFOLD: 5, EPOCH: 15, train_loss: 0.019586377886726576, valid_loss: 0.017799734758834045\nFOLD: 5, EPOCH: 16, train_loss: 0.01955552169067018, valid_loss: 0.017775334417819977\nFOLD: 5, EPOCH: 17, train_loss: 0.019516178680693403, valid_loss: 0.01784402256210645\nFOLD: 5, EPOCH: 18, train_loss: 0.019470411407596925, valid_loss: 0.017814919352531433\nFOLD: 5, EPOCH: 19, train_loss: 0.019416020525728956, valid_loss: 0.017907992626229923\nFOLD: 5, EPOCH: 20, train_loss: 0.019416702527771976, valid_loss: 0.01777038009216388\nFOLD: 5, EPOCH: 21, train_loss: 0.019367351580192062, valid_loss: 0.0177454703177015\nFOLD: 5, EPOCH: 22, train_loss: 0.019330858965130412, valid_loss: 0.017738350356618564\nFOLD: 5, EPOCH: 23, train_loss: 0.019320193185087514, valid_loss: 0.017677228276928265\nFOLD: 5, EPOCH: 24, train_loss: 0.019284622524591052, valid_loss: 0.0177615179369847\nFOLD: 5, EPOCH: 25, train_loss: 0.019266638904809952, valid_loss: 0.017764681329329807\nFOLD: 5, EPOCH: 26, train_loss: 0.019248691716176623, valid_loss: 0.017731542078157265\nFOLD: 5, EPOCH: 27, train_loss: 0.019228611963198465, valid_loss: 0.017709191267689068\nFOLD: 5, EPOCH: 28, train_loss: 0.019227222813402906, valid_loss: 0.01768491044640541\nFOLD: 5, EPOCH: 29, train_loss: 0.019194830702069926, valid_loss: 0.017709359526634216\nFOLD: 5, EPOCH: 30, train_loss: 0.019206452731262234, valid_loss: 0.017707571076850098\nFOLD: 5, EPOCH: 31, train_loss: 0.0191823217250845, valid_loss: 0.01769532822072506\nFOLD: 5, EPOCH: 32, train_loss: 0.019157815121990794, valid_loss: 0.017703779662648838\nFOLD: 5, EPOCH: 33, train_loss: 0.01916678951067083, valid_loss: 0.017705638892948627\nFOLD: 5, EPOCH: 34, train_loss: 0.019146663848968112, valid_loss: 0.017704556385676067\nBeginning pretraining for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.7194013332619387, valid_loss: 0.6693776647249857\nFOLD: 6, EPOCH: 1, train_loss: 0.33992312125423374, valid_loss: 0.02522304467856884\nFOLD: 6, EPOCH: 2, train_loss: 0.013386012986302376, valid_loss: 0.005743766746794184\nFOLD: 6, EPOCH: 3, train_loss: 0.00873514634611852, valid_loss: 0.0050449427993347245\nFOLD: 6, EPOCH: 4, train_loss: 0.008435319259981899, valid_loss: 0.0047554677197088795\nFOLD: 6, EPOCH: 5, train_loss: 0.008424473405980012, valid_loss: 0.004767519111434619\nFOLD: 6, EPOCH: 6, train_loss: 0.008283661979743662, valid_loss: 0.004463754245080054\nFOLD: 6, EPOCH: 7, train_loss: 0.008215895388275385, valid_loss: 0.004726044367998838\nFOLD: 6, EPOCH: 8, train_loss: 0.008190470281988382, valid_loss: 0.005099125284080704\nFOLD: 6, EPOCH: 9, train_loss: 0.008126197848469019, valid_loss: 0.005026062019169331\nFOLD: 6, EPOCH: 10, train_loss: 0.008105713613879155, valid_loss: 0.004468729137443006\nFOLD: 6, EPOCH: 11, train_loss: 0.008073622525176582, valid_loss: 0.004732680274173617\nFOLD: 6, EPOCH: 12, train_loss: 0.008046151755158515, valid_loss: 0.004920681783308585\nFOLD: 6, EPOCH: 13, train_loss: 0.008009048356838962, valid_loss: 0.0044048480146254105\nFOLD: 6, EPOCH: 14, train_loss: 0.007963140303378595, valid_loss: 0.004505211681437989\nFOLD: 6, EPOCH: 15, train_loss: 0.00794662203749313, valid_loss: 0.004717667043829958\nFOLD: 6, EPOCH: 16, train_loss: 0.007903522181817713, valid_loss: 0.004521960004543264\nFOLD: 6, EPOCH: 17, train_loss: 0.007880091886310017, valid_loss: 0.004558355780318379\nFOLD: 6, EPOCH: 18, train_loss: 0.007847554036690033, valid_loss: 0.004587193174908559\nFOLD: 6, EPOCH: 19, train_loss: 0.007811867727843278, valid_loss: 0.004811647270495693\nFOLD: 6, EPOCH: 20, train_loss: 0.007802215213065638, valid_loss: 0.0046524712815880775\nFOLD: 6, EPOCH: 21, train_loss: 0.0077582091668292, valid_loss: 0.004785156575962901\nFOLD: 6, EPOCH: 22, train_loss: 0.007728923262809129, valid_loss: 0.0048915627412498\nFOLD: 6, EPOCH: 23, train_loss: 0.007692234056508716, valid_loss: 0.004740243622412284\nFOLD: 6, EPOCH: 24, train_loss: 0.007659103666596553, valid_loss: 0.004645207043116291\nBeginning training for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.40719827505595546, valid_loss: 0.037956902757287025\nFOLD: 6, EPOCH: 1, train_loss: 0.027124948087422288, valid_loss: 0.02035130001604557\nFOLD: 6, EPOCH: 2, train_loss: 0.022492970066035494, valid_loss: 0.022490983518461388\nFOLD: 6, EPOCH: 3, train_loss: 0.021582981955040905, valid_loss: 0.01859085417042176\nFOLD: 6, EPOCH: 4, train_loss: 0.021232634015819606, valid_loss: 0.018029659676055115\nFOLD: 6, EPOCH: 5, train_loss: 0.02107406369246104, valid_loss: 0.0180759293337663\nFOLD: 6, EPOCH: 6, train_loss: 0.020821537658133927, valid_loss: 0.018033360751966637\nFOLD: 6, EPOCH: 7, train_loss: 0.020756264052846852, valid_loss: 0.01761728214720885\nFOLD: 6, EPOCH: 8, train_loss: 0.02059284657897318, valid_loss: 0.017633488091329735\nFOLD: 6, EPOCH: 9, train_loss: 0.020505482330918312, valid_loss: 0.017565523274242878\nFOLD: 6, EPOCH: 10, train_loss: 0.02045322023332119, valid_loss: 0.017346854011217754\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 11, train_loss: 0.020317222649121985, valid_loss: 0.017757943520943325\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 12, train_loss: 0.0199700717759483, valid_loss: 0.0171689223498106\nFOLD: 6, EPOCH: 13, train_loss: 0.01984385959804058, valid_loss: 0.017339616703490417\nFOLD: 6, EPOCH: 14, train_loss: 0.019780968535034096, valid_loss: 0.01721081603318453\nFOLD: 6, EPOCH: 15, train_loss: 0.01971701077897759, valid_loss: 0.01715508320679267\nFOLD: 6, EPOCH: 16, train_loss: 0.01966731869341696, valid_loss: 0.01708579125503699\nFOLD: 6, EPOCH: 17, train_loss: 0.019650456933852506, valid_loss: 0.017214539771278698\nFOLD: 6, EPOCH: 18, train_loss: 0.019601246177711907, valid_loss: 0.01703848782926798\nFOLD: 6, EPOCH: 19, train_loss: 0.019558872972779414, valid_loss: 0.017202571344872315\nFOLD: 6, EPOCH: 20, train_loss: 0.01950809501988046, valid_loss: 0.01713093773772319\nFOLD: 6, EPOCH: 21, train_loss: 0.019498538938077056, valid_loss: 0.017186913018425305\nFOLD: 6, EPOCH: 22, train_loss: 0.019463258083252347, valid_loss: 0.017054297340412933\nFOLD: 6, EPOCH: 23, train_loss: 0.01943158484337961, valid_loss: 0.017095454037189484\nFOLD: 6, EPOCH: 24, train_loss: 0.019361900713513878, valid_loss: 0.01711982550720374\nFOLD: 6, EPOCH: 25, train_loss: 0.019361279083087164, valid_loss: 0.01710089420278867\nFOLD: 6, EPOCH: 26, train_loss: 0.019359359955962968, valid_loss: 0.017040724245210487\nFOLD: 6, EPOCH: 27, train_loss: 0.019331142470678863, valid_loss: 0.017036217575271923\nFOLD: 6, EPOCH: 28, train_loss: 0.019299519467441476, valid_loss: 0.017032347929974396\nFOLD: 6, EPOCH: 29, train_loss: 0.01928309816867113, valid_loss: 0.017076933446029823\nFOLD: 6, EPOCH: 30, train_loss: 0.019272408009890246, valid_loss: 0.01705017654846112\nFOLD: 6, EPOCH: 31, train_loss: 0.01928064106580089, valid_loss: 0.017040935655434925\nFOLD: 6, EPOCH: 32, train_loss: 0.019248154781320515, valid_loss: 0.01704500491420428\nFOLD: 6, EPOCH: 33, train_loss: 0.019248513943132234, valid_loss: 0.017047662908832233\nFOLD: 6, EPOCH: 34, train_loss: 0.0192494371029384, valid_loss: 0.017046671360731125\nBeginning pretraining for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.7185158080914441, valid_loss: 0.6694680750370026\nFOLD: 0, EPOCH: 1, train_loss: 0.34138985526035814, valid_loss: 0.026106042166550953\nFOLD: 0, EPOCH: 2, train_loss: 0.01345782570869607, valid_loss: 0.0059992762592931586\nFOLD: 0, EPOCH: 3, train_loss: 0.008850976742584915, valid_loss: 0.005269483430311084\nFOLD: 0, EPOCH: 4, train_loss: 0.008549820823485361, valid_loss: 0.005195314840724071\nFOLD: 0, EPOCH: 5, train_loss: 0.008462205830523196, valid_loss: 0.0050171414234985905\nFOLD: 0, EPOCH: 6, train_loss: 0.008381022578653167, valid_loss: 0.004691777207578222\nFOLD: 0, EPOCH: 7, train_loss: 0.00832911891698399, valid_loss: 0.004804701233903567\nFOLD: 0, EPOCH: 8, train_loss: 0.00829015824231593, valid_loss: 0.005297128421564897\nFOLD: 0, EPOCH: 9, train_loss: 0.008266128544860026, valid_loss: 0.0052857366390526295\nFOLD: 0, EPOCH: 10, train_loss: 0.008236452485160792, valid_loss: 0.0050475201569497585\nFOLD: 0, EPOCH: 11, train_loss: 0.008203877437421504, valid_loss: 0.005433043542628487\nFOLD: 0, EPOCH: 12, train_loss: 0.008143170243676971, valid_loss: 0.00518867326900363\nFOLD: 0, EPOCH: 13, train_loss: 0.008105122991015805, valid_loss: 0.004896183032542467\nFOLD: 0, EPOCH: 14, train_loss: 0.00807753624394536, valid_loss: 0.004979580097521345\nFOLD: 0, EPOCH: 15, train_loss: 0.00803180132061243, valid_loss: 0.004880003863945603\nFOLD: 0, EPOCH: 16, train_loss: 0.00800003916682566, valid_loss: 0.004727919663613041\nFOLD: 0, EPOCH: 17, train_loss: 0.007964731235166682, valid_loss: 0.005046704628815253\nFOLD: 0, EPOCH: 18, train_loss: 0.0079357950564693, valid_loss: 0.005045629649733503\nFOLD: 0, EPOCH: 19, train_loss: 0.00790648503393373, valid_loss: 0.0047522323826948805\nFOLD: 0, EPOCH: 20, train_loss: 0.007876670070211677, valid_loss: 0.00508866513458391\nFOLD: 0, EPOCH: 21, train_loss: 0.007836043300545392, valid_loss: 0.0048044224580128985\nFOLD: 0, EPOCH: 22, train_loss: 0.007792359889101456, valid_loss: 0.004698510398156941\nFOLD: 0, EPOCH: 23, train_loss: 0.00774732362205053, valid_loss: 0.004850586022560795\nFOLD: 0, EPOCH: 24, train_loss: 0.007707341670003884, valid_loss: 0.004840722307562828\nBeginning training for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.42550089319839196, valid_loss: 0.04784521212180456\nFOLD: 0, EPOCH: 1, train_loss: 0.0281993266852463, valid_loss: 0.021096938600142796\nFOLD: 0, EPOCH: 2, train_loss: 0.022714906755615685, valid_loss: 0.026162218923370045\nFOLD: 0, EPOCH: 3, train_loss: 0.02164079780306886, valid_loss: 0.01830960561831792\nFOLD: 0, EPOCH: 4, train_loss: 0.021122356548028833, valid_loss: 0.018145941197872162\nFOLD: 0, EPOCH: 5, train_loss: 0.020841718760921675, valid_loss: 0.01803746974716584\nFOLD: 0, EPOCH: 6, train_loss: 0.020658285630976454, valid_loss: 0.017720021928350132\nFOLD: 0, EPOCH: 7, train_loss: 0.02051766704329673, valid_loss: 0.01777408334116141\nFOLD: 0, EPOCH: 8, train_loss: 0.020400360117063802, valid_loss: 0.01762652366111676\nFOLD: 0, EPOCH: 9, train_loss: 0.020312660592882073, valid_loss: 0.017683309813340504\nFOLD: 0, EPOCH: 10, train_loss: 0.020241809789748752, valid_loss: 0.017637197238703568\nFOLD: 0, EPOCH: 11, train_loss: 0.020145646406008917, valid_loss: 0.017833264855047066\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 12, train_loss: 0.02002724205308101, valid_loss: 0.01744466119756301\nFOLD: 0, EPOCH: 13, train_loss: 0.019786264199544403, valid_loss: 0.017479264177381992\nFOLD: 0, EPOCH: 14, train_loss: 0.01964555894408156, valid_loss: 0.017720857945581276\nFOLD: 0, EPOCH: 15, train_loss: 0.019566842519185123, valid_loss: 0.017472921560208004\nFOLD: 0, EPOCH: 16, train_loss: 0.019453371655853355, valid_loss: 0.017408113305767376\nFOLD: 0, EPOCH: 17, train_loss: 0.019406189703766036, valid_loss: 0.017488735727965832\nFOLD: 0, EPOCH: 18, train_loss: 0.01935630171176265, valid_loss: 0.017450360270837944\nFOLD: 0, EPOCH: 19, train_loss: 0.01928929173771073, valid_loss: 0.017440550960600376\nFOLD: 0, EPOCH: 20, train_loss: 0.019219519406118813, valid_loss: 0.017433347490926582\nFOLD: 0, EPOCH: 21, train_loss: 0.019134211518308696, valid_loss: 0.017413838766515255\nFOLD: 0, EPOCH: 22, train_loss: 0.019080353188602364, valid_loss: 0.01735327175507943\nFOLD: 0, EPOCH: 23, train_loss: 0.019014862585155404, valid_loss: 0.01739027351140976\nFOLD: 0, EPOCH: 24, train_loss: 0.01892359499983928, valid_loss: 0.01735339251657327\nFOLD: 0, EPOCH: 25, train_loss: 0.01886089551536476, valid_loss: 0.017348393176992733\nFOLD: 0, EPOCH: 26, train_loss: 0.01879389151273405, valid_loss: 0.017380443091193836\nFOLD: 0, EPOCH: 27, train_loss: 0.01870667956331197, valid_loss: 0.017324318177998066\nFOLD: 0, EPOCH: 28, train_loss: 0.018650700064266428, valid_loss: 0.01733009082575639\nFOLD: 0, EPOCH: 29, train_loss: 0.018596672946039366, valid_loss: 0.01730751246213913\nFOLD: 0, EPOCH: 30, train_loss: 0.018533890299937305, valid_loss: 0.017308889888226986\nFOLD: 0, EPOCH: 31, train_loss: 0.0184626556275522, valid_loss: 0.017297389296193916\nFOLD: 0, EPOCH: 32, train_loss: 0.01842236179201042, valid_loss: 0.017330103864272434\nFOLD: 0, EPOCH: 33, train_loss: 0.018421671955901033, valid_loss: 0.017320219116906326\nFOLD: 0, EPOCH: 34, train_loss: 0.018372762729139888, valid_loss: 0.017321955723067124\nBeginning pretraining for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.7184619815910563, valid_loss: 0.6689269244670868\nFOLD: 1, EPOCH: 1, train_loss: 0.34120885063620177, valid_loss: 0.026639417745172977\nFOLD: 1, EPOCH: 2, train_loss: 0.013365387505687335, valid_loss: 0.006067821988835931\nFOLD: 1, EPOCH: 3, train_loss: 0.008824594784528017, valid_loss: 0.005381741172944506\nFOLD: 1, EPOCH: 4, train_loss: 0.008471532286528279, valid_loss: 0.005045535663763682\nFOLD: 1, EPOCH: 5, train_loss: 0.008346909492769661, valid_loss: 0.00484544124143819\nFOLD: 1, EPOCH: 6, train_loss: 0.008323481881662327, valid_loss: 0.005098399007692933\nFOLD: 1, EPOCH: 7, train_loss: 0.008259347845416735, valid_loss: 0.00528859564413627\nFOLD: 1, EPOCH: 8, train_loss: 0.008239044232622665, valid_loss: 0.005183093870679538\nFOLD: 1, EPOCH: 9, train_loss: 0.008198765229762477, valid_loss: 0.00496833142824471\nFOLD: 1, EPOCH: 10, train_loss: 0.00818235838018796, valid_loss: 0.005010819528251886\nFOLD: 1, EPOCH: 11, train_loss: 0.008141974411795245, valid_loss: 0.005303529867281516\nFOLD: 1, EPOCH: 12, train_loss: 0.008102404994561392, valid_loss: 0.004902195185422897\nFOLD: 1, EPOCH: 13, train_loss: 0.0080687107573099, valid_loss: 0.0049841882816205425\nFOLD: 1, EPOCH: 14, train_loss: 0.00804023485740318, valid_loss: 0.0048491505440324545\nFOLD: 1, EPOCH: 15, train_loss: 0.008021133773795822, valid_loss: 0.005843005919208129\nFOLD: 1, EPOCH: 16, train_loss: 0.00798729685244753, valid_loss: 0.005017805689324935\nFOLD: 1, EPOCH: 17, train_loss: 0.007956770892419359, valid_loss: 0.005016459928204616\nFOLD: 1, EPOCH: 18, train_loss: 0.007910969174083541, valid_loss: 0.0048665056626002\nFOLD: 1, EPOCH: 19, train_loss: 0.007882378680412383, valid_loss: 0.004931352101266384\nFOLD: 1, EPOCH: 20, train_loss: 0.007850692675942007, valid_loss: 0.005074732626477878\nFOLD: 1, EPOCH: 21, train_loss: 0.007825972279533744, valid_loss: 0.0049690674059093\nFOLD: 1, EPOCH: 22, train_loss: 0.007788414341014098, valid_loss: 0.004967828746885061\nFOLD: 1, EPOCH: 23, train_loss: 0.007762096507255645, valid_loss: 0.004966549032057325\nFOLD: 1, EPOCH: 24, train_loss: 0.007718989598181318, valid_loss: 0.004890324159835775\nBeginning training for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.4316521347226465, valid_loss: 0.04800099258621534\nFOLD: 1, EPOCH: 1, train_loss: 0.028771367800586364, valid_loss: 0.02051600192983945\nFOLD: 1, EPOCH: 2, train_loss: 0.0235477129873984, valid_loss: 0.018853860596815746\nFOLD: 1, EPOCH: 3, train_loss: 0.021844814805423513, valid_loss: 0.018313293655713398\nFOLD: 1, EPOCH: 4, train_loss: 0.021308230893576845, valid_loss: 0.01805297192186117\nFOLD: 1, EPOCH: 5, train_loss: 0.021022813887718844, valid_loss: 0.01813852445532878\nFOLD: 1, EPOCH: 6, train_loss: 0.02074420786298373, valid_loss: 0.017996561092634995\nFOLD: 1, EPOCH: 7, train_loss: 0.02060145282131784, valid_loss: 0.017798541424175102\nFOLD: 1, EPOCH: 8, train_loss: 0.020476960522287032, valid_loss: 0.017682944734891255\nFOLD: 1, EPOCH: 9, train_loss: 0.02031629757188699, valid_loss: 0.01773574048032363\nFOLD: 1, EPOCH: 10, train_loss: 0.020237638615071774, valid_loss: 0.017589034823079903\nFOLD: 1, EPOCH: 11, train_loss: 0.020092896569301102, valid_loss: 0.01756907037148873\nFOLD: 1, EPOCH: 12, train_loss: 0.020042961353764814, valid_loss: 0.01753277548899253\nFOLD: 1, EPOCH: 13, train_loss: 0.01993775400607025, valid_loss: 0.017530178961654503\nFOLD: 1, EPOCH: 14, train_loss: 0.01987921879352892, valid_loss: 0.017648724528650444\nFOLD: 1, EPOCH: 15, train_loss: 0.019766269963892066, valid_loss: 0.01745685872932275\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 16, train_loss: 0.019615844823420048, valid_loss: 0.01750681021561225\nFOLD: 1, EPOCH: 17, train_loss: 0.019322397680405307, valid_loss: 0.01731167019655307\nFOLD: 1, EPOCH: 18, train_loss: 0.019166849334450328, valid_loss: 0.01739093443999688\nFOLD: 1, EPOCH: 19, train_loss: 0.019158122951493543, valid_loss: 0.017315138441820938\nFOLD: 1, EPOCH: 20, train_loss: 0.01903451721677009, valid_loss: 0.017378184323509533\nFOLD: 1, EPOCH: 21, train_loss: 0.018938244539586938, valid_loss: 0.01730033103376627\nFOLD: 1, EPOCH: 22, train_loss: 0.018849310136454946, valid_loss: 0.017196720155576866\nFOLD: 1, EPOCH: 23, train_loss: 0.018803586032898986, valid_loss: 0.017308164698382218\nFOLD: 1, EPOCH: 24, train_loss: 0.018739424481549683, valid_loss: 0.017313893573979538\nFOLD: 1, EPOCH: 25, train_loss: 0.018597611280925134, valid_loss: 0.017234493046998978\nFOLD: 1, EPOCH: 26, train_loss: 0.01853441573022043, valid_loss: 0.01715648298462232\nFOLD: 1, EPOCH: 27, train_loss: 0.01845667905667249, valid_loss: 0.017208142206072807\nFOLD: 1, EPOCH: 28, train_loss: 0.018380511004258606, valid_loss: 0.01715845490495364\nFOLD: 1, EPOCH: 29, train_loss: 0.018281034522635096, valid_loss: 0.017161928117275238\nFOLD: 1, EPOCH: 30, train_loss: 0.018244571764679515, valid_loss: 0.017170690931379795\nFOLD: 1, EPOCH: 31, train_loss: 0.01820222260978292, valid_loss: 0.01715650750945012\nFOLD: 1, EPOCH: 32, train_loss: 0.018134618451928392, valid_loss: 0.01717618169883887\nFOLD: 1, EPOCH: 33, train_loss: 0.018128767171326804, valid_loss: 0.017170691241820652\nFOLD: 1, EPOCH: 34, train_loss: 0.018115296412040207, valid_loss: 0.017168169220288593\nBeginning pretraining for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.7183809245333952, valid_loss: 0.6682032148043314\nFOLD: 2, EPOCH: 1, train_loss: 0.33971667004858747, valid_loss: 0.0254739277685682\nFOLD: 2, EPOCH: 2, train_loss: 0.013664268751573913, valid_loss: 0.005890510976314545\nFOLD: 2, EPOCH: 3, train_loss: 0.00894557824358344, valid_loss: 0.005128096633901198\nFOLD: 2, EPOCH: 4, train_loss: 0.008555321762447847, valid_loss: 0.004958864767104387\nFOLD: 2, EPOCH: 5, train_loss: 0.00848250326645725, valid_loss: 0.0050500088060895605\nFOLD: 2, EPOCH: 6, train_loss: 0.008414474165286212, valid_loss: 0.005047221357623736\nFOLD: 2, EPOCH: 7, train_loss: 0.008388734710238436, valid_loss: 0.004649179521948099\nFOLD: 2, EPOCH: 8, train_loss: 0.008316275074749309, valid_loss: 0.004849093267694116\nFOLD: 2, EPOCH: 9, train_loss: 0.008290918174144976, valid_loss: 0.00530365901067853\nFOLD: 2, EPOCH: 10, train_loss: 0.008256231352467747, valid_loss: 0.01912660679469506\nFOLD: 2, EPOCH: 11, train_loss: 0.008248469080118573, valid_loss: 0.0044705293063695235\nFOLD: 2, EPOCH: 12, train_loss: 0.00819752413286444, valid_loss: 0.004927295648182432\nFOLD: 2, EPOCH: 13, train_loss: 0.00817450849503717, valid_loss: 0.005144420778378844\nFOLD: 2, EPOCH: 14, train_loss: 0.008140769691261299, valid_loss: 0.0057835387997329235\nFOLD: 2, EPOCH: 15, train_loss: 0.008090843610903797, valid_loss: 0.0046839185524731874\nFOLD: 2, EPOCH: 16, train_loss: 0.008064811309689985, valid_loss: 0.004803621054937442\nFOLD: 2, EPOCH: 17, train_loss: 0.008008672047735137, valid_loss: 0.004921459282437961\nFOLD: 2, EPOCH: 18, train_loss: 0.007978155072230627, valid_loss: 0.004889523998523752\nFOLD: 2, EPOCH: 19, train_loss: 0.00793429512037512, valid_loss: 0.0050800378279139595\nFOLD: 2, EPOCH: 20, train_loss: 0.00790215439765769, valid_loss: 0.005023759479324023\nFOLD: 2, EPOCH: 21, train_loss: 0.007848614224177949, valid_loss: 0.004772040876559913\nFOLD: 2, EPOCH: 22, train_loss: 0.007821664625011823, valid_loss: 0.0048313788914432125\nFOLD: 2, EPOCH: 23, train_loss: 0.0077798360794344365, valid_loss: 0.004783826569716136\nFOLD: 2, EPOCH: 24, train_loss: 0.007739417926024865, valid_loss: 0.004746265787010391\nBeginning training for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.4110344849965152, valid_loss: 0.03673792940874895\nFOLD: 2, EPOCH: 1, train_loss: 0.027071201297290185, valid_loss: 0.020925805283089478\nFOLD: 2, EPOCH: 2, train_loss: 0.02215005221831448, valid_loss: 0.021962387797733147\nFOLD: 2, EPOCH: 3, train_loss: 0.02124903860556729, valid_loss: 0.018893606029450893\nFOLD: 2, EPOCH: 4, train_loss: 0.020928369396749663, valid_loss: 0.01863510689387719\nFOLD: 2, EPOCH: 5, train_loss: 0.020775443107327995, valid_loss: 0.01862418558448553\nFOLD: 2, EPOCH: 6, train_loss: 0.02058043615782962, valid_loss: 0.018565340898931026\nFOLD: 2, EPOCH: 7, train_loss: 0.020539998734260306, valid_loss: 0.018455191204945248\nFOLD: 2, EPOCH: 8, train_loss: 0.020382286158992964, valid_loss: 0.01842681256433328\nFOLD: 2, EPOCH: 9, train_loss: 0.020269658964346435, valid_loss: 0.018304340230921905\nFOLD: 2, EPOCH: 10, train_loss: 0.020226348410634434, valid_loss: 0.01818508158127467\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 11, train_loss: 0.020084585172726828, valid_loss: 0.018211553804576397\nFOLD: 2, EPOCH: 12, train_loss: 0.019887337844599697, valid_loss: 0.018197587691247463\nFOLD: 2, EPOCH: 13, train_loss: 0.019717172774321893, valid_loss: 0.018290688283741474\nFOLD: 2, EPOCH: 14, train_loss: 0.01966890972107649, valid_loss: 0.01813640259206295\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 15, train_loss: 0.019588348017457652, valid_loss: 0.01797486872722705\nFOLD: 2, EPOCH: 16, train_loss: 0.019434745697414175, valid_loss: 0.01794407082100709\nFOLD: 2, EPOCH: 17, train_loss: 0.01933339117642711, valid_loss: 0.01809149297575156\nFOLD: 2, EPOCH: 18, train_loss: 0.01930352848242311, valid_loss: 0.01799141087879737\nFOLD: 2, EPOCH: 19, train_loss: 0.019254594438654536, valid_loss: 0.01810668936620156\nFOLD: 2, EPOCH: 20, train_loss: 0.01921959237798172, valid_loss: 0.018158027281363804\nFOLD: 2, EPOCH: 21, train_loss: 0.01917151109698941, valid_loss: 0.01807381274799506\nFOLD: 2, EPOCH: 22, train_loss: 0.019152471576543414, valid_loss: 0.018047787559529144\nFOLD: 2, EPOCH: 23, train_loss: 0.019102616180830142, valid_loss: 0.0180096502105395\nFOLD: 2, EPOCH: 24, train_loss: 0.01909163292935666, valid_loss: 0.018021858607729275\nFOLD: 2, EPOCH: 25, train_loss: 0.019049534300232634, valid_loss: 0.017996732145547867\nFOLD: 2, EPOCH: 26, train_loss: 0.019023971577339312, valid_loss: 0.017963723900417488\nFOLD: 2, EPOCH: 27, train_loss: 0.019039487192297682, valid_loss: 0.01796929258853197\nFOLD: 2, EPOCH: 28, train_loss: 0.018978459760546684, valid_loss: 0.017997462612887222\nFOLD: 2, EPOCH: 29, train_loss: 0.018942641149110654, valid_loss: 0.01796142819027106\nFOLD: 2, EPOCH: 30, train_loss: 0.018963674466837856, valid_loss: 0.017953063050905865\nFOLD: 2, EPOCH: 31, train_loss: 0.01895644882803454, valid_loss: 0.0179458890731136\nFOLD: 2, EPOCH: 32, train_loss: 0.018921583836131236, valid_loss: 0.01794890562693278\nFOLD: 2, EPOCH: 33, train_loss: 0.018928258625023505, valid_loss: 0.017956993232170742\nFOLD: 2, EPOCH: 34, train_loss: 0.018929456470205504, valid_loss: 0.01794809941202402\nBeginning pretraining for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.7185595877030316, valid_loss: 0.6702277561028799\nFOLD: 3, EPOCH: 1, train_loss: 0.33977357539183956, valid_loss: 0.026140437771876652\nFOLD: 3, EPOCH: 2, train_loss: 0.013443372760187177, valid_loss: 0.005991532001644373\nFOLD: 3, EPOCH: 3, train_loss: 0.008862356847042547, valid_loss: 0.0053407473800083\nFOLD: 3, EPOCH: 4, train_loss: 0.008531150216346277, valid_loss: 0.005236544879153371\nFOLD: 3, EPOCH: 5, train_loss: 0.0084154331125319, valid_loss: 0.004816459181408088\nFOLD: 3, EPOCH: 6, train_loss: 0.008634860079516382, valid_loss: 0.0048405558336526155\nFOLD: 3, EPOCH: 7, train_loss: 0.008324419148266315, valid_loss: 0.004733774112537503\nFOLD: 3, EPOCH: 8, train_loss: 0.008279197931508808, valid_loss: 0.0049315352613727255\nFOLD: 3, EPOCH: 9, train_loss: 0.008265184676822494, valid_loss: 0.005187601316720247\nFOLD: 3, EPOCH: 10, train_loss: 0.00823587690041784, valid_loss: 0.004693079041317105\nFOLD: 3, EPOCH: 11, train_loss: 0.008186549112639007, valid_loss: 0.00499496771954\nFOLD: 3, EPOCH: 12, train_loss: 0.008164872853633235, valid_loss: 0.004771944833919406\nFOLD: 3, EPOCH: 13, train_loss: 0.0081250865751987, valid_loss: 0.005047150111446778\nFOLD: 3, EPOCH: 14, train_loss: 0.008088786661734475, valid_loss: 0.004750021966174245\nFOLD: 3, EPOCH: 15, train_loss: 0.00804140355766696, valid_loss: 0.004809866348902385\nFOLD: 3, EPOCH: 16, train_loss: 0.008015637505142129, valid_loss: 0.005074782141794761\nFOLD: 3, EPOCH: 17, train_loss: 0.007976614568820772, valid_loss: 0.0049650805691878\nFOLD: 3, EPOCH: 18, train_loss: 0.00793753720491248, valid_loss: 0.005037570217003425\nFOLD: 3, EPOCH: 19, train_loss: 0.007920576629283674, valid_loss: 0.0048980081143478555\nFOLD: 3, EPOCH: 20, train_loss: 0.007887289955225937, valid_loss: 0.0049155208592613535\nFOLD: 3, EPOCH: 21, train_loss: 0.007844447610242403, valid_loss: 0.00484048668295145\nFOLD: 3, EPOCH: 22, train_loss: 0.0078116560300045155, valid_loss: 0.004876704265673955\nFOLD: 3, EPOCH: 23, train_loss: 0.007774087203228299, valid_loss: 0.004831918437654774\nFOLD: 3, EPOCH: 24, train_loss: 0.007726310957771014, valid_loss: 0.005492564834033449\nBeginning training for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.41885474685798674, valid_loss: 0.04236043617129326\nFOLD: 3, EPOCH: 1, train_loss: 0.02736032272086424, valid_loss: 0.020865058215955894\nFOLD: 3, EPOCH: 2, train_loss: 0.022163243799963418, valid_loss: 0.018685295867423218\nFOLD: 3, EPOCH: 3, train_loss: 0.021338915288010064, valid_loss: 0.018978650060792763\nFOLD: 3, EPOCH: 4, train_loss: 0.021037694231113967, valid_loss: 0.018093012583752472\nFOLD: 3, EPOCH: 5, train_loss: 0.02086128354729975, valid_loss: 0.017983837363620598\nFOLD: 3, EPOCH: 6, train_loss: 0.020766427795238355, valid_loss: 0.017988772441943485\nFOLD: 3, EPOCH: 7, train_loss: 0.020684180890812594, valid_loss: 0.018211804951230686\nFOLD: 3, EPOCH: 8, train_loss: 0.020532705656745854, valid_loss: 0.017927718969682854\nFOLD: 3, EPOCH: 9, train_loss: 0.020447066875503343, valid_loss: 0.01789840217679739\nFOLD: 3, EPOCH: 10, train_loss: 0.020376129349803224, valid_loss: 0.0177814153333505\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 11, train_loss: 0.02024665639242705, valid_loss: 0.017539586561421554\nFOLD: 3, EPOCH: 12, train_loss: 0.019969469977214056, valid_loss: 0.01756881767263015\nFOLD: 3, EPOCH: 13, train_loss: 0.019861470743575516, valid_loss: 0.01789105962961912\nFOLD: 3, EPOCH: 14, train_loss: 0.019842343733591193, valid_loss: 0.017485990809897583\nFOLD: 3, EPOCH: 15, train_loss: 0.019741833155207774, valid_loss: 0.017252831098934013\nFOLD: 3, EPOCH: 16, train_loss: 0.01968166812816087, valid_loss: 0.017576671205461025\nFOLD: 3, EPOCH: 17, train_loss: 0.01961318392525701, valid_loss: 0.0176239637658\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 18, train_loss: 0.019621740533586812, valid_loss: 0.01722334511578083\nFOLD: 3, EPOCH: 19, train_loss: 0.019434715730740744, valid_loss: 0.017360888421535492\nFOLD: 3, EPOCH: 20, train_loss: 0.01937714750495027, valid_loss: 0.017261674006779987\nFOLD: 3, EPOCH: 21, train_loss: 0.01931294333189726, valid_loss: 0.017256719370683033\nFOLD: 3, EPOCH: 22, train_loss: 0.019276118289460156, valid_loss: 0.017256610095500946\nFOLD: 3, EPOCH: 23, train_loss: 0.019242008874083266, valid_loss: 0.01717420294880867\nFOLD: 3, EPOCH: 24, train_loss: 0.0192201442687827, valid_loss: 0.01720835144321124\nFOLD: 3, EPOCH: 25, train_loss: 0.019197360283749944, valid_loss: 0.017229186681409676\nFOLD: 3, EPOCH: 26, train_loss: 0.019172716721454087, valid_loss: 0.01716268000503381\nFOLD: 3, EPOCH: 27, train_loss: 0.019118424943264795, valid_loss: 0.01718467753380537\nFOLD: 3, EPOCH: 28, train_loss: 0.019082974313813096, valid_loss: 0.017141152483721573\nFOLD: 3, EPOCH: 29, train_loss: 0.019088024163947385, valid_loss: 0.017212596411506336\nFOLD: 3, EPOCH: 30, train_loss: 0.019077509148594213, valid_loss: 0.017185450221101444\nFOLD: 3, EPOCH: 31, train_loss: 0.01908988007070387, valid_loss: 0.017184574467440445\nFOLD: 3, EPOCH: 32, train_loss: 0.019067514578209203, valid_loss: 0.01717184701313575\nFOLD: 3, EPOCH: 33, train_loss: 0.019057656200054812, valid_loss: 0.017183404105405014\nFOLD: 3, EPOCH: 34, train_loss: 0.019055587842183953, valid_loss: 0.017186326906085014\nBeginning pretraining for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.7185925543308258, valid_loss: 0.6700616876284281\nFOLD: 4, EPOCH: 1, train_loss: 0.3415738381445408, valid_loss: 0.02577906164030234\nFOLD: 4, EPOCH: 2, train_loss: 0.013270777321475394, valid_loss: 0.0060453452945997315\nFOLD: 4, EPOCH: 3, train_loss: 0.008886831648209515, valid_loss: 0.0052687918456892175\nFOLD: 4, EPOCH: 4, train_loss: 0.008537068579565077, valid_loss: 0.004890169870729248\nFOLD: 4, EPOCH: 5, train_loss: 0.00839388685520081, valid_loss: 0.004780736907074849\nFOLD: 4, EPOCH: 6, train_loss: 0.008367643910734093, valid_loss: 0.004970773821696639\nFOLD: 4, EPOCH: 7, train_loss: 0.008284993853200884, valid_loss: 0.00473867729306221\nFOLD: 4, EPOCH: 8, train_loss: 0.008244385381284006, valid_loss: 0.005100220364208023\nFOLD: 4, EPOCH: 9, train_loss: 0.008205079035285641, valid_loss: 0.00529532313036422\nFOLD: 4, EPOCH: 10, train_loss: 0.008182173142867052, valid_loss: 0.005015039894108971\nFOLD: 4, EPOCH: 11, train_loss: 0.008143533158170827, valid_loss: 0.005051371408626437\nFOLD: 4, EPOCH: 12, train_loss: 0.008115683558999616, valid_loss: 0.004674457324047883\nFOLD: 4, EPOCH: 13, train_loss: 0.008091194062110256, valid_loss: 0.004832097406809528\nFOLD: 4, EPOCH: 14, train_loss: 0.00804651398485636, valid_loss: 0.0047287171085675555\nFOLD: 4, EPOCH: 15, train_loss: 0.008032049257856081, valid_loss: 0.004734451028828819\nFOLD: 4, EPOCH: 16, train_loss: 0.00798380495432545, valid_loss: 0.004877803847193718\nFOLD: 4, EPOCH: 17, train_loss: 0.007949883638716796, valid_loss: 0.0049048779377092915\nFOLD: 4, EPOCH: 18, train_loss: 0.007921382510925041, valid_loss: 0.004747440805658698\nFOLD: 4, EPOCH: 19, train_loss: 0.007888359387460001, valid_loss: 0.004927790568520625\nFOLD: 4, EPOCH: 20, train_loss: 0.007873463074621908, valid_loss: 0.004761101600403587\nFOLD: 4, EPOCH: 21, train_loss: 0.00783018403522232, valid_loss: 0.004808484499032299\nFOLD: 4, EPOCH: 22, train_loss: 0.007803429123562048, valid_loss: 0.0048879210371524096\nFOLD: 4, EPOCH: 23, train_loss: 0.007770064382759088, valid_loss: 0.004800824370856087\nFOLD: 4, EPOCH: 24, train_loss: 0.007725946112152408, valid_loss: 0.0049073488141099615\nBeginning training for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.4136109951445285, valid_loss: 0.03691627209385236\nFOLD: 4, EPOCH: 1, train_loss: 0.02720200297806193, valid_loss: 0.021389321113626163\nFOLD: 4, EPOCH: 2, train_loss: 0.02229988421587383, valid_loss: 0.019291875263055164\nFOLD: 4, EPOCH: 3, train_loss: 0.021443029701271477, valid_loss: 0.018681660604973633\nFOLD: 4, EPOCH: 4, train_loss: 0.021039122332106617, valid_loss: 0.018438728836675484\nFOLD: 4, EPOCH: 5, train_loss: 0.020885698168593293, valid_loss: 0.01822175644338131\nFOLD: 4, EPOCH: 6, train_loss: 0.020678093054277057, valid_loss: 0.018096210124591987\nFOLD: 4, EPOCH: 7, train_loss: 0.02056750759263249, valid_loss: 0.018150101105372112\nFOLD: 4, EPOCH: 8, train_loss: 0.02048788866137757, valid_loss: 0.018539011478424072\nFOLD: 4, EPOCH: 9, train_loss: 0.020394907880793598, valid_loss: 0.017945061437785625\nFOLD: 4, EPOCH: 10, train_loss: 0.02031224566128324, valid_loss: 0.017996554573376972\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 11, train_loss: 0.02016436675673022, valid_loss: 0.01785582521309455\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 12, train_loss: 0.01991307017776896, valid_loss: 0.01776853421082099\nFOLD: 4, EPOCH: 13, train_loss: 0.019755310378968716, valid_loss: 0.017889410878221195\nFOLD: 4, EPOCH: 14, train_loss: 0.019665719119503218, valid_loss: 0.017897337364653747\nFOLD: 4, EPOCH: 15, train_loss: 0.0196170831537422, valid_loss: 0.017716023760537308\nFOLD: 4, EPOCH: 16, train_loss: 0.019566362121087665, valid_loss: 0.01772420605023702\nFOLD: 4, EPOCH: 17, train_loss: 0.019504819174899775, valid_loss: 0.01772198857118686\nFOLD: 4, EPOCH: 18, train_loss: 0.019478367586784503, valid_loss: 0.017710852747162182\nFOLD: 4, EPOCH: 19, train_loss: 0.01942247218068908, valid_loss: 0.017630007738868397\nFOLD: 4, EPOCH: 20, train_loss: 0.01940644494093516, valid_loss: 0.017574434479077656\nFOLD: 4, EPOCH: 21, train_loss: 0.01936466696069521, valid_loss: 0.01758553273975849\nFOLD: 4, EPOCH: 22, train_loss: 0.01935084295623443, valid_loss: 0.01759796403348446\nFOLD: 4, EPOCH: 23, train_loss: 0.019321003983564237, valid_loss: 0.017626629521449406\nFOLD: 4, EPOCH: 24, train_loss: 0.01929006725549698, valid_loss: 0.01758826244622469\nFOLD: 4, EPOCH: 25, train_loss: 0.019264079137321782, valid_loss: 0.017634958028793335\nFOLD: 4, EPOCH: 26, train_loss: 0.019270944792558167, valid_loss: 0.017561564221978188\nFOLD: 4, EPOCH: 27, train_loss: 0.019239855711074436, valid_loss: 0.01758441918840011\nFOLD: 4, EPOCH: 28, train_loss: 0.019222169018843594, valid_loss: 0.017530405273040135\nFOLD: 4, EPOCH: 29, train_loss: 0.019208628191229177, valid_loss: 0.017571125800410908\nFOLD: 4, EPOCH: 30, train_loss: 0.019180691472309476, valid_loss: 0.017547744947175186\nFOLD: 4, EPOCH: 31, train_loss: 0.019184882095193163, valid_loss: 0.017546513428290684\nFOLD: 4, EPOCH: 32, train_loss: 0.019160804503104267, valid_loss: 0.017542834393680096\nFOLD: 4, EPOCH: 33, train_loss: 0.019191755310577506, valid_loss: 0.017557753560443718\nFOLD: 4, EPOCH: 34, train_loss: 0.019178173121284035, valid_loss: 0.01755906641483307\nBeginning pretraining for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.718610447995803, valid_loss: 0.6669732431570689\nFOLD: 5, EPOCH: 1, train_loss: 0.33942532999550595, valid_loss: 0.02608166014154752\nFOLD: 5, EPOCH: 2, train_loss: 0.013390281812890488, valid_loss: 0.006076139863580465\nFOLD: 5, EPOCH: 3, train_loss: 0.008771584780119798, valid_loss: 0.00530560555246969\nFOLD: 5, EPOCH: 4, train_loss: 0.008472888141541797, valid_loss: 0.0050268838337312145\nFOLD: 5, EPOCH: 5, train_loss: 0.008370216059334138, valid_loss: 0.0050274628059317665\nFOLD: 5, EPOCH: 6, train_loss: 0.008318139966029455, valid_loss: 0.00490673235617578\nFOLD: 5, EPOCH: 7, train_loss: 0.008254705462604761, valid_loss: 0.004661139488841097\nFOLD: 5, EPOCH: 8, train_loss: 0.00823158327051822, valid_loss: 0.004830918585260709\nFOLD: 5, EPOCH: 9, train_loss: 0.008196050342281951, valid_loss: 0.006753205166508754\nFOLD: 5, EPOCH: 10, train_loss: 0.008160988526309238, valid_loss: 0.004916579540198048\nFOLD: 5, EPOCH: 11, train_loss: 0.008084594833609812, valid_loss: 0.004897556966170669\nFOLD: 5, EPOCH: 12, train_loss: 0.00805400199640323, valid_loss: 0.005761817873766025\nFOLD: 5, EPOCH: 13, train_loss: 0.00803035225116593, valid_loss: 0.004795987857505679\nFOLD: 5, EPOCH: 14, train_loss: 0.007995748878730571, valid_loss: 0.005132548899079363\nFOLD: 5, EPOCH: 15, train_loss: 0.007950063808547222, valid_loss: 0.004950402692581217\nFOLD: 5, EPOCH: 16, train_loss: 0.007903140723047888, valid_loss: 0.0047064588870853186\nFOLD: 5, EPOCH: 17, train_loss: 0.007884179625441046, valid_loss: 0.00485027472799023\nFOLD: 5, EPOCH: 18, train_loss: 0.00785923534182503, valid_loss: 0.00489810931806763\nFOLD: 5, EPOCH: 19, train_loss: 0.007826766259420444, valid_loss: 0.004709778353571892\nFOLD: 5, EPOCH: 20, train_loss: 0.007775618941249216, valid_loss: 0.00496191958275934\nFOLD: 5, EPOCH: 21, train_loss: 0.00774124961839441, valid_loss: 0.004760587199901541\nFOLD: 5, EPOCH: 22, train_loss: 0.00769886578542783, valid_loss: 0.004886993595088522\nFOLD: 5, EPOCH: 23, train_loss: 0.007674384985447806, valid_loss: 0.004878322438647349\nFOLD: 5, EPOCH: 24, train_loss: 0.007616443504743716, valid_loss: 0.004862986970692873\nBeginning training for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.42987918196355596, valid_loss: 0.049070632085204124\nFOLD: 5, EPOCH: 1, train_loss: 0.02827346620752531, valid_loss: 0.02137953384468953\nFOLD: 5, EPOCH: 2, train_loss: 0.02241650766090435, valid_loss: 0.019163695474465687\nFOLD: 5, EPOCH: 3, train_loss: 0.02134338689639288, valid_loss: 0.01872079198559125\nFOLD: 5, EPOCH: 4, train_loss: 0.02102407971944879, valid_loss: 0.018192190676927567\nFOLD: 5, EPOCH: 5, train_loss: 0.020702160675736034, valid_loss: 0.018617847003042698\nFOLD: 5, EPOCH: 6, train_loss: 0.020563394762575626, valid_loss: 0.01835461302349965\nFOLD: 5, EPOCH: 7, train_loss: 0.020374370081459776, valid_loss: 0.018128879678746063\nFOLD: 5, EPOCH: 8, train_loss: 0.02023881279370364, valid_loss: 0.0181807242333889\nFOLD: 5, EPOCH: 9, train_loss: 0.020211452782592353, valid_loss: 0.01798872711757819\nFOLD: 5, EPOCH: 10, train_loss: 0.020071479327538434, valid_loss: 0.01803701774527629\nFOLD: 5, EPOCH: 11, train_loss: 0.020043941926868522, valid_loss: 0.01803782458106677\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 12, train_loss: 0.019913872210856748, valid_loss: 0.01784600658963124\nFOLD: 5, EPOCH: 13, train_loss: 0.019592309699339026, valid_loss: 0.017815570657451946\nFOLD: 5, EPOCH: 14, train_loss: 0.019522594945395693, valid_loss: 0.01793636040141185\nFOLD: 5, EPOCH: 15, train_loss: 0.019413514391464347, valid_loss: 0.018091574932138126\nFOLD: 5, EPOCH: 16, train_loss: 0.01937113329768181, valid_loss: 0.017862194528182346\nFOLD: 5, EPOCH: 17, train_loss: 0.01928141444702359, valid_loss: 0.01779780009140571\nFOLD: 5, EPOCH: 18, train_loss: 0.019209823625929216, valid_loss: 0.017800262197852135\nFOLD: 5, EPOCH: 19, train_loss: 0.019185128293054944, valid_loss: 0.01789403644700845\nFOLD: 5, EPOCH: 20, train_loss: 0.01910550711566911, valid_loss: 0.017752517014741898\nFOLD: 5, EPOCH: 21, train_loss: 0.019039406824637863, valid_loss: 0.01785323489457369\nFOLD: 5, EPOCH: 22, train_loss: 0.01898432057350874, valid_loss: 0.0177221925308307\nFOLD: 5, EPOCH: 23, train_loss: 0.018937565660213724, valid_loss: 0.017870777286589146\nFOLD: 5, EPOCH: 24, train_loss: 0.018851793754626724, valid_loss: 0.017782852053642273\nFOLD: 5, EPOCH: 25, train_loss: 0.01878296846852583, valid_loss: 0.017742055468261242\nFOLD: 5, EPOCH: 26, train_loss: 0.01869911251260954, valid_loss: 0.017729016331334908\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 27, train_loss: 0.01857791161712478, valid_loss: 0.017669896595180035\nFOLD: 5, EPOCH: 28, train_loss: 0.018527318668716094, valid_loss: 0.017732480851312477\nFOLD: 5, EPOCH: 29, train_loss: 0.01851546945159926, valid_loss: 0.01766252238303423\nFOLD: 5, EPOCH: 30, train_loss: 0.018477345542872652, valid_loss: 0.017697739104429882\nFOLD: 5, EPOCH: 31, train_loss: 0.018459921647958896, valid_loss: 0.017681446236868698\nFOLD: 5, EPOCH: 32, train_loss: 0.018455647808663985, valid_loss: 0.017683611251413822\nFOLD: 5, EPOCH: 33, train_loss: 0.018475952229517346, valid_loss: 0.017679729188481968\nFOLD: 5, EPOCH: 34, train_loss: 0.018438641694100463, valid_loss: 0.01768577564507723\nBeginning pretraining for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.7187373059637406, valid_loss: 0.6695990463097891\nFOLD: 6, EPOCH: 1, train_loss: 0.3431783631882247, valid_loss: 0.025592707407971222\nFOLD: 6, EPOCH: 2, train_loss: 0.013500868589343393, valid_loss: 0.005784208187833428\nFOLD: 6, EPOCH: 3, train_loss: 0.008733387789962924, valid_loss: 0.005084581983586152\nFOLD: 6, EPOCH: 4, train_loss: 0.008440812693580109, valid_loss: 0.00478338139752547\nFOLD: 6, EPOCH: 5, train_loss: 0.008340768091490163, valid_loss: 0.004774948116391897\nFOLD: 6, EPOCH: 6, train_loss: 0.008411464623778182, valid_loss: 0.004549250976803402\nFOLD: 6, EPOCH: 7, train_loss: 0.008265044663430136, valid_loss: 0.004637567326426506\nFOLD: 6, EPOCH: 8, train_loss: 0.008207456838777837, valid_loss: 0.005041596634934346\nFOLD: 6, EPOCH: 9, train_loss: 0.00817225398221875, valid_loss: 0.0048166033035765094\nFOLD: 6, EPOCH: 10, train_loss: 0.008154990178916384, valid_loss: 0.004712965805083513\nFOLD: 6, EPOCH: 11, train_loss: 0.008134468075107126, valid_loss: 0.00530544831417501\nFOLD: 6, EPOCH: 12, train_loss: 0.008078220533207059, valid_loss: 0.005932235391810536\nFOLD: 6, EPOCH: 13, train_loss: 0.008083518156233956, valid_loss: 0.004702599408725898\nFOLD: 6, EPOCH: 14, train_loss: 0.007995207629659596, valid_loss: 0.004409682587720454\nFOLD: 6, EPOCH: 15, train_loss: 0.007958712097366942, valid_loss: 0.004844850084433953\nFOLD: 6, EPOCH: 16, train_loss: 0.007967828937313137, valid_loss: 0.004658822823936741\nFOLD: 6, EPOCH: 17, train_loss: 0.00791734061203897, valid_loss: 0.004589806931714217\nFOLD: 6, EPOCH: 18, train_loss: 0.007871557794073048, valid_loss: 0.004897029139101505\nFOLD: 6, EPOCH: 19, train_loss: 0.00782904030262109, valid_loss: 0.004520365347464879\nFOLD: 6, EPOCH: 20, train_loss: 0.007801248979590395, valid_loss: 0.004557577737917502\nFOLD: 6, EPOCH: 21, train_loss: 0.007757841116365264, valid_loss: 0.00448836013674736\nFOLD: 6, EPOCH: 22, train_loss: 0.007719801725162303, valid_loss: 0.004718823358416557\nFOLD: 6, EPOCH: 23, train_loss: 0.007669987663736238, valid_loss: 0.0047283528838306665\nFOLD: 6, EPOCH: 24, train_loss: 0.007634073358905666, valid_loss: 0.004583201448743542\nBeginning training for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.40768530165009637, valid_loss: 0.033980416133999825\nFOLD: 6, EPOCH: 1, train_loss: 0.026818486314047787, valid_loss: 0.01992735732346773\nFOLD: 6, EPOCH: 2, train_loss: 0.02230980888228206, valid_loss: 0.018432045976320904\nFOLD: 6, EPOCH: 3, train_loss: 0.021479649962309527, valid_loss: 0.018961375579237938\nFOLD: 6, EPOCH: 4, train_loss: 0.02114035194630132, valid_loss: 0.01792982469002406\nFOLD: 6, EPOCH: 5, train_loss: 0.020949837094282404, valid_loss: 0.017651054697732132\nFOLD: 6, EPOCH: 6, train_loss: 0.0208197631599272, valid_loss: 0.018121979509790737\nFOLD: 6, EPOCH: 7, train_loss: 0.020795967977713135, valid_loss: 0.01789084915071726\nFOLD: 6, EPOCH: 8, train_loss: 0.020598405095584253, valid_loss: 0.01774476437518994\nFOLD: 6, EPOCH: 9, train_loss: 0.020480597336940905, valid_loss: 0.018234288940827053\nFOLD: 6, EPOCH: 10, train_loss: 0.020381963811814785, valid_loss: 0.017531368881464005\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 11, train_loss: 0.020231172825921986, valid_loss: 0.01749519755442937\nFOLD: 6, EPOCH: 12, train_loss: 0.01996590712052934, valid_loss: 0.01735069106022517\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 13, train_loss: 0.019826268886818606, valid_loss: 0.017235230654478073\nFOLD: 6, EPOCH: 14, train_loss: 0.01967705079518697, valid_loss: 0.017232822254300117\nFOLD: 6, EPOCH: 15, train_loss: 0.019618273603127283, valid_loss: 0.017163544582823913\nFOLD: 6, EPOCH: 16, train_loss: 0.019556224510511932, valid_loss: 0.01714172524710496\nFOLD: 6, EPOCH: 17, train_loss: 0.01951499129919445, valid_loss: 0.01708328320334355\nFOLD: 6, EPOCH: 18, train_loss: 0.01947444540393703, valid_loss: 0.017180845451851685\nFOLD: 6, EPOCH: 19, train_loss: 0.0194588008829776, valid_loss: 0.017125622369349003\nFOLD: 6, EPOCH: 20, train_loss: 0.019414547995171127, valid_loss: 0.017075746630628903\nFOLD: 6, EPOCH: 21, train_loss: 0.019390226122649276, valid_loss: 0.017266489875813324\nFOLD: 6, EPOCH: 22, train_loss: 0.019371733891175073, valid_loss: 0.01717888346562783\nFOLD: 6, EPOCH: 23, train_loss: 0.01931380184696001, valid_loss: 0.017068421468138695\nFOLD: 6, EPOCH: 24, train_loss: 0.019292632610920596, valid_loss: 0.017038909407953422\nFOLD: 6, EPOCH: 25, train_loss: 0.01926518215195221, valid_loss: 0.017058500088751316\nFOLD: 6, EPOCH: 26, train_loss: 0.019247385015820757, valid_loss: 0.01700713951140642\nFOLD: 6, EPOCH: 27, train_loss: 0.019226322279256934, valid_loss: 0.017065463898082573\nFOLD: 6, EPOCH: 28, train_loss: 0.019189639016985893, valid_loss: 0.017037740908563137\nFOLD: 6, EPOCH: 29, train_loss: 0.019174225957078093, valid_loss: 0.01702901131163041\nFOLD: 6, EPOCH: 30, train_loss: 0.01918094714774805, valid_loss: 0.017069159386058647\nFOLD: 6, EPOCH: 31, train_loss: 0.01914926607380895, valid_loss: 0.01703969668596983\nFOLD: 6, EPOCH: 32, train_loss: 0.019143618533716482, valid_loss: 0.017050963826477528\nFOLD: 6, EPOCH: 33, train_loss: 0.01916376402711167, valid_loss: 0.01704952058692773\nFOLD: 6, EPOCH: 34, train_loss: 0.019172078655923113, valid_loss: 0.01704182227452596\nBeginning pretraining for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.7179915081052219, valid_loss: 0.6680015524228414\nFOLD: 0, EPOCH: 1, train_loss: 0.3387628458440304, valid_loss: 0.025758835797508556\nFOLD: 0, EPOCH: 2, train_loss: 0.013715961263241136, valid_loss: 0.005912480332578222\nFOLD: 0, EPOCH: 3, train_loss: 0.008831493666066843, valid_loss: 0.00525024455661575\nFOLD: 0, EPOCH: 4, train_loss: 0.008551015148815863, valid_loss: 0.005003061223154266\nFOLD: 0, EPOCH: 5, train_loss: 0.008452885345939328, valid_loss: 0.005342654030149181\nFOLD: 0, EPOCH: 6, train_loss: 0.008417034028645824, valid_loss: 0.004740003418798248\nFOLD: 0, EPOCH: 7, train_loss: 0.008341424934127751, valid_loss: 0.00505648017860949\nFOLD: 0, EPOCH: 8, train_loss: 0.008327430826337898, valid_loss: 0.005138593648249905\nFOLD: 0, EPOCH: 9, train_loss: 0.008263837751548956, valid_loss: 0.004802287478620808\nFOLD: 0, EPOCH: 10, train_loss: 0.008251187501146513, valid_loss: 0.004973519205426176\nFOLD: 0, EPOCH: 11, train_loss: 0.008217304383459337, valid_loss: 0.005320312765737374\nFOLD: 0, EPOCH: 12, train_loss: 0.008217305780443199, valid_loss: 0.005018796616544326\nFOLD: 0, EPOCH: 13, train_loss: 0.008142691256259294, valid_loss: 0.004949582119782765\nFOLD: 0, EPOCH: 14, train_loss: 0.008095837499508086, valid_loss: 0.004708932984309892\nFOLD: 0, EPOCH: 15, train_loss: 0.008069112321690601, valid_loss: 0.004710603117321928\nFOLD: 0, EPOCH: 16, train_loss: 0.008026951965054168, valid_loss: 0.004890072857961059\nFOLD: 0, EPOCH: 17, train_loss: 0.007986807634177454, valid_loss: 0.00475537427701056\nFOLD: 0, EPOCH: 18, train_loss: 0.007956860655480447, valid_loss: 0.0049042038929959135\nFOLD: 0, EPOCH: 19, train_loss: 0.007934688604162896, valid_loss: 0.004860376473516226\nFOLD: 0, EPOCH: 20, train_loss: 0.007887548465719995, valid_loss: 0.00475776851332436\nFOLD: 0, EPOCH: 21, train_loss: 0.007849212955026066, valid_loss: 0.004921291721984744\nFOLD: 0, EPOCH: 22, train_loss: 0.0078014483647968836, valid_loss: 0.004776093487938245\nFOLD: 0, EPOCH: 23, train_loss: 0.0077600523774676465, valid_loss: 0.004955374325315158\nFOLD: 0, EPOCH: 24, train_loss: 0.007721329943331725, valid_loss: 0.004867276021589835\nBeginning training for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.4114300553851268, valid_loss: 0.03178498117874066\nFOLD: 0, EPOCH: 1, train_loss: 0.026909584124736926, valid_loss: 0.020897419191896915\nFOLD: 0, EPOCH: 2, train_loss: 0.0223227047964054, valid_loss: 0.018960229742030304\nFOLD: 0, EPOCH: 3, train_loss: 0.02147517172510133, valid_loss: 0.019353685279687245\nFOLD: 0, EPOCH: 4, train_loss: 0.02116029430180788, valid_loss: 0.01808007061481476\nFOLD: 0, EPOCH: 5, train_loss: 0.020922663073767635, valid_loss: 0.01814342352251212\nFOLD: 0, EPOCH: 6, train_loss: 0.02079282465445645, valid_loss: 0.017937533867855866\nFOLD: 0, EPOCH: 7, train_loss: 0.020668956067632225, valid_loss: 0.017829847211639088\nFOLD: 0, EPOCH: 8, train_loss: 0.02056539644870688, valid_loss: 0.01809060884018739\nFOLD: 0, EPOCH: 9, train_loss: 0.02045717731337337, valid_loss: 0.017645299434661865\nFOLD: 0, EPOCH: 10, train_loss: 0.020326660343391055, valid_loss: 0.017529289859036606\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 11, train_loss: 0.020192198071847942, valid_loss: 0.01755878422409296\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 12, train_loss: 0.01989282963468748, valid_loss: 0.017679606874783833\nFOLD: 0, EPOCH: 13, train_loss: 0.01972258003319011, valid_loss: 0.017371992270151775\nFOLD: 0, EPOCH: 14, train_loss: 0.019622392568956402, valid_loss: 0.01736669211337964\nFOLD: 0, EPOCH: 15, train_loss: 0.01956598159364041, valid_loss: 0.017519592307507992\nFOLD: 0, EPOCH: 16, train_loss: 0.019535390386248335, valid_loss: 0.01736739029486974\nFOLD: 0, EPOCH: 17, train_loss: 0.019466159920043805, valid_loss: 0.01753561074535052\nFOLD: 0, EPOCH: 18, train_loss: 0.019448868382503006, valid_loss: 0.017311756499111652\nFOLD: 0, EPOCH: 19, train_loss: 0.019388600745621848, valid_loss: 0.01754006650298834\nFOLD: 0, EPOCH: 20, train_loss: 0.01935970613404232, valid_loss: 0.017391622997820377\nFOLD: 0, EPOCH: 21, train_loss: 0.01932750510818818, valid_loss: 0.017349686473608017\nFOLD: 0, EPOCH: 22, train_loss: 0.019298207672203287, valid_loss: 0.01740586602439483\nFOLD: 0, EPOCH: 23, train_loss: 0.01926929985775667, valid_loss: 0.01734690771748622\nFOLD: 0, EPOCH: 24, train_loss: 0.019231845241259125, valid_loss: 0.017389677775402863\nFOLD: 0, EPOCH: 25, train_loss: 0.019212857436607864, valid_loss: 0.017320148336390655\nFOLD: 0, EPOCH: 26, train_loss: 0.01917404555441702, valid_loss: 0.017309679339329403\nFOLD: 0, EPOCH: 27, train_loss: 0.0191704312010723, valid_loss: 0.017353242884079616\nFOLD: 0, EPOCH: 28, train_loss: 0.019155256121474153, valid_loss: 0.017306070153911907\nFOLD: 0, EPOCH: 29, train_loss: 0.01915146761080798, valid_loss: 0.017332202444473904\nFOLD: 0, EPOCH: 30, train_loss: 0.01911929392201059, valid_loss: 0.01730530895292759\nFOLD: 0, EPOCH: 31, train_loss: 0.019117083510055262, valid_loss: 0.017301851883530617\nFOLD: 0, EPOCH: 32, train_loss: 0.019117872066357556, valid_loss: 0.017318814372022946\nFOLD: 0, EPOCH: 33, train_loss: 0.01910998869468184, valid_loss: 0.017314122679332893\nFOLD: 0, EPOCH: 34, train_loss: 0.019126748447032535, valid_loss: 0.017319061793386936\nBeginning pretraining for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.7183390066904181, valid_loss: 0.6669081747531891\nFOLD: 1, EPOCH: 1, train_loss: 0.33856397593284354, valid_loss: 0.025593940168619156\nFOLD: 1, EPOCH: 2, train_loss: 0.013441766146570444, valid_loss: 0.006088483690594633\nFOLD: 1, EPOCH: 3, train_loss: 0.008843398789929994, valid_loss: 0.005468610053261121\nFOLD: 1, EPOCH: 4, train_loss: 0.008512784836485106, valid_loss: 0.005108717285717527\nFOLD: 1, EPOCH: 5, train_loss: 0.008381037041544914, valid_loss: 0.004837594227865338\nFOLD: 1, EPOCH: 6, train_loss: 0.008349947658750941, valid_loss: 0.004979956972723206\nFOLD: 1, EPOCH: 7, train_loss: 0.008280803969896892, valid_loss: 0.005451173676798741\nFOLD: 1, EPOCH: 8, train_loss: 0.008246803587740836, valid_loss: 0.005095563751334946\nFOLD: 1, EPOCH: 9, train_loss: 0.008342745344099753, valid_loss: 0.0049708526736746235\nFOLD: 1, EPOCH: 10, train_loss: 0.008225425790228388, valid_loss: 0.004886404688780506\nFOLD: 1, EPOCH: 11, train_loss: 0.008153625873520094, valid_loss: 0.004717499017715454\nFOLD: 1, EPOCH: 12, train_loss: 0.008118656094131224, valid_loss: 0.004913354137291511\nFOLD: 1, EPOCH: 13, train_loss: 0.008074707228361684, valid_loss: 0.004806724765027563\nFOLD: 1, EPOCH: 14, train_loss: 0.00806625043589841, valid_loss: 0.004872545056665937\nFOLD: 1, EPOCH: 15, train_loss: 0.008026589187519514, valid_loss: 0.00510361270668606\nFOLD: 1, EPOCH: 16, train_loss: 0.007985621827709325, valid_loss: 0.004918681380028526\nFOLD: 1, EPOCH: 17, train_loss: 0.007951059830648935, valid_loss: 0.004971834442888697\nFOLD: 1, EPOCH: 18, train_loss: 0.007911420016385177, valid_loss: 0.0048641283841182785\nFOLD: 1, EPOCH: 19, train_loss: 0.007872425526490106, valid_loss: 0.004826154559850693\nFOLD: 1, EPOCH: 20, train_loss: 0.007824074120863396, valid_loss: 0.005004080167661111\nFOLD: 1, EPOCH: 21, train_loss: 0.007806081434383112, valid_loss: 0.004807197566454609\nFOLD: 1, EPOCH: 22, train_loss: 0.007756658418871024, valid_loss: 0.00514307850971818\nFOLD: 1, EPOCH: 23, train_loss: 0.007708403911879834, valid_loss: 0.005022092955186963\nFOLD: 1, EPOCH: 24, train_loss: 0.007668118293890182, valid_loss: 0.005009689213087161\nBeginning training for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.4196341101299314, valid_loss: 0.03941908416648706\nFOLD: 1, EPOCH: 1, train_loss: 0.02738720441565794, valid_loss: 0.020628093741834164\nFOLD: 1, EPOCH: 2, train_loss: 0.02256362803061219, valid_loss: 0.018200081462661426\nFOLD: 1, EPOCH: 3, train_loss: 0.021442980560309747, valid_loss: 0.01843503738443057\nFOLD: 1, EPOCH: 4, train_loss: 0.02109336420236265, valid_loss: 0.017907824056843918\nFOLD: 1, EPOCH: 5, train_loss: 0.020881469580618775, valid_loss: 0.01792542388041814\nFOLD: 1, EPOCH: 6, train_loss: 0.02076147381654557, valid_loss: 0.017857651536663372\nFOLD: 1, EPOCH: 7, train_loss: 0.02058081334347234, valid_loss: 0.017741230316460133\nFOLD: 1, EPOCH: 8, train_loss: 0.020468419472522596, valid_loss: 0.01791315587858359\nFOLD: 1, EPOCH: 9, train_loss: 0.0204199148232446, valid_loss: 0.0176908556992809\nFOLD: 1, EPOCH: 10, train_loss: 0.02031199601204956, valid_loss: 0.017672165917853516\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 11, train_loss: 0.02022574277704253, valid_loss: 0.017624093530078728\nFOLD: 1, EPOCH: 12, train_loss: 0.019919050035669524, valid_loss: 0.017472919387121994\nFOLD: 1, EPOCH: 13, train_loss: 0.019828247618587577, valid_loss: 0.017687410737077396\nFOLD: 1, EPOCH: 14, train_loss: 0.01977122071034768, valid_loss: 0.017590174761911232\nFOLD: 1, EPOCH: 15, train_loss: 0.01967369408949333, valid_loss: 0.017461780458688736\nFOLD: 1, EPOCH: 16, train_loss: 0.019585894146824583, valid_loss: 0.017438781758149464\nFOLD: 1, EPOCH: 17, train_loss: 0.0195379421553191, valid_loss: 0.017306553820768993\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 18, train_loss: 0.01940128842697424, valid_loss: 0.01733601527909438\nFOLD: 1, EPOCH: 19, train_loss: 0.019335144966402474, valid_loss: 0.017351665534079075\nFOLD: 1, EPOCH: 20, train_loss: 0.019303280805401942, valid_loss: 0.01723632433762153\nFOLD: 1, EPOCH: 21, train_loss: 0.019225427716532174, valid_loss: 0.01727888050178687\nFOLD: 1, EPOCH: 22, train_loss: 0.01921701173791114, valid_loss: 0.017413282456497352\nFOLD: 1, EPOCH: 23, train_loss: 0.01917013745097553, valid_loss: 0.0172536950558424\nFOLD: 1, EPOCH: 24, train_loss: 0.01910771062488065, valid_loss: 0.017315118573606014\nFOLD: 1, EPOCH: 25, train_loss: 0.019105637884315324, valid_loss: 0.0172698808213075\nFOLD: 1, EPOCH: 26, train_loss: 0.01906500422560117, valid_loss: 0.01725817130257686\nFOLD: 1, EPOCH: 27, train_loss: 0.019039032213828144, valid_loss: 0.01727785635739565\nFOLD: 1, EPOCH: 28, train_loss: 0.019012999556520405, valid_loss: 0.01730070977161328\nFOLD: 1, EPOCH: 29, train_loss: 0.01900154828806134, valid_loss: 0.017259785595039528\nFOLD: 1, EPOCH: 30, train_loss: 0.018977176507606226, valid_loss: 0.017248816788196564\nFOLD: 1, EPOCH: 31, train_loss: 0.01898480036898571, valid_loss: 0.017259828746318817\nFOLD: 1, EPOCH: 32, train_loss: 0.01898958073819385, valid_loss: 0.017253101182480652\nFOLD: 1, EPOCH: 33, train_loss: 0.01896174454732853, valid_loss: 0.017259603676696617\nFOLD: 1, EPOCH: 34, train_loss: 0.018979249193387872, valid_loss: 0.017264522922535736\nBeginning pretraining for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.7184992081978742, valid_loss: 0.6683353384335836\nFOLD: 2, EPOCH: 1, train_loss: 0.34083471666364107, valid_loss: 0.0262512918561697\nFOLD: 2, EPOCH: 2, train_loss: 0.013326402096187366, valid_loss: 0.005892930862804254\nFOLD: 2, EPOCH: 3, train_loss: 0.008878893985906067, valid_loss: 0.005133352475240827\nFOLD: 2, EPOCH: 4, train_loss: 0.008560874910258195, valid_loss: 0.0048318154489000635\nFOLD: 2, EPOCH: 5, train_loss: 0.008466583265758613, valid_loss: 0.004948451882228255\nFOLD: 2, EPOCH: 6, train_loss: 0.008467642439748435, valid_loss: 0.004612261313013732\nFOLD: 2, EPOCH: 7, train_loss: 0.00837404586319976, valid_loss: 0.00502390608501931\nFOLD: 2, EPOCH: 8, train_loss: 0.00832775526899187, valid_loss: 0.004565351875498891\nFOLD: 2, EPOCH: 9, train_loss: 0.008291961063685663, valid_loss: 0.004775774509956439\nFOLD: 2, EPOCH: 10, train_loss: 0.008262581726097885, valid_loss: 0.005406295492624243\nFOLD: 2, EPOCH: 11, train_loss: 0.008217053939862287, valid_loss: 0.005017645036180814\nFOLD: 2, EPOCH: 12, train_loss: 0.008193023105645004, valid_loss: 0.005122360540553927\nFOLD: 2, EPOCH: 13, train_loss: 0.008138151798287736, valid_loss: 0.004505799850448966\nFOLD: 2, EPOCH: 14, train_loss: 0.008121802772888365, valid_loss: 0.004739099609044691\nFOLD: 2, EPOCH: 15, train_loss: 0.008084657300701913, valid_loss: 0.004896939033642411\nFOLD: 2, EPOCH: 16, train_loss: 0.008055053829379818, valid_loss: 0.004690458532422781\nFOLD: 2, EPOCH: 17, train_loss: 0.007997319910346586, valid_loss: 0.004736477237505217\nFOLD: 2, EPOCH: 18, train_loss: 0.007954279056695454, valid_loss: 0.004962183612709244\nFOLD: 2, EPOCH: 19, train_loss: 0.007925372584449017, valid_loss: 0.005132623637715976\nFOLD: 2, EPOCH: 20, train_loss: 0.00790050795630497, valid_loss: 0.005008302240942915\nFOLD: 2, EPOCH: 21, train_loss: 0.007854517193182427, valid_loss: 0.004824295950432618\nFOLD: 2, EPOCH: 22, train_loss: 0.007815570967710194, valid_loss: 0.005102808742473523\nFOLD: 2, EPOCH: 23, train_loss: 0.007771913249812582, valid_loss: 0.004800754211222132\nFOLD: 2, EPOCH: 24, train_loss: 0.007742684170165483, valid_loss: 0.004821631281326215\nBeginning training for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.4087188412599704, valid_loss: 0.036563451091448464\nFOLD: 2, EPOCH: 1, train_loss: 0.027018910876530057, valid_loss: 0.02115045953541994\nFOLD: 2, EPOCH: 2, train_loss: 0.0223117410479223, valid_loss: 0.019945309186975162\nFOLD: 2, EPOCH: 3, train_loss: 0.021372060017550692, valid_loss: 0.01927137530098359\nFOLD: 2, EPOCH: 4, train_loss: 0.021034151096554363, valid_loss: 0.018736426408092182\nFOLD: 2, EPOCH: 5, train_loss: 0.02083579883636797, valid_loss: 0.01875670626759529\nFOLD: 2, EPOCH: 6, train_loss: 0.020658012534327367, valid_loss: 0.01875853134940068\nFOLD: 2, EPOCH: 7, train_loss: 0.020554037216831657, valid_loss: 0.018461812908450764\nFOLD: 2, EPOCH: 8, train_loss: 0.02050367123721277, valid_loss: 0.01848044681052367\nFOLD: 2, EPOCH: 9, train_loss: 0.02033311835325816, valid_loss: 0.018460587908824284\nFOLD: 2, EPOCH: 10, train_loss: 0.02025470816913773, valid_loss: 0.018285281645754974\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 11, train_loss: 0.020172506954301807, valid_loss: 0.01817977335304022\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 12, train_loss: 0.019915502134929684, valid_loss: 0.01805623092999061\nFOLD: 2, EPOCH: 13, train_loss: 0.01970122425871737, valid_loss: 0.018119599980612595\nFOLD: 2, EPOCH: 14, train_loss: 0.019568738472812316, valid_loss: 0.0182610172778368\nFOLD: 2, EPOCH: 15, train_loss: 0.019507875337320214, valid_loss: 0.01822648694117864\nFOLD: 2, EPOCH: 16, train_loss: 0.019451887620722547, valid_loss: 0.018100361960629623\nFOLD: 2, EPOCH: 17, train_loss: 0.019421218127450523, valid_loss: 0.018008971276382606\nFOLD: 2, EPOCH: 18, train_loss: 0.01936033116105725, valid_loss: 0.018153530545532703\nFOLD: 2, EPOCH: 19, train_loss: 0.01931480553877704, valid_loss: 0.018007167614996433\nFOLD: 2, EPOCH: 20, train_loss: 0.019292321549180674, valid_loss: 0.017934855694572132\nFOLD: 2, EPOCH: 21, train_loss: 0.01927267644992646, valid_loss: 0.01799834209183852\nFOLD: 2, EPOCH: 22, train_loss: 0.019211571444483364, valid_loss: 0.018016146185497444\nFOLD: 2, EPOCH: 23, train_loss: 0.019201058675261104, valid_loss: 0.017986719806989033\nFOLD: 2, EPOCH: 24, train_loss: 0.0191564532544683, valid_loss: 0.017964675401647884\nFOLD: 2, EPOCH: 25, train_loss: 0.01915343029095846, valid_loss: 0.017940268230934937\nFOLD: 2, EPOCH: 26, train_loss: 0.019109964315943858, valid_loss: 0.017977607436478138\nFOLD: 2, EPOCH: 27, train_loss: 0.019090302178964895, valid_loss: 0.017957598591844242\nFOLD: 2, EPOCH: 28, train_loss: 0.019071102087550303, valid_loss: 0.017979909665882587\nFOLD: 2, EPOCH: 29, train_loss: 0.019045923453043487, valid_loss: 0.01795948234697183\nFOLD: 2, EPOCH: 30, train_loss: 0.01903832402518567, valid_loss: 0.017961998159686726\nFOLD: 2, EPOCH: 31, train_loss: 0.019026088528335094, valid_loss: 0.017953382494548958\nFOLD: 2, EPOCH: 32, train_loss: 0.01903153134181219, valid_loss: 0.0179598955437541\nFOLD: 2, EPOCH: 33, train_loss: 0.019026474205448347, valid_loss: 0.017967510658005875\nFOLD: 2, EPOCH: 34, train_loss: 0.01904405965743696, valid_loss: 0.01796013582497835\nBeginning pretraining for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.7180252250503091, valid_loss: 0.6686948339144388\nFOLD: 3, EPOCH: 1, train_loss: 0.3376077880315921, valid_loss: 0.025526841791967552\nFOLD: 3, EPOCH: 2, train_loss: 0.013664871837724657, valid_loss: 0.006015318135420482\nFOLD: 3, EPOCH: 3, train_loss: 0.008863739066702478, valid_loss: 0.005174930052210887\nFOLD: 3, EPOCH: 4, train_loss: 0.008555932833320078, valid_loss: 0.005164885117361943\nFOLD: 3, EPOCH: 5, train_loss: 0.008456512956934817, valid_loss: 0.004842876767118772\nFOLD: 3, EPOCH: 6, train_loss: 0.008444534737945479, valid_loss: 0.004850651680802305\nFOLD: 3, EPOCH: 7, train_loss: 0.00835371357114876, valid_loss: 0.004814384582762917\nFOLD: 3, EPOCH: 8, train_loss: 0.008313485270585208, valid_loss: 0.004773181397467852\nFOLD: 3, EPOCH: 9, train_loss: 0.008295773679171415, valid_loss: 0.005483378345767657\nFOLD: 3, EPOCH: 10, train_loss: 0.00823187014526304, valid_loss: 0.004823067458346486\nFOLD: 3, EPOCH: 11, train_loss: 0.008250128518899573, valid_loss: 0.004638709283123414\nFOLD: 3, EPOCH: 12, train_loss: 0.008166575009989388, valid_loss: 0.005248074575016896\nFOLD: 3, EPOCH: 13, train_loss: 0.00813186272759648, valid_loss: 0.0048604625432441635\nFOLD: 3, EPOCH: 14, train_loss: 0.008117730633410461, valid_loss: 0.004907236046468218\nFOLD: 3, EPOCH: 15, train_loss: 0.00808486936833052, valid_loss: 0.005768759098524849\nFOLD: 3, EPOCH: 16, train_loss: 0.008048113791600746, valid_loss: 0.005012640884766976\nFOLD: 3, EPOCH: 17, train_loss: 0.007998268106294906, valid_loss: 0.004763181476543347\nFOLD: 3, EPOCH: 18, train_loss: 0.007968597196261673, valid_loss: 0.004711446817964315\nFOLD: 3, EPOCH: 19, train_loss: 0.007913252023760886, valid_loss: 0.005046242071936528\nFOLD: 3, EPOCH: 20, train_loss: 0.007902659872985062, valid_loss: 0.004846449475735426\nFOLD: 3, EPOCH: 21, train_loss: 0.007844995953800046, valid_loss: 0.004763786525775989\nFOLD: 3, EPOCH: 22, train_loss: 0.0077862878850496866, valid_loss: 0.004822442540898919\nFOLD: 3, EPOCH: 23, train_loss: 0.007740314775968299, valid_loss: 0.004921486368402839\nFOLD: 3, EPOCH: 24, train_loss: 0.007690156618242755, valid_loss: 0.004919972891608874\nBeginning training for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.41523915737429085, valid_loss: 0.04246317160626253\nFOLD: 3, EPOCH: 1, train_loss: 0.0275069704717573, valid_loss: 0.02096536010503769\nFOLD: 3, EPOCH: 2, train_loss: 0.022381229106994235, valid_loss: 0.019725325517356396\nFOLD: 3, EPOCH: 3, train_loss: 0.021385493523934308, valid_loss: 0.01850119326263666\nFOLD: 3, EPOCH: 4, train_loss: 0.021087335970471886, valid_loss: 0.017886589281260967\nFOLD: 3, EPOCH: 5, train_loss: 0.02087669644285651, valid_loss: 0.01818513994415601\nFOLD: 3, EPOCH: 6, train_loss: 0.020692383103510913, valid_loss: 0.017707894866665203\nFOLD: 3, EPOCH: 7, train_loss: 0.020598782993414822, valid_loss: 0.017794875738521416\nFOLD: 3, EPOCH: 8, train_loss: 0.02050709817558527, valid_loss: 0.01780231265972058\nFOLD: 3, EPOCH: 9, train_loss: 0.020425204242415288, valid_loss: 0.01763693057000637\nFOLD: 3, EPOCH: 10, train_loss: 0.02031999727820649, valid_loss: 0.01778401465465625\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 11, train_loss: 0.020173429566271165, valid_loss: 0.01741699781268835\nFOLD: 3, EPOCH: 12, train_loss: 0.019955251201548996, valid_loss: 0.017549818692107994\nFOLD: 3, EPOCH: 13, train_loss: 0.019825279383974916, valid_loss: 0.01735324723025163\nFOLD: 3, EPOCH: 14, train_loss: 0.019825826618163025, valid_loss: 0.017383387312293053\nFOLD: 3, EPOCH: 15, train_loss: 0.01972206205348758, valid_loss: 0.01742414788653453\nFOLD: 3, EPOCH: 16, train_loss: 0.019661923706093255, valid_loss: 0.017437233900030453\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 17, train_loss: 0.01962237257291289, valid_loss: 0.017284128814935684\nFOLD: 3, EPOCH: 18, train_loss: 0.0194852469379411, valid_loss: 0.01724121781686942\nFOLD: 3, EPOCH: 19, train_loss: 0.01941040125401581, valid_loss: 0.017439579280714195\nFOLD: 3, EPOCH: 20, train_loss: 0.019370150259312463, valid_loss: 0.017328840990861256\nFOLD: 3, EPOCH: 21, train_loss: 0.0193413135540836, valid_loss: 0.01723441792031129\nFOLD: 3, EPOCH: 22, train_loss: 0.019280490958515334, valid_loss: 0.017171423882246017\nFOLD: 3, EPOCH: 23, train_loss: 0.019221411579672026, valid_loss: 0.01716867151359717\nFOLD: 3, EPOCH: 24, train_loss: 0.01916847180794267, valid_loss: 0.01717664549748103\nFOLD: 3, EPOCH: 25, train_loss: 0.019194293383728054, valid_loss: 0.017178760841488838\nFOLD: 3, EPOCH: 26, train_loss: 0.01915956469362273, valid_loss: 0.017144596204161644\nFOLD: 3, EPOCH: 27, train_loss: 0.01912374799961553, valid_loss: 0.017179175280034542\nFOLD: 3, EPOCH: 28, train_loss: 0.01908840425312519, valid_loss: 0.01713643440355857\nFOLD: 3, EPOCH: 29, train_loss: 0.019088956417844576, valid_loss: 0.017126137080291908\nFOLD: 3, EPOCH: 30, train_loss: 0.01908762292826877, valid_loss: 0.017135946390529472\nFOLD: 3, EPOCH: 31, train_loss: 0.019067115150392056, valid_loss: 0.017115891290207703\nFOLD: 3, EPOCH: 32, train_loss: 0.01907154797192882, valid_loss: 0.017125462802747887\nFOLD: 3, EPOCH: 33, train_loss: 0.01904335124965976, valid_loss: 0.01712678714344899\nFOLD: 3, EPOCH: 34, train_loss: 0.019053479492226067, valid_loss: 0.017118678738673527\nBeginning pretraining for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.7181247928563286, valid_loss: 0.6672641436258951\nFOLD: 4, EPOCH: 1, train_loss: 0.3399114502484308, valid_loss: 0.026096925760308903\nFOLD: 4, EPOCH: 2, train_loss: 0.01344061955152189, valid_loss: 0.005975784578671058\nFOLD: 4, EPOCH: 3, train_loss: 0.008857107310391524, valid_loss: 0.005227448962007959\nFOLD: 4, EPOCH: 4, train_loss: 0.008514734231592977, valid_loss: 0.005124720511958003\nFOLD: 4, EPOCH: 5, train_loss: 0.008406381246031207, valid_loss: 0.004599382868036628\nFOLD: 4, EPOCH: 6, train_loss: 0.008396853651741849, valid_loss: 0.004783733127017816\nFOLD: 4, EPOCH: 7, train_loss: 0.008276820306063574, valid_loss: 0.004704916849732399\nFOLD: 4, EPOCH: 8, train_loss: 0.00825293640167836, valid_loss: 0.005014743035038312\nFOLD: 4, EPOCH: 9, train_loss: 0.008205606355605757, valid_loss: 0.005040667407835524\nFOLD: 4, EPOCH: 10, train_loss: 0.008188572794418125, valid_loss: 0.005300648355235656\nFOLD: 4, EPOCH: 11, train_loss: 0.008135602987535736, valid_loss: 0.005310799150417249\nFOLD: 4, EPOCH: 12, train_loss: 0.00809317410868757, valid_loss: 0.006025343202054501\nFOLD: 4, EPOCH: 13, train_loss: 0.008080757606555434, valid_loss: 0.004700666371112068\nFOLD: 4, EPOCH: 14, train_loss: 0.008044982397490564, valid_loss: 0.004702247601623337\nFOLD: 4, EPOCH: 15, train_loss: 0.008013160967761102, valid_loss: 0.005314724907899897\nFOLD: 4, EPOCH: 16, train_loss: 0.007968030335819897, valid_loss: 0.004612734541296959\nFOLD: 4, EPOCH: 17, train_loss: 0.007923637270270026, valid_loss: 0.00491659645922482\nFOLD: 4, EPOCH: 18, train_loss: 0.007881475365994609, valid_loss: 0.004929420538246632\nFOLD: 4, EPOCH: 19, train_loss: 0.007858980693580472, valid_loss: 0.0048588605131953955\nFOLD: 4, EPOCH: 20, train_loss: 0.007807802217190757, valid_loss: 0.0048125005171944695\nFOLD: 4, EPOCH: 21, train_loss: 0.007784349186455502, valid_loss: 0.004825207249571879\nFOLD: 4, EPOCH: 22, train_loss: 0.007734264123856145, valid_loss: 0.004813120933249593\nFOLD: 4, EPOCH: 23, train_loss: 0.007695644162595272, valid_loss: 0.004834664752706885\nFOLD: 4, EPOCH: 24, train_loss: 0.007656208828420323, valid_loss: 0.004759185326596101\nBeginning training for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.4359012853135081, valid_loss: 0.04693170450627804\nFOLD: 4, EPOCH: 1, train_loss: 0.028913270024692312, valid_loss: 0.021132492770751316\nFOLD: 4, EPOCH: 2, train_loss: 0.022854135043042546, valid_loss: 0.019521691525975864\nFOLD: 4, EPOCH: 3, train_loss: 0.021576920305104816, valid_loss: 0.01867510285228491\nFOLD: 4, EPOCH: 4, train_loss: 0.021190531661405283, valid_loss: 0.01866364696373542\nFOLD: 4, EPOCH: 5, train_loss: 0.020790089579189524, valid_loss: 0.018376893053452175\nFOLD: 4, EPOCH: 6, train_loss: 0.020660194075282884, valid_loss: 0.018025368452072144\nFOLD: 4, EPOCH: 7, train_loss: 0.020525884431074646, valid_loss: 0.01803032650301854\nFOLD: 4, EPOCH: 8, train_loss: 0.020326398477396545, valid_loss: 0.018225692833463352\nFOLD: 4, EPOCH: 9, train_loss: 0.02015355322510004, valid_loss: 0.018000871563951176\nFOLD: 4, EPOCH: 10, train_loss: 0.020138665759826407, valid_loss: 0.017899740176896255\nFOLD: 4, EPOCH: 11, train_loss: 0.02007790857597309, valid_loss: 0.01796656505515178\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 12, train_loss: 0.019965573980966034, valid_loss: 0.017687276005744934\nFOLD: 4, EPOCH: 13, train_loss: 0.01965437298092772, valid_loss: 0.018019499878088634\nFOLD: 4, EPOCH: 14, train_loss: 0.019533777992953274, valid_loss: 0.017678372872372467\nFOLD: 4, EPOCH: 15, train_loss: 0.019467884345966226, valid_loss: 0.017677306818465393\nFOLD: 4, EPOCH: 16, train_loss: 0.019410696483271962, valid_loss: 0.017651654159029324\nFOLD: 4, EPOCH: 17, train_loss: 0.01935445878873853, valid_loss: 0.017775484671195347\nFOLD: 4, EPOCH: 18, train_loss: 0.01924308482557535, valid_loss: 0.01759935573985179\nFOLD: 4, EPOCH: 19, train_loss: 0.019208533196326566, valid_loss: 0.01769223560889562\nFOLD: 4, EPOCH: 20, train_loss: 0.019136980121188304, valid_loss: 0.017761968076229095\nFOLD: 4, EPOCH: 21, train_loss: 0.019075147917165476, valid_loss: 0.01760414025435845\nFOLD: 4, EPOCH: 22, train_loss: 0.0189872556092108, valid_loss: 0.017583599934975307\nFOLD: 4, EPOCH: 23, train_loss: 0.01894237276385812, valid_loss: 0.017521290108561516\nFOLD: 4, EPOCH: 24, train_loss: 0.01884944468517514, valid_loss: 0.017522612276176613\nFOLD: 4, EPOCH: 25, train_loss: 0.01875766525592874, valid_loss: 0.01758447413643201\nFOLD: 4, EPOCH: 26, train_loss: 0.01869580281131408, valid_loss: 0.01750261088212331\nFOLD: 4, EPOCH: 27, train_loss: 0.018605624270789763, valid_loss: 0.01754469083001216\nFOLD: 4, EPOCH: 28, train_loss: 0.018562238334732896, valid_loss: 0.017516129339734714\nFOLD: 4, EPOCH: 29, train_loss: 0.018453999915543723, valid_loss: 0.017488613724708557\nFOLD: 4, EPOCH: 30, train_loss: 0.018422044649281922, valid_loss: 0.01748942056049903\nFOLD: 4, EPOCH: 31, train_loss: 0.018356620553223527, valid_loss: 0.017486616658667724\nFOLD: 4, EPOCH: 32, train_loss: 0.018314870896146578, valid_loss: 0.017482758810122807\nFOLD: 4, EPOCH: 33, train_loss: 0.018317621529978866, valid_loss: 0.01748240242401759\nFOLD: 4, EPOCH: 34, train_loss: 0.018276345203904545, valid_loss: 0.017490912849704426\nBeginning pretraining for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.7184609525343951, valid_loss: 0.6693070034186045\nFOLD: 5, EPOCH: 1, train_loss: 0.3413939073900966, valid_loss: 0.027006663071612518\nFOLD: 5, EPOCH: 2, train_loss: 0.013393857817658606, valid_loss: 0.0059649265216042595\nFOLD: 5, EPOCH: 3, train_loss: 0.008751549100612892, valid_loss: 0.0051836382287244005\nFOLD: 5, EPOCH: 4, train_loss: 0.008457949713749044, valid_loss: 0.005170128385846813\nFOLD: 5, EPOCH: 5, train_loss: 0.008399378152235466, valid_loss: 0.005102076878150304\nFOLD: 5, EPOCH: 6, train_loss: 0.008315396139069516, valid_loss: 0.0048368283702681465\nFOLD: 5, EPOCH: 7, train_loss: 0.008279871442081296, valid_loss: 0.005046573855603735\nFOLD: 5, EPOCH: 8, train_loss: 0.008248391506426474, valid_loss: 0.004992150391141574\nFOLD: 5, EPOCH: 9, train_loss: 0.008213983355637859, valid_loss: 0.004754805006086826\nFOLD: 5, EPOCH: 10, train_loss: 0.008178524385370752, valid_loss: 0.005456325365230441\nFOLD: 5, EPOCH: 11, train_loss: 0.008147761170916697, valid_loss: 0.005960811239977677\nFOLD: 5, EPOCH: 12, train_loss: 0.008127115379251978, valid_loss: 0.004837598962088426\nFOLD: 5, EPOCH: 13, train_loss: 0.008097972734557354, valid_loss: 0.0048311788123101\nFOLD: 5, EPOCH: 14, train_loss: 0.008043718373622088, valid_loss: 0.004643470204124848\nFOLD: 5, EPOCH: 15, train_loss: 0.008016690077698407, valid_loss: 0.004800473727906744\nFOLD: 5, EPOCH: 16, train_loss: 0.007986916229128838, valid_loss: 0.005485980849092205\nFOLD: 5, EPOCH: 17, train_loss: 0.007944621310076293, valid_loss: 0.004809618539487322\nFOLD: 5, EPOCH: 18, train_loss: 0.0079120689564768, valid_loss: 0.0047472266014665365\nFOLD: 5, EPOCH: 19, train_loss: 0.007862230009563705, valid_loss: 0.005047963233664632\nFOLD: 5, EPOCH: 20, train_loss: 0.007819940897581331, valid_loss: 0.004880099867781003\nFOLD: 5, EPOCH: 21, train_loss: 0.0077638346017064415, valid_loss: 0.00485626352019608\nFOLD: 5, EPOCH: 22, train_loss: 0.007719926303252578, valid_loss: 0.004759504226967692\nFOLD: 5, EPOCH: 23, train_loss: 0.007662551176241215, valid_loss: 0.004787772428244352\nFOLD: 5, EPOCH: 24, train_loss: 0.007627841153675143, valid_loss: 0.004796789648632209\nBeginning training for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.4099596093463547, valid_loss: 0.029051025708516438\nFOLD: 5, EPOCH: 1, train_loss: 0.02675641459577224, valid_loss: 0.02108770205328862\nFOLD: 5, EPOCH: 2, train_loss: 0.022257584749775773, valid_loss: 0.02570132352411747\nFOLD: 5, EPOCH: 3, train_loss: 0.021676748662310487, valid_loss: 0.018898581465085346\nFOLD: 5, EPOCH: 4, train_loss: 0.021079017616370144, valid_loss: 0.019131443463265896\nFOLD: 5, EPOCH: 5, train_loss: 0.020882889299708253, valid_loss: 0.018502772164841492\nFOLD: 5, EPOCH: 6, train_loss: 0.020717776763965103, valid_loss: 0.018315098869303863\nFOLD: 5, EPOCH: 7, train_loss: 0.020653088248389607, valid_loss: 0.01816929007569949\nFOLD: 5, EPOCH: 8, train_loss: 0.0205325824482476, valid_loss: 0.018321196548640728\nFOLD: 5, EPOCH: 9, train_loss: 0.020426005563315225, valid_loss: 0.018275178037583828\nFOLD: 5, EPOCH: 10, train_loss: 0.020311504383297527, valid_loss: 0.018197254588206608\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 11, train_loss: 0.020278049578123233, valid_loss: 0.01800510349373023\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 12, train_loss: 0.019960420808809644, valid_loss: 0.01801567431539297\nFOLD: 5, EPOCH: 13, train_loss: 0.019742125754847246, valid_loss: 0.017943047918379307\nFOLD: 5, EPOCH: 14, train_loss: 0.019664067884578425, valid_loss: 0.01787753279010455\nFOLD: 5, EPOCH: 15, train_loss: 0.01959684622638366, valid_loss: 0.017802823955814045\nFOLD: 5, EPOCH: 16, train_loss: 0.01954707454013474, valid_loss: 0.01776729989796877\nFOLD: 5, EPOCH: 17, train_loss: 0.01948945733773358, valid_loss: 0.01773501591136058\nFOLD: 5, EPOCH: 18, train_loss: 0.019424348193056443, valid_loss: 0.017787720387180645\nFOLD: 5, EPOCH: 19, train_loss: 0.01942575936589171, valid_loss: 0.01783839923640092\nFOLD: 5, EPOCH: 20, train_loss: 0.019367467118974996, valid_loss: 0.017754073875645798\nFOLD: 5, EPOCH: 21, train_loss: 0.019358388970003408, valid_loss: 0.01783912566800912\nFOLD: 5, EPOCH: 22, train_loss: 0.019305728430695394, valid_loss: 0.017739467943708103\nFOLD: 5, EPOCH: 23, train_loss: 0.019285626983379617, valid_loss: 0.01775565029432376\nFOLD: 5, EPOCH: 24, train_loss: 0.01927683118949918, valid_loss: 0.01773594133555889\nFOLD: 5, EPOCH: 25, train_loss: 0.019252126324264443, valid_loss: 0.01776312788327535\nFOLD: 5, EPOCH: 26, train_loss: 0.01920689954696333, valid_loss: 0.017772805877029896\nFOLD: 5, EPOCH: 27, train_loss: 0.01920063656700008, valid_loss: 0.0177570100252827\nFOLD: 5, EPOCH: 28, train_loss: 0.019174479824655196, valid_loss: 0.017724294525881607\nFOLD: 5, EPOCH: 29, train_loss: 0.019147402990390274, valid_loss: 0.017734625066320103\nFOLD: 5, EPOCH: 30, train_loss: 0.019129189498284283, valid_loss: 0.017713002550105255\nFOLD: 5, EPOCH: 31, train_loss: 0.01915339911904405, valid_loss: 0.01771124576528867\nFOLD: 5, EPOCH: 32, train_loss: 0.01914661279057755, valid_loss: 0.017724586029847462\nFOLD: 5, EPOCH: 33, train_loss: 0.01911655397099607, valid_loss: 0.01772357492397229\nFOLD: 5, EPOCH: 34, train_loss: 0.019149292479543126, valid_loss: 0.017717683066924412\nBeginning pretraining for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.7184202373027802, valid_loss: 0.6685102184613546\nFOLD: 6, EPOCH: 1, train_loss: 0.3385488039211315, valid_loss: 0.026507156901061535\nFOLD: 6, EPOCH: 2, train_loss: 0.013640770661261152, valid_loss: 0.005852321318040292\nFOLD: 6, EPOCH: 3, train_loss: 0.008769025998737882, valid_loss: 0.005120774731040001\nFOLD: 6, EPOCH: 4, train_loss: 0.008445998777032775, valid_loss: 0.004997121791044871\nFOLD: 6, EPOCH: 5, train_loss: 0.008339356165379286, valid_loss: 0.004570553389688333\nFOLD: 6, EPOCH: 6, train_loss: 0.008321358096402358, valid_loss: 0.004979820844406883\nFOLD: 6, EPOCH: 7, train_loss: 0.008257712360800189, valid_loss: 0.004634754887471597\nFOLD: 6, EPOCH: 8, train_loss: 0.008211107746533611, valid_loss: 0.004694766830652952\nFOLD: 6, EPOCH: 9, train_loss: 0.008146559573052561, valid_loss: 0.005016741265232365\nFOLD: 6, EPOCH: 10, train_loss: 0.00812971089309191, valid_loss: 0.004647698796664675\nFOLD: 6, EPOCH: 11, train_loss: 0.008084344362620921, valid_loss: 0.005849836161360145\nFOLD: 6, EPOCH: 12, train_loss: 0.008020274094579852, valid_loss: 0.004821516806259751\nFOLD: 6, EPOCH: 13, train_loss: 0.008009211215026239, valid_loss: 0.005000942541907231\nFOLD: 6, EPOCH: 14, train_loss: 0.007967090508078827, valid_loss: 0.004558663970480363\nFOLD: 6, EPOCH: 15, train_loss: 0.007940913988825153, valid_loss: 0.004471279098652303\nFOLD: 6, EPOCH: 16, train_loss: 0.007913955898188493, valid_loss: 0.004629310375700395\nFOLD: 6, EPOCH: 17, train_loss: 0.007876462193534654, valid_loss: 0.004506682938275238\nFOLD: 6, EPOCH: 18, train_loss: 0.007837125771295498, valid_loss: 0.004444822591419022\nFOLD: 6, EPOCH: 19, train_loss: 0.007802028099403661, valid_loss: 0.004651418887078762\nFOLD: 6, EPOCH: 20, train_loss: 0.007764314356095651, valid_loss: 0.004744039227565129\nFOLD: 6, EPOCH: 21, train_loss: 0.00772476990652435, valid_loss: 0.004872142880534132\nFOLD: 6, EPOCH: 22, train_loss: 0.007683746215394314, valid_loss: 0.004771946929395199\nFOLD: 6, EPOCH: 23, train_loss: 0.007647487841656103, valid_loss: 0.004936370300129056\nFOLD: 6, EPOCH: 24, train_loss: 0.007591722368755762, valid_loss: 0.004679328451553981\nBeginning training for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.4055423274197999, valid_loss: 0.032788561657071114\nFOLD: 6, EPOCH: 1, train_loss: 0.026946784106685835, valid_loss: 0.02005447664608558\nFOLD: 6, EPOCH: 2, train_loss: 0.02266289315679494, valid_loss: 0.019788332593937714\nFOLD: 6, EPOCH: 3, train_loss: 0.021669520284323132, valid_loss: 0.01825632620602846\nFOLD: 6, EPOCH: 4, train_loss: 0.021280135959386826, valid_loss: 0.018136545705298584\nFOLD: 6, EPOCH: 5, train_loss: 0.021104646517949945, valid_loss: 0.01799889902273814\nFOLD: 6, EPOCH: 6, train_loss: 0.02089583479306277, valid_loss: 0.017640837157766025\nFOLD: 6, EPOCH: 7, train_loss: 0.020745588850010845, valid_loss: 0.017873714057107765\nFOLD: 6, EPOCH: 8, train_loss: 0.020639072994098943, valid_loss: 0.01776728282372157\nFOLD: 6, EPOCH: 9, train_loss: 0.020506239605738837, valid_loss: 0.017593278860052425\nFOLD: 6, EPOCH: 10, train_loss: 0.020417360205422428, valid_loss: 0.01743371256937583\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 11, train_loss: 0.020314740071840146, valid_loss: 0.017509877992173035\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 12, train_loss: 0.020015595042530226, valid_loss: 0.017329364083707333\nFOLD: 6, EPOCH: 13, train_loss: 0.019887313630212757, valid_loss: 0.017155833231906097\nFOLD: 6, EPOCH: 14, train_loss: 0.019782121293246746, valid_loss: 0.017060114070773125\nFOLD: 6, EPOCH: 15, train_loss: 0.01973162279190386, valid_loss: 0.017103688791394234\nFOLD: 6, EPOCH: 16, train_loss: 0.01967054232954979, valid_loss: 0.017133696625630062\nFOLD: 6, EPOCH: 17, train_loss: 0.01960708058493979, valid_loss: 0.01714690464238326\nFOLD: 6, EPOCH: 18, train_loss: 0.019557835260296568, valid_loss: 0.017246315876642864\nFOLD: 6, EPOCH: 19, train_loss: 0.019511643249322388, valid_loss: 0.017146046583851177\nFOLD: 6, EPOCH: 20, train_loss: 0.01950800807817894, valid_loss: 0.017147285553316276\nFOLD: 6, EPOCH: 21, train_loss: 0.019466358510886923, valid_loss: 0.01712614173690478\nFOLD: 6, EPOCH: 22, train_loss: 0.01944901712019654, valid_loss: 0.017188353464007378\nFOLD: 6, EPOCH: 23, train_loss: 0.019437023657648003, valid_loss: 0.017123465115825336\nFOLD: 6, EPOCH: 24, train_loss: 0.01937000108334948, valid_loss: 0.017005609969298046\nFOLD: 6, EPOCH: 25, train_loss: 0.019359757575918648, valid_loss: 0.017037695584197838\nFOLD: 6, EPOCH: 26, train_loss: 0.019331261789535776, valid_loss: 0.017003002886970837\nFOLD: 6, EPOCH: 27, train_loss: 0.019324124900295454, valid_loss: 0.016995900000135105\nFOLD: 6, EPOCH: 28, train_loss: 0.019297376439413604, valid_loss: 0.01697600819170475\nFOLD: 6, EPOCH: 29, train_loss: 0.019291775301098824, valid_loss: 0.017014155785242718\nFOLD: 6, EPOCH: 30, train_loss: 0.019247026292278487, valid_loss: 0.01701444201171398\nFOLD: 6, EPOCH: 31, train_loss: 0.019264396718319726, valid_loss: 0.016992329619824886\nFOLD: 6, EPOCH: 32, train_loss: 0.01925390071290381, valid_loss: 0.01699647804101308\nFOLD: 6, EPOCH: 33, train_loss: 0.019248367561136976, valid_loss: 0.016988953265051048\nFOLD: 6, EPOCH: 34, train_loss: 0.01924510546686018, valid_loss: 0.01699571715046962\nBeginning pretraining for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.7198832683703479, valid_loss: 0.6709977487723032\nFOLD: 0, EPOCH: 1, train_loss: 0.34338446640793013, valid_loss: 0.02672174945473671\nFOLD: 0, EPOCH: 2, train_loss: 0.01350047776256414, valid_loss: 0.006000575531894962\nFOLD: 0, EPOCH: 3, train_loss: 0.008867081665598294, valid_loss: 0.005324077559635043\nFOLD: 0, EPOCH: 4, train_loss: 0.008544737829224151, valid_loss: 0.004907717152188222\nFOLD: 0, EPOCH: 5, train_loss: 0.008466510595205952, valid_loss: 0.004864598469187816\nFOLD: 0, EPOCH: 6, train_loss: 0.008397711235482027, valid_loss: 0.005362040596082807\nFOLD: 0, EPOCH: 7, train_loss: 0.008324390263570583, valid_loss: 0.00567017262801528\nFOLD: 0, EPOCH: 8, train_loss: 0.008304623298018295, valid_loss: 0.004978688821817438\nFOLD: 0, EPOCH: 9, train_loss: 0.008305628386819187, valid_loss: 0.004816500237211585\nFOLD: 0, EPOCH: 10, train_loss: 0.008283523144200444, valid_loss: 0.004997568437829614\nFOLD: 0, EPOCH: 11, train_loss: 0.00821705610381768, valid_loss: 0.0047071986288453145\nFOLD: 0, EPOCH: 12, train_loss: 0.008178086157011636, valid_loss: 0.005445277395968636\nFOLD: 0, EPOCH: 13, train_loss: 0.008135157034677617, valid_loss: 0.004901754514624675\nFOLD: 0, EPOCH: 14, train_loss: 0.00809590460951714, valid_loss: 0.004846284631639719\nFOLD: 0, EPOCH: 15, train_loss: 0.008051230174982372, valid_loss: 0.004816054444139202\nFOLD: 0, EPOCH: 16, train_loss: 0.008029577020994005, valid_loss: 0.00485636293888092\nFOLD: 0, EPOCH: 17, train_loss: 0.007992244489929256, valid_loss: 0.004778810737965007\nFOLD: 0, EPOCH: 18, train_loss: 0.007953384973327903, valid_loss: 0.004941183064753811\nFOLD: 0, EPOCH: 19, train_loss: 0.007915886749020395, valid_loss: 0.004903946925575535\nFOLD: 0, EPOCH: 20, train_loss: 0.007890381124418448, valid_loss: 0.005118213050688307\nFOLD: 0, EPOCH: 21, train_loss: 0.007873468594077756, valid_loss: 0.0048044352636982994\nFOLD: 0, EPOCH: 22, train_loss: 0.007819818880628137, valid_loss: 0.004936388460919261\nFOLD: 0, EPOCH: 23, train_loss: 0.007781668593559195, valid_loss: 0.004807718098163605\nFOLD: 0, EPOCH: 24, train_loss: 0.007740396937793668, valid_loss: 0.00491130903052787\nBeginning training for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.4174949275877546, valid_loss: 0.038705103720227875\nFOLD: 0, EPOCH: 1, train_loss: 0.02735559461528764, valid_loss: 0.02080484665930271\nFOLD: 0, EPOCH: 2, train_loss: 0.022215391564018586, valid_loss: 0.01940176698068778\nFOLD: 0, EPOCH: 3, train_loss: 0.021378054283559322, valid_loss: 0.018863519032796223\nFOLD: 0, EPOCH: 4, train_loss: 0.021092568140696075, valid_loss: 0.018060925727089245\nFOLD: 0, EPOCH: 5, train_loss: 0.020875878686852315, valid_loss: 0.017953167979915936\nFOLD: 0, EPOCH: 6, train_loss: 0.020699372405514997, valid_loss: 0.017855919897556305\nFOLD: 0, EPOCH: 7, train_loss: 0.02062406193684129, valid_loss: 0.01782902019719283\nFOLD: 0, EPOCH: 8, train_loss: 0.020477304070749703, valid_loss: 0.01782448620845874\nFOLD: 0, EPOCH: 9, train_loss: 0.020372305448879215, valid_loss: 0.01790365514655908\nFOLD: 0, EPOCH: 10, train_loss: 0.02032743827165926, valid_loss: 0.01758260279893875\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 11, train_loss: 0.020227700197959646, valid_loss: 0.01766605240603288\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 12, train_loss: 0.019942485179532978, valid_loss: 0.017409588831166427\nFOLD: 0, EPOCH: 13, train_loss: 0.0197532765348168, valid_loss: 0.01744988343367974\nFOLD: 0, EPOCH: 14, train_loss: 0.019617909839486376, valid_loss: 0.017381538947423298\nFOLD: 0, EPOCH: 15, train_loss: 0.019562969148597297, valid_loss: 0.017365169400970142\nFOLD: 0, EPOCH: 16, train_loss: 0.019510002532864317, valid_loss: 0.01745584172507127\nFOLD: 0, EPOCH: 17, train_loss: 0.019455780987353885, valid_loss: 0.01726540271192789\nFOLD: 0, EPOCH: 18, train_loss: 0.019397996639942423, valid_loss: 0.017322685569524765\nFOLD: 0, EPOCH: 19, train_loss: 0.019385629771825147, valid_loss: 0.01735174935311079\nFOLD: 0, EPOCH: 20, train_loss: 0.01934107524507186, valid_loss: 0.017387565846244495\nFOLD: 0, EPOCH: 21, train_loss: 0.01930668462506112, valid_loss: 0.01725511687497298\nFOLD: 0, EPOCH: 22, train_loss: 0.01930180926095037, valid_loss: 0.017376387491822243\nFOLD: 0, EPOCH: 23, train_loss: 0.019237446215222862, valid_loss: 0.017331543378531933\nFOLD: 0, EPOCH: 24, train_loss: 0.01919708669404773, valid_loss: 0.017255508651336033\nFOLD: 0, EPOCH: 25, train_loss: 0.019204131930189973, valid_loss: 0.017293123838802178\nFOLD: 0, EPOCH: 26, train_loss: 0.019160174709909102, valid_loss: 0.017252034197251003\nFOLD: 0, EPOCH: 27, train_loss: 0.01913454947883592, valid_loss: 0.0172652555629611\nFOLD: 0, EPOCH: 28, train_loss: 0.01915012447930434, valid_loss: 0.017279904646178085\nFOLD: 0, EPOCH: 29, train_loss: 0.019113255555138868, valid_loss: 0.017253123534222443\nFOLD: 0, EPOCH: 30, train_loss: 0.01910961452214157, valid_loss: 0.01726100593805313\nFOLD: 0, EPOCH: 31, train_loss: 0.01908670469899388, valid_loss: 0.017249626728395622\nFOLD: 0, EPOCH: 32, train_loss: 0.019078415654161397, valid_loss: 0.017253321285049122\nFOLD: 0, EPOCH: 33, train_loss: 0.01907131410039523, valid_loss: 0.017252422248323757\nFOLD: 0, EPOCH: 34, train_loss: 0.019102996160440585, valid_loss: 0.017256091969708603\nBeginning pretraining for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.7196345942861894, valid_loss: 0.6699328621228536\nFOLD: 1, EPOCH: 1, train_loss: 0.34181067336569815, valid_loss: 0.027150703594088554\nFOLD: 1, EPOCH: 2, train_loss: 0.013431679703952634, valid_loss: 0.006100759919111927\nFOLD: 1, EPOCH: 3, train_loss: 0.008842274889021236, valid_loss: 0.005340505624189973\nFOLD: 1, EPOCH: 4, train_loss: 0.008486449334989576, valid_loss: 0.005069405383740862\nFOLD: 1, EPOCH: 5, train_loss: 0.008377359645879445, valid_loss: 0.005183179552356402\nFOLD: 1, EPOCH: 6, train_loss: 0.008309622486943708, valid_loss: 0.0053608769085258245\nFOLD: 1, EPOCH: 7, train_loss: 0.008279260097290663, valid_loss: 0.005191125441342592\nFOLD: 1, EPOCH: 8, train_loss: 0.008237454204765312, valid_loss: 0.005134108088289698\nFOLD: 1, EPOCH: 9, train_loss: 0.008198094389894429, valid_loss: 0.0052641805571814375\nFOLD: 1, EPOCH: 10, train_loss: 0.008167796179323512, valid_loss: 0.005604189820587635\nFOLD: 1, EPOCH: 11, train_loss: 0.008109393776120508, valid_loss: 0.005461654004951318\nFOLD: 1, EPOCH: 12, train_loss: 0.008107693276970702, valid_loss: 0.005605839736138781\nFOLD: 1, EPOCH: 13, train_loss: 0.008060076082234873, valid_loss: 0.005011546580741803\nFOLD: 1, EPOCH: 14, train_loss: 0.008024691946475822, valid_loss: 0.004811608077337344\nFOLD: 1, EPOCH: 15, train_loss: 0.008001833126935013, valid_loss: 0.005051078042015433\nFOLD: 1, EPOCH: 16, train_loss: 0.007971796303001396, valid_loss: 0.0046918979690720635\nFOLD: 1, EPOCH: 17, train_loss: 0.007944555966840947, valid_loss: 0.0049611961003392935\nFOLD: 1, EPOCH: 18, train_loss: 0.007903567939887153, valid_loss: 0.0049059411976486444\nFOLD: 1, EPOCH: 19, train_loss: 0.007880621329497765, valid_loss: 0.004960495047271252\nFOLD: 1, EPOCH: 20, train_loss: 0.007848097641459283, valid_loss: 0.005090322500715653\nFOLD: 1, EPOCH: 21, train_loss: 0.007820859756868552, valid_loss: 0.004971181740984321\nFOLD: 1, EPOCH: 22, train_loss: 0.007787234163569177, valid_loss: 0.004992007355516155\nFOLD: 1, EPOCH: 23, train_loss: 0.007752009420929586, valid_loss: 0.005155652528628707\nFOLD: 1, EPOCH: 24, train_loss: 0.007699246326571002, valid_loss: 0.005283646595974763\nBeginning training for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.40839021850158186, valid_loss: 0.029235982646544773\nFOLD: 1, EPOCH: 1, train_loss: 0.027208311702398694, valid_loss: 0.020350764816006024\nFOLD: 1, EPOCH: 2, train_loss: 0.02273060496458236, valid_loss: 0.02008196742584308\nFOLD: 1, EPOCH: 3, train_loss: 0.021778513129581425, valid_loss: 0.018374690786004066\nFOLD: 1, EPOCH: 4, train_loss: 0.021343339563292617, valid_loss: 0.018649484651784103\nFOLD: 1, EPOCH: 5, train_loss: 0.021078993347199523, valid_loss: 0.017941500060260296\nFOLD: 1, EPOCH: 6, train_loss: 0.020911181838635134, valid_loss: 0.01787567014495532\nFOLD: 1, EPOCH: 7, train_loss: 0.020792581140995026, valid_loss: 0.017932838760316372\nFOLD: 1, EPOCH: 8, train_loss: 0.020667004782487366, valid_loss: 0.017888507805764675\nFOLD: 1, EPOCH: 9, train_loss: 0.020486177219187513, valid_loss: 0.017667056061327457\nFOLD: 1, EPOCH: 10, train_loss: 0.020427682930055785, valid_loss: 0.017674872030814488\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 11, train_loss: 0.020253829164978338, valid_loss: 0.017692707168559235\nFOLD: 1, EPOCH: 12, train_loss: 0.019981361103846747, valid_loss: 0.017479100575049717\nFOLD: 1, EPOCH: 13, train_loss: 0.019866735290955093, valid_loss: 0.01738989322135846\nFOLD: 1, EPOCH: 14, train_loss: 0.019801170181702164, valid_loss: 0.017451290972530842\nFOLD: 1, EPOCH: 15, train_loss: 0.01974424675983541, valid_loss: 0.017440665513277054\nFOLD: 1, EPOCH: 16, train_loss: 0.01972151536713628, valid_loss: 0.01754137935737769\nFOLD: 1, EPOCH: 17, train_loss: 0.019636229995419, valid_loss: 0.017333883481721084\nFOLD: 1, EPOCH: 18, train_loss: 0.01962627601974151, valid_loss: 0.01745824795216322\nFOLD: 1, EPOCH: 19, train_loss: 0.01958879444967298, valid_loss: 0.017411531570057075\nFOLD: 1, EPOCH: 20, train_loss: 0.019544176373849895, valid_loss: 0.017402683695157368\nFOLD: 1, EPOCH: 21, train_loss: 0.01952442280290758, valid_loss: 0.01745871299256881\nFOLD: 1, EPOCH: 22, train_loss: 0.019453674061771703, valid_loss: 0.017356617997090023\nFOLD: 1, EPOCH: 23, train_loss: 0.019427268108462587, valid_loss: 0.01737571880221367\nFOLD: 1, EPOCH: 24, train_loss: 0.019411199178327534, valid_loss: 0.017257999628782272\nFOLD: 1, EPOCH: 25, train_loss: 0.0194170110146789, valid_loss: 0.01728076736132304\nFOLD: 1, EPOCH: 26, train_loss: 0.019373359651688266, valid_loss: 0.017321082142492134\nFOLD: 1, EPOCH: 27, train_loss: 0.019358465941075015, valid_loss: 0.01725909175972144\nFOLD: 1, EPOCH: 28, train_loss: 0.019337514634518063, valid_loss: 0.017314854077994823\nFOLD: 1, EPOCH: 29, train_loss: 0.01930633926873698, valid_loss: 0.01729180756956339\nFOLD: 1, EPOCH: 30, train_loss: 0.019311889677363282, valid_loss: 0.01729906102021535\nFOLD: 1, EPOCH: 31, train_loss: 0.019319874617983315, valid_loss: 0.01728575552503268\nFOLD: 1, EPOCH: 32, train_loss: 0.019293214577962372, valid_loss: 0.017296455490092438\nFOLD: 1, EPOCH: 33, train_loss: 0.019298281849307174, valid_loss: 0.0172828771173954\nFOLD: 1, EPOCH: 34, train_loss: 0.01928147202467217, valid_loss: 0.01728621901323398\nBeginning pretraining for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.7194522584185881, valid_loss: 0.6698244015375773\nFOLD: 2, EPOCH: 1, train_loss: 0.3416191750370404, valid_loss: 0.025969024126728375\nFOLD: 2, EPOCH: 2, train_loss: 0.013414776062264162, valid_loss: 0.005919416124622027\nFOLD: 2, EPOCH: 3, train_loss: 0.008890216019661987, valid_loss: 0.005232843492800991\nFOLD: 2, EPOCH: 4, train_loss: 0.00856204076177057, valid_loss: 0.004780158090094726\nFOLD: 2, EPOCH: 5, train_loss: 0.008489619173547801, valid_loss: 0.004819841201727589\nFOLD: 2, EPOCH: 6, train_loss: 0.0083965395905954, valid_loss: 0.004766704320597152\nFOLD: 2, EPOCH: 7, train_loss: 0.008367361309116377, valid_loss: 0.005350759097685416\nFOLD: 2, EPOCH: 8, train_loss: 0.008308267563252765, valid_loss: 0.004791944132496913\nFOLD: 2, EPOCH: 9, train_loss: 0.008290081586250487, valid_loss: 0.005308628780767322\nFOLD: 2, EPOCH: 10, train_loss: 0.008247429477598737, valid_loss: 0.004854880506172776\nFOLD: 2, EPOCH: 11, train_loss: 0.008199095931451987, valid_loss: 0.005297454306855798\nFOLD: 2, EPOCH: 12, train_loss: 0.008167060407097726, valid_loss: 0.004908327168474595\nFOLD: 2, EPOCH: 13, train_loss: 0.008145912522997926, valid_loss: 0.004762347865228851\nFOLD: 2, EPOCH: 14, train_loss: 0.008112054674283546, valid_loss: 0.0045410459861159325\nFOLD: 2, EPOCH: 15, train_loss: 0.008080911685657851, valid_loss: 0.005002514769633611\nFOLD: 2, EPOCH: 16, train_loss: 0.00804560795864638, valid_loss: 0.0045380846519644065\nFOLD: 2, EPOCH: 17, train_loss: 0.008018017253454994, valid_loss: 0.0047497948398813605\nFOLD: 2, EPOCH: 18, train_loss: 0.00796111938817536, valid_loss: 0.004966328386217356\nFOLD: 2, EPOCH: 19, train_loss: 0.007944230264162315, valid_loss: 0.004768574299911658\nFOLD: 2, EPOCH: 20, train_loss: 0.007907399866620408, valid_loss: 0.0053600890096277\nFOLD: 2, EPOCH: 21, train_loss: 0.007884372381822151, valid_loss: 0.004684999119490385\nFOLD: 2, EPOCH: 22, train_loss: 0.007849814041572459, valid_loss: 0.004821174157162507\nFOLD: 2, EPOCH: 23, train_loss: 0.007808762985993834, valid_loss: 0.004715552786365151\nFOLD: 2, EPOCH: 24, train_loss: 0.007760495851364206, valid_loss: 0.0047074449636663\nBeginning training for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.41068789198556366, valid_loss: 0.03628733940422535\nFOLD: 2, EPOCH: 1, train_loss: 0.02705706946332665, valid_loss: 0.021053985382119816\nFOLD: 2, EPOCH: 2, train_loss: 0.022661416846163133, valid_loss: 0.02026287466287613\nFOLD: 2, EPOCH: 3, train_loss: 0.021679988733547574, valid_loss: 0.01945668924599886\nFOLD: 2, EPOCH: 4, train_loss: 0.021267381825429553, valid_loss: 0.018894814575711887\nFOLD: 2, EPOCH: 5, train_loss: 0.02102088594042203, valid_loss: 0.0190919100617369\nFOLD: 2, EPOCH: 6, train_loss: 0.020871492157525876, valid_loss: 0.018949490661422413\nFOLD: 2, EPOCH: 7, train_loss: 0.020776909502113566, valid_loss: 0.01848626012603442\nFOLD: 2, EPOCH: 8, train_loss: 0.020658370710032826, valid_loss: 0.018568674412866432\nFOLD: 2, EPOCH: 9, train_loss: 0.020490913213614154, valid_loss: 0.018353433348238468\nFOLD: 2, EPOCH: 10, train_loss: 0.020407789112890467, valid_loss: 0.018426931152741115\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 11, train_loss: 0.02032394230584888, valid_loss: 0.018329312403996784\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 12, train_loss: 0.019996784298735505, valid_loss: 0.018177468950549763\nFOLD: 2, EPOCH: 13, train_loss: 0.01982426259885816, valid_loss: 0.018087964815398056\nFOLD: 2, EPOCH: 14, train_loss: 0.019726498490747285, valid_loss: 0.018182760725418728\nFOLD: 2, EPOCH: 15, train_loss: 0.019669477882630685, valid_loss: 0.01811577721188466\nFOLD: 2, EPOCH: 16, train_loss: 0.019597177996354943, valid_loss: 0.01805799330274264\nFOLD: 2, EPOCH: 17, train_loss: 0.019551553982583916, valid_loss: 0.018148173888524372\nFOLD: 2, EPOCH: 18, train_loss: 0.019515507087549743, valid_loss: 0.01815061705807845\nFOLD: 2, EPOCH: 19, train_loss: 0.01947852985604721, valid_loss: 0.018047045605878036\nFOLD: 2, EPOCH: 20, train_loss: 0.019474997513872737, valid_loss: 0.0181196170548598\nFOLD: 2, EPOCH: 21, train_loss: 0.019436784143395284, valid_loss: 0.01806801340232293\nFOLD: 2, EPOCH: 22, train_loss: 0.019382427775246257, valid_loss: 0.01810711684326331\nFOLD: 2, EPOCH: 23, train_loss: 0.019342776168795192, valid_loss: 0.01807796210050583\nFOLD: 2, EPOCH: 24, train_loss: 0.01931682344087783, valid_loss: 0.0180635799964269\nFOLD: 2, EPOCH: 25, train_loss: 0.019302320283125427, valid_loss: 0.018079080618917942\nFOLD: 2, EPOCH: 26, train_loss: 0.019291294355164555, valid_loss: 0.018010815605521202\nFOLD: 2, EPOCH: 27, train_loss: 0.019268521600786376, valid_loss: 0.01803421291212241\nFOLD: 2, EPOCH: 28, train_loss: 0.019257785752415657, valid_loss: 0.018042697571218014\nFOLD: 2, EPOCH: 29, train_loss: 0.019240401630454203, valid_loss: 0.018023620049158733\nFOLD: 2, EPOCH: 30, train_loss: 0.019220442730276024, valid_loss: 0.01803501012424628\nFOLD: 2, EPOCH: 31, train_loss: 0.019217296229565844, valid_loss: 0.018024901549021404\nFOLD: 2, EPOCH: 32, train_loss: 0.01921751213205211, valid_loss: 0.018019857816398144\nFOLD: 2, EPOCH: 33, train_loss: 0.019180092577110317, valid_loss: 0.018028720902899902\nFOLD: 2, EPOCH: 34, train_loss: 0.019176727051244062, valid_loss: 0.018026118477185566\nBeginning pretraining for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.7194315545699176, valid_loss: 0.6702287197113037\nFOLD: 3, EPOCH: 1, train_loss: 0.3394234820323832, valid_loss: 0.025428193310896557\nFOLD: 3, EPOCH: 2, train_loss: 0.013349253656890462, valid_loss: 0.006020621318990986\nFOLD: 3, EPOCH: 3, train_loss: 0.008889183949898271, valid_loss: 0.00526313631174465\nFOLD: 3, EPOCH: 4, train_loss: 0.008563909980961504, valid_loss: 0.00506932536760966\nFOLD: 3, EPOCH: 5, train_loss: 0.008443296955460134, valid_loss: 0.004830052532876532\nFOLD: 3, EPOCH: 6, train_loss: 0.008372863754630089, valid_loss: 0.004957820754498243\nFOLD: 3, EPOCH: 7, train_loss: 0.008333834627752794, valid_loss: 0.0050934610577921076\nFOLD: 3, EPOCH: 8, train_loss: 0.008281355573083548, valid_loss: 0.00505057613675793\nFOLD: 3, EPOCH: 9, train_loss: 0.008354642387369977, valid_loss: 0.004968425879875819\nFOLD: 3, EPOCH: 10, train_loss: 0.008280317230588374, valid_loss: 0.005132567835971713\nFOLD: 3, EPOCH: 11, train_loss: 0.008218159460846116, valid_loss: 0.00486948830075562\nFOLD: 3, EPOCH: 12, train_loss: 0.008197321405853419, valid_loss: 0.005042966144780318\nFOLD: 3, EPOCH: 13, train_loss: 0.008163867915487465, valid_loss: 0.005512108327820897\nFOLD: 3, EPOCH: 14, train_loss: 0.008117430131225026, valid_loss: 0.004685991831744711\nFOLD: 3, EPOCH: 15, train_loss: 0.008083977024344838, valid_loss: 0.004921895374233524\nFOLD: 3, EPOCH: 16, train_loss: 0.00804940371445435, valid_loss: 0.004974898028497894\nFOLD: 3, EPOCH: 17, train_loss: 0.008008529432117939, valid_loss: 0.00477766493956248\nFOLD: 3, EPOCH: 18, train_loss: 0.00797894858645604, valid_loss: 0.004963250287498037\nFOLD: 3, EPOCH: 19, train_loss: 0.007947617826764198, valid_loss: 0.004693745713060101\nFOLD: 3, EPOCH: 20, train_loss: 0.007908685530042824, valid_loss: 0.004909034663190444\nFOLD: 3, EPOCH: 21, train_loss: 0.007871675291372573, valid_loss: 0.004843581467866898\nFOLD: 3, EPOCH: 22, train_loss: 0.007833970669547425, valid_loss: 0.00525588922513028\nFOLD: 3, EPOCH: 23, train_loss: 0.007800250615486328, valid_loss: 0.004906548808018367\nFOLD: 3, EPOCH: 24, train_loss: 0.007755222867297775, valid_loss: 0.004771401019146045\nBeginning training for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.411442749969223, valid_loss: 0.03926687749723593\nFOLD: 3, EPOCH: 1, train_loss: 0.027151306324145374, valid_loss: 0.02060594627012809\nFOLD: 3, EPOCH: 2, train_loss: 0.02242068962796646, valid_loss: 0.018835536514719326\nFOLD: 3, EPOCH: 3, train_loss: 0.021564296391956946, valid_loss: 0.0184349175542593\nFOLD: 3, EPOCH: 4, train_loss: 0.02117689200403059, valid_loss: 0.018373689614236355\nFOLD: 3, EPOCH: 5, train_loss: 0.021014129085575834, valid_loss: 0.018067267412940662\nFOLD: 3, EPOCH: 6, train_loss: 0.02090716712615069, valid_loss: 0.018121885135769844\nFOLD: 3, EPOCH: 7, train_loss: 0.02072839022559278, valid_loss: 0.01806227583438158\nFOLD: 3, EPOCH: 8, train_loss: 0.02057827959823258, valid_loss: 0.018080512061715126\nFOLD: 3, EPOCH: 9, train_loss: 0.02050341643831309, valid_loss: 0.01791898502657811\nFOLD: 3, EPOCH: 10, train_loss: 0.020427039879209855, valid_loss: 0.017597463602821033\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 11, train_loss: 0.02035523096428198, valid_loss: 0.017794822963575523\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 12, train_loss: 0.020057413891396102, valid_loss: 0.017399318205813568\nFOLD: 3, EPOCH: 13, train_loss: 0.01986253442352309, valid_loss: 0.017423147335648537\nFOLD: 3, EPOCH: 14, train_loss: 0.01978237527039121, valid_loss: 0.017424584676822025\nFOLD: 3, EPOCH: 15, train_loss: 0.01973693220711806, valid_loss: 0.01735091581940651\nFOLD: 3, EPOCH: 16, train_loss: 0.01969435065984726, valid_loss: 0.01749470829963684\nFOLD: 3, EPOCH: 17, train_loss: 0.01962596013703767, valid_loss: 0.01729535652945439\nFOLD: 3, EPOCH: 18, train_loss: 0.019623555133447927, valid_loss: 0.0173971230785052\nFOLD: 3, EPOCH: 19, train_loss: 0.019544592620257067, valid_loss: 0.017258472740650177\nFOLD: 3, EPOCH: 20, train_loss: 0.019522611544850993, valid_loss: 0.017432195134460926\nFOLD: 3, EPOCH: 21, train_loss: 0.019483776927432594, valid_loss: 0.01723864333083232\nFOLD: 3, EPOCH: 22, train_loss: 0.019459175548570996, valid_loss: 0.017234816526373226\nFOLD: 3, EPOCH: 23, train_loss: 0.01943694559090278, valid_loss: 0.017231919181843598\nFOLD: 3, EPOCH: 24, train_loss: 0.019409730756545767, valid_loss: 0.017206091433763504\nFOLD: 3, EPOCH: 25, train_loss: 0.019398127189453912, valid_loss: 0.017197950432697933\nFOLD: 3, EPOCH: 26, train_loss: 0.019358977675437927, valid_loss: 0.01715743262320757\nFOLD: 3, EPOCH: 27, train_loss: 0.019358387545627707, valid_loss: 0.017259644034008186\nFOLD: 3, EPOCH: 28, train_loss: 0.019331243163084284, valid_loss: 0.017190763105948765\nFOLD: 3, EPOCH: 29, train_loss: 0.019286851179512107, valid_loss: 0.017201567068696022\nFOLD: 3, EPOCH: 30, train_loss: 0.01930877736166996, valid_loss: 0.01719237274179856\nFOLD: 3, EPOCH: 31, train_loss: 0.019286530585411715, valid_loss: 0.017192985552052658\nFOLD: 3, EPOCH: 32, train_loss: 0.019271008999032134, valid_loss: 0.017181680537760258\nFOLD: 3, EPOCH: 33, train_loss: 0.01926756989868248, valid_loss: 0.01718561879048745\nFOLD: 3, EPOCH: 34, train_loss: 0.019269194399171016, valid_loss: 0.017186265128354233\nBeginning pretraining for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.7193933760418612, valid_loss: 0.6699274877707163\nFOLD: 4, EPOCH: 1, train_loss: 0.34132754638352814, valid_loss: 0.02643372304737568\nFOLD: 4, EPOCH: 2, train_loss: 0.013515517745605287, valid_loss: 0.005943655657271544\nFOLD: 4, EPOCH: 3, train_loss: 0.008817208026919295, valid_loss: 0.005142500934501489\nFOLD: 4, EPOCH: 4, train_loss: 0.008575877119951388, valid_loss: 0.0048828386546423035\nFOLD: 4, EPOCH: 5, train_loss: 0.008421185768812019, valid_loss: 0.004782580925772588\nFOLD: 4, EPOCH: 6, train_loss: 0.00833799027125625, valid_loss: 0.004666415276005864\nFOLD: 4, EPOCH: 7, train_loss: 0.008317399578278555, valid_loss: 0.004817555425688624\nFOLD: 4, EPOCH: 8, train_loss: 0.00826085071243784, valid_loss: 0.004815975592161219\nFOLD: 4, EPOCH: 9, train_loss: 0.008266083033317152, valid_loss: 0.005251368274912238\nFOLD: 4, EPOCH: 10, train_loss: 0.008241082199246567, valid_loss: 0.005332014678666989\nFOLD: 4, EPOCH: 11, train_loss: 0.008168848135563381, valid_loss: 0.004854468783984582\nFOLD: 4, EPOCH: 12, train_loss: 0.00813965140567983, valid_loss: 0.005357595315823953\nFOLD: 4, EPOCH: 13, train_loss: 0.008100737803889549, valid_loss: 0.0051623086134592695\nFOLD: 4, EPOCH: 14, train_loss: 0.00806797934038674, valid_loss: 0.0049064077126483125\nFOLD: 4, EPOCH: 15, train_loss: 0.008029198643806227, valid_loss: 0.00485418535148104\nFOLD: 4, EPOCH: 16, train_loss: 0.007987659150624977, valid_loss: 0.004813793891419967\nFOLD: 4, EPOCH: 17, train_loss: 0.007948407889617717, valid_loss: 0.004607128677889705\nFOLD: 4, EPOCH: 18, train_loss: 0.007911289754488012, valid_loss: 0.004759562822679679\nFOLD: 4, EPOCH: 19, train_loss: 0.007878759999156874, valid_loss: 0.004872189213832219\nFOLD: 4, EPOCH: 20, train_loss: 0.007829110466820352, valid_loss: 0.00491278952298065\nFOLD: 4, EPOCH: 21, train_loss: 0.007794818019165713, valid_loss: 0.004668081179261208\nFOLD: 4, EPOCH: 22, train_loss: 0.007743653252392131, valid_loss: 0.004935850699742635\nFOLD: 4, EPOCH: 23, train_loss: 0.007704673636266414, valid_loss: 0.004798880390202005\nFOLD: 4, EPOCH: 24, train_loss: 0.007650369340006043, valid_loss: 0.004954913863912225\nBeginning training for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.4033851295928745, valid_loss: 0.037282075732946396\nFOLD: 4, EPOCH: 1, train_loss: 0.026926476617946345, valid_loss: 0.020633781018356483\nFOLD: 4, EPOCH: 2, train_loss: 0.022536342032253742, valid_loss: 0.019637752945224445\nFOLD: 4, EPOCH: 3, train_loss: 0.021595488083274925, valid_loss: 0.018657428212463856\nFOLD: 4, EPOCH: 4, train_loss: 0.02121755908079007, valid_loss: 0.0183200699587663\nFOLD: 4, EPOCH: 5, train_loss: 0.020942227531443623, valid_loss: 0.018623126360277336\nFOLD: 4, EPOCH: 6, train_loss: 0.02078787555151126, valid_loss: 0.018335594485203426\nFOLD: 4, EPOCH: 7, train_loss: 0.020734438946580187, valid_loss: 0.01814055225501458\nFOLD: 4, EPOCH: 8, train_loss: 0.02055439111941001, valid_loss: 0.01805223214129607\nFOLD: 4, EPOCH: 9, train_loss: 0.020470057559363982, valid_loss: 0.018256272189319134\nFOLD: 4, EPOCH: 10, train_loss: 0.020372675019590294, valid_loss: 0.017999708652496338\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 11, train_loss: 0.02025138510062414, valid_loss: 0.017823359308143456\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 12, train_loss: 0.019913436187540785, valid_loss: 0.01784421441455682\nFOLD: 4, EPOCH: 13, train_loss: 0.01972393938066328, valid_loss: 0.017777280571560066\nFOLD: 4, EPOCH: 14, train_loss: 0.019666770034853148, valid_loss: 0.017902489751577377\nFOLD: 4, EPOCH: 15, train_loss: 0.01959749694694491, valid_loss: 0.0177074009552598\nFOLD: 4, EPOCH: 16, train_loss: 0.019552848959232077, valid_loss: 0.017703978344798088\nFOLD: 4, EPOCH: 17, train_loss: 0.019522851278238437, valid_loss: 0.01777076069265604\nFOLD: 4, EPOCH: 18, train_loss: 0.019463191849782187, valid_loss: 0.01775103062391281\nFOLD: 4, EPOCH: 19, train_loss: 0.019450032152235508, valid_loss: 0.01768076978623867\nFOLD: 4, EPOCH: 20, train_loss: 0.01940130102722084, valid_loss: 0.017580884508788586\nFOLD: 4, EPOCH: 21, train_loss: 0.01933434046804905, valid_loss: 0.017857979983091354\nFOLD: 4, EPOCH: 22, train_loss: 0.01935643034384531, valid_loss: 0.01761589975406726\nFOLD: 4, EPOCH: 23, train_loss: 0.019330779254874763, valid_loss: 0.017630582364896934\nFOLD: 4, EPOCH: 24, train_loss: 0.019295481526676345, valid_loss: 0.017691000054279964\nFOLD: 4, EPOCH: 25, train_loss: 0.019244480549412614, valid_loss: 0.017617344856262207\nFOLD: 4, EPOCH: 26, train_loss: 0.019227240837233907, valid_loss: 0.01762337703257799\nFOLD: 4, EPOCH: 27, train_loss: 0.019241684445125216, valid_loss: 0.017598238152762253\nFOLD: 4, EPOCH: 28, train_loss: 0.019207008676055598, valid_loss: 0.017580379731953144\nFOLD: 4, EPOCH: 29, train_loss: 0.019163735156111857, valid_loss: 0.017592136437694233\nFOLD: 4, EPOCH: 30, train_loss: 0.01916861780645216, valid_loss: 0.017606033943593502\nFOLD: 4, EPOCH: 31, train_loss: 0.019155101248008365, valid_loss: 0.017593720617393654\nFOLD: 4, EPOCH: 32, train_loss: 0.019164691460044944, valid_loss: 0.01760320334384839\nFOLD: 4, EPOCH: 33, train_loss: 0.019160165341899675, valid_loss: 0.017600890559454758\nFOLD: 4, EPOCH: 34, train_loss: 0.019157101071494466, valid_loss: 0.01759496983140707\nBeginning pretraining for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.71980500571868, valid_loss: 0.6724691192309061\nFOLD: 5, EPOCH: 1, train_loss: 0.3429058532723609, valid_loss: 0.02640179730951786\nFOLD: 5, EPOCH: 2, train_loss: 0.013337174156571137, valid_loss: 0.005935939494520426\nFOLD: 5, EPOCH: 3, train_loss: 0.008796141017228365, valid_loss: 0.005389861762523651\nFOLD: 5, EPOCH: 4, train_loss: 0.008448885315481354, valid_loss: 0.005191532118866841\nFOLD: 5, EPOCH: 5, train_loss: 0.008401622256154524, valid_loss: 0.005123912201573451\nFOLD: 5, EPOCH: 6, train_loss: 0.008277180166367222, valid_loss: 0.0048575761417547865\nFOLD: 5, EPOCH: 7, train_loss: 0.008245537988841534, valid_loss: 0.004716695053502917\nFOLD: 5, EPOCH: 8, train_loss: 0.008215305546079488, valid_loss: 0.005317726094896595\nFOLD: 5, EPOCH: 9, train_loss: 0.008164582404253237, valid_loss: 0.004777584535380204\nFOLD: 5, EPOCH: 10, train_loss: 0.008133368333801627, valid_loss: 0.004673814323420326\nFOLD: 5, EPOCH: 11, train_loss: 0.00808685038731817, valid_loss: 0.005782516517986854\nFOLD: 5, EPOCH: 12, train_loss: 0.008062871816732427, valid_loss: 0.004799804960687955\nFOLD: 5, EPOCH: 13, train_loss: 0.008010800612871261, valid_loss: 0.004913378972560167\nFOLD: 5, EPOCH: 14, train_loss: 0.00797960888046552, valid_loss: 0.00470029244509836\nFOLD: 5, EPOCH: 15, train_loss: 0.007954384378321907, valid_loss: 0.005478359914074342\nFOLD: 5, EPOCH: 16, train_loss: 0.00792181191911154, valid_loss: 0.004796589957550168\nFOLD: 5, EPOCH: 17, train_loss: 0.00787965367164682, valid_loss: 0.005160250312959154\nFOLD: 5, EPOCH: 18, train_loss: 0.007864944280727822, valid_loss: 0.00467661046423018\nFOLD: 5, EPOCH: 19, train_loss: 0.00782528133405482, valid_loss: 0.0049123680995156365\nFOLD: 5, EPOCH: 20, train_loss: 0.007795632008791846, valid_loss: 0.004970587479571502\nFOLD: 5, EPOCH: 21, train_loss: 0.007773872889468775, valid_loss: 0.004830314389740427\nFOLD: 5, EPOCH: 22, train_loss: 0.0077270877081900835, valid_loss: 0.005032238162433107\nFOLD: 5, EPOCH: 23, train_loss: 0.007701046723762856, valid_loss: 0.0047798628608385725\nFOLD: 5, EPOCH: 24, train_loss: 0.007669662440414815, valid_loss: 0.004757080459967256\nBeginning training for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.4163743535823682, valid_loss: 0.04037483036518097\nFOLD: 5, EPOCH: 1, train_loss: 0.027349363628994015, valid_loss: 0.02075790086140235\nFOLD: 5, EPOCH: 2, train_loss: 0.022344095796784934, valid_loss: 0.019349508608380955\nFOLD: 5, EPOCH: 3, train_loss: 0.021428703604375616, valid_loss: 0.018807197300096352\nFOLD: 5, EPOCH: 4, train_loss: 0.021028326166903272, valid_loss: 0.018626643655200798\nFOLD: 5, EPOCH: 5, train_loss: 0.020815586561665815, valid_loss: 0.018206466920673847\nFOLD: 5, EPOCH: 6, train_loss: 0.020636178169618633, valid_loss: 0.018314838409423828\nFOLD: 5, EPOCH: 7, train_loss: 0.02051993941559511, valid_loss: 0.01833021578689416\nFOLD: 5, EPOCH: 8, train_loss: 0.02041177703615497, valid_loss: 0.018039412175615627\nFOLD: 5, EPOCH: 9, train_loss: 0.02030457348069724, valid_loss: 0.018016324068109196\nFOLD: 5, EPOCH: 10, train_loss: 0.020275391802630004, valid_loss: 0.01814139603326718\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 11, train_loss: 0.0200687563046813, valid_loss: 0.017918358867367108\nFOLD: 5, EPOCH: 12, train_loss: 0.019844379988225067, valid_loss: 0.017953423783183098\nFOLD: 5, EPOCH: 13, train_loss: 0.01970891939366565, valid_loss: 0.018091981299221516\nFOLD: 5, EPOCH: 14, train_loss: 0.019632357665721106, valid_loss: 0.017979727126657963\nFOLD: 5, EPOCH: 15, train_loss: 0.019601422471596915, valid_loss: 0.01783230248838663\nFOLD: 5, EPOCH: 16, train_loss: 0.019517053192590967, valid_loss: 0.017869433698554833\nFOLD: 5, EPOCH: 17, train_loss: 0.01946052541846738, valid_loss: 0.018018712600072224\nFOLD: 5, EPOCH: 18, train_loss: 0.019409070120138282, valid_loss: 0.017737843406697113\nFOLD: 5, EPOCH: 19, train_loss: 0.019359752809738413, valid_loss: 0.01780291087925434\nFOLD: 5, EPOCH: 20, train_loss: 0.01927883145125473, valid_loss: 0.017762209599216778\nFOLD: 5, EPOCH: 21, train_loss: 0.019236952011637828, valid_loss: 0.017804502819975216\nFOLD: 5, EPOCH: 22, train_loss: 0.019168366239789653, valid_loss: 0.01770087331533432\nFOLD: 5, EPOCH: 23, train_loss: 0.01910345656249453, valid_loss: 0.017693082181115944\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 24, train_loss: 0.018984757473363596, valid_loss: 0.0176230085392793\nFOLD: 5, EPOCH: 25, train_loss: 0.0188937084749341, valid_loss: 0.017703441282113392\nFOLD: 5, EPOCH: 26, train_loss: 0.01884503777631942, valid_loss: 0.01770248729735613\nFOLD: 5, EPOCH: 27, train_loss: 0.018822125816608175, valid_loss: 0.0176670765504241\nFOLD: 5, EPOCH: 28, train_loss: 0.01878861233811168, valid_loss: 0.017677499291797478\nFOLD: 5, EPOCH: 29, train_loss: 0.01876333383295466, valid_loss: 0.017683386181791622\nFOLD: 5, EPOCH: 30, train_loss: 0.018749998498927143, valid_loss: 0.0176671352237463\nFOLD: 5, EPOCH: 31, train_loss: 0.018743679749176782, valid_loss: 0.01767720902959506\nFOLD: 5, EPOCH: 32, train_loss: 0.018705496674074847, valid_loss: 0.01768557820469141\nFOLD: 5, EPOCH: 33, train_loss: 0.0187017037807142, valid_loss: 0.017684357861677807\nFOLD: 5, EPOCH: 34, train_loss: 0.018726754002273083, valid_loss: 0.0176845733076334\nBeginning pretraining for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.7196944215718437, valid_loss: 0.6715441842873892\nFOLD: 6, EPOCH: 1, train_loss: 0.340733180699103, valid_loss: 0.026322645445664723\nFOLD: 6, EPOCH: 2, train_loss: 0.013266615679158884, valid_loss: 0.005810206678385536\nFOLD: 6, EPOCH: 3, train_loss: 0.008797139203285469, valid_loss: 0.005032943716893594\nFOLD: 6, EPOCH: 4, train_loss: 0.008514241507167326, valid_loss: 0.005044234916567802\nFOLD: 6, EPOCH: 5, train_loss: 0.0083448256859008, valid_loss: 0.004620528159042199\nFOLD: 6, EPOCH: 6, train_loss: 0.00826567888040753, valid_loss: 0.0045484685494254036\nFOLD: 6, EPOCH: 7, train_loss: 0.008216268259703237, valid_loss: 0.004564531147480011\nFOLD: 6, EPOCH: 8, train_loss: 0.008159014752463382, valid_loss: 0.004689870635047555\nFOLD: 6, EPOCH: 9, train_loss: 0.008121905766208382, valid_loss: 0.005537648607666294\nFOLD: 6, EPOCH: 10, train_loss: 0.008126840639092466, valid_loss: 0.005947537099321683\nFOLD: 6, EPOCH: 11, train_loss: 0.008068503264118643, valid_loss: 0.0044425179560979204\nFOLD: 6, EPOCH: 12, train_loss: 0.008030957583447589, valid_loss: 0.005023488697285454\nFOLD: 6, EPOCH: 13, train_loss: 0.007991663193987572, valid_loss: 0.004598161205649376\nFOLD: 6, EPOCH: 14, train_loss: 0.007955637623501174, valid_loss: 0.004449825966730714\nFOLD: 6, EPOCH: 15, train_loss: 0.007936752045198399, valid_loss: 0.004452763629766802\nFOLD: 6, EPOCH: 16, train_loss: 0.007904823842074941, valid_loss: 0.004516744986176491\nFOLD: 6, EPOCH: 17, train_loss: 0.00786625440506374, valid_loss: 0.00460611159602801\nFOLD: 6, EPOCH: 18, train_loss: 0.007835618877673851, valid_loss: 0.004513621136235694\nFOLD: 6, EPOCH: 19, train_loss: 0.007814775796278435, valid_loss: 0.004832821510111292\nFOLD: 6, EPOCH: 20, train_loss: 0.007779879700940321, valid_loss: 0.00470940846328934\nFOLD: 6, EPOCH: 21, train_loss: 0.007770678562605206, valid_loss: 0.0047276624633620186\nFOLD: 6, EPOCH: 22, train_loss: 0.007730023839565761, valid_loss: 0.004544093894461791\nFOLD: 6, EPOCH: 23, train_loss: 0.007695445681319517, valid_loss: 0.0047023728645096225\nFOLD: 6, EPOCH: 24, train_loss: 0.007664231460212785, valid_loss: 0.004784151213243604\nBeginning training for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.4149377430405687, valid_loss: 0.03878713088730971\nFOLD: 6, EPOCH: 1, train_loss: 0.02733864224351504, valid_loss: 0.02096576864520709\nFOLD: 6, EPOCH: 2, train_loss: 0.02230668139150914, valid_loss: 0.018291021697223186\nFOLD: 6, EPOCH: 3, train_loss: 0.02148903621470227, valid_loss: 0.018317387128869694\nFOLD: 6, EPOCH: 4, train_loss: 0.021136660293182907, valid_loss: 0.018609735183417797\nFOLD: 6, EPOCH: 5, train_loss: 0.020930791492847836, valid_loss: 0.01837143364051978\nFOLD: 6, EPOCH: 6, train_loss: 0.02076968490420019, valid_loss: 0.01767957303673029\nFOLD: 6, EPOCH: 7, train_loss: 0.020624218344250145, valid_loss: 0.017500600777566433\nFOLD: 6, EPOCH: 8, train_loss: 0.020505550755735708, valid_loss: 0.017673313928147156\nFOLD: 6, EPOCH: 9, train_loss: 0.02046257520423216, valid_loss: 0.01743223952750365\nFOLD: 6, EPOCH: 10, train_loss: 0.0203431709395612, valid_loss: 0.017650020619233448\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 11, train_loss: 0.020253871074494195, valid_loss: 0.01717415141562621\nFOLD: 6, EPOCH: 12, train_loss: 0.019983497064779785, valid_loss: 0.01730330071101586\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 13, train_loss: 0.019833325024913338, valid_loss: 0.017247656049827736\nFOLD: 6, EPOCH: 14, train_loss: 0.01968596755143474, valid_loss: 0.017103103299935658\nFOLD: 6, EPOCH: 15, train_loss: 0.019603226005154496, valid_loss: 0.017215133955081303\nFOLD: 6, EPOCH: 16, train_loss: 0.019542775555130315, valid_loss: 0.01715715570996205\nFOLD: 6, EPOCH: 17, train_loss: 0.019496364190297967, valid_loss: 0.017151632035772007\nFOLD: 6, EPOCH: 18, train_loss: 0.019439688171533978, valid_loss: 0.01705226891984542\nFOLD: 6, EPOCH: 19, train_loss: 0.019412316381931305, valid_loss: 0.016988949850201607\nFOLD: 6, EPOCH: 20, train_loss: 0.019367221414166337, valid_loss: 0.01701142390569051\nFOLD: 6, EPOCH: 21, train_loss: 0.01934049404500162, valid_loss: 0.01705382267634074\nFOLD: 6, EPOCH: 22, train_loss: 0.019295216976281476, valid_loss: 0.01704932718227307\nFOLD: 6, EPOCH: 23, train_loss: 0.019295023480320677, valid_loss: 0.01703055202960968\nFOLD: 6, EPOCH: 24, train_loss: 0.019224019008962548, valid_loss: 0.017117055443425972\nFOLD: 6, EPOCH: 25, train_loss: 0.019196802311960387, valid_loss: 0.017025362079342205\nFOLD: 6, EPOCH: 26, train_loss: 0.019202915622907525, valid_loss: 0.017014633243282635\nFOLD: 6, EPOCH: 27, train_loss: 0.019171999712638995, valid_loss: 0.017024976201355457\nFOLD: 6, EPOCH: 28, train_loss: 0.019128088346299005, valid_loss: 0.017006023476521175\nFOLD: 6, EPOCH: 29, train_loss: 0.01914249809787554, valid_loss: 0.016984754242002964\nFOLD: 6, EPOCH: 30, train_loss: 0.01912560379680465, valid_loss: 0.016998915001749992\nFOLD: 6, EPOCH: 31, train_loss: 0.01912352771443479, valid_loss: 0.016997951393326122\nFOLD: 6, EPOCH: 32, train_loss: 0.01910489918116261, valid_loss: 0.016992664895951748\nFOLD: 6, EPOCH: 33, train_loss: 0.019112695775487843, valid_loss: 0.016999276354908943\nFOLD: 6, EPOCH: 34, train_loss: 0.019108212991234136, valid_loss: 0.016993868475159008\nBeginning pretraining for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.7188128636163824, valid_loss: 0.6717524031798044\nFOLD: 0, EPOCH: 1, train_loss: 0.34449245517744737, valid_loss: 0.02580193926890691\nFOLD: 0, EPOCH: 2, train_loss: 0.013393221696948303, valid_loss: 0.005973660309488575\nFOLD: 0, EPOCH: 3, train_loss: 0.008883205899859177, valid_loss: 0.005376809121419986\nFOLD: 0, EPOCH: 4, train_loss: 0.008566521984689376, valid_loss: 0.0050248752037684126\nFOLD: 0, EPOCH: 5, train_loss: 0.008446277940974516, valid_loss: 0.004749997518956661\nFOLD: 0, EPOCH: 6, train_loss: 0.00840315192609149, valid_loss: 0.004777073084066312\nFOLD: 0, EPOCH: 7, train_loss: 0.00836039611193187, valid_loss: 0.00549576284053425\nFOLD: 0, EPOCH: 8, train_loss: 0.008331565939656952, valid_loss: 0.004782288335263729\nFOLD: 0, EPOCH: 9, train_loss: 0.008277213118751259, valid_loss: 0.004927279660478234\nFOLD: 0, EPOCH: 10, train_loss: 0.008268253192962968, valid_loss: 0.004905319772660732\nFOLD: 0, EPOCH: 11, train_loss: 0.008240896057994926, valid_loss: 0.005061352625489235\nFOLD: 0, EPOCH: 12, train_loss: 0.008182740729192601, valid_loss: 0.005972169882928331\nFOLD: 0, EPOCH: 13, train_loss: 0.008146875811850323, valid_loss: 0.0047132500136892\nFOLD: 0, EPOCH: 14, train_loss: 0.008126832983073066, valid_loss: 0.004858897610877951\nFOLD: 0, EPOCH: 15, train_loss: 0.008090061601251364, valid_loss: 0.004848949844017625\nFOLD: 0, EPOCH: 16, train_loss: 0.0080543645137154, valid_loss: 0.00505694809059302\nFOLD: 0, EPOCH: 17, train_loss: 0.008003048667245927, valid_loss: 0.004656783343913655\nFOLD: 0, EPOCH: 18, train_loss: 0.007973646320512189, valid_loss: 0.004902229178696871\nFOLD: 0, EPOCH: 19, train_loss: 0.007939060341895503, valid_loss: 0.0047089663955072565\nFOLD: 0, EPOCH: 20, train_loss: 0.007882596020970275, valid_loss: 0.00493688586478432\nFOLD: 0, EPOCH: 21, train_loss: 0.007850195705781089, valid_loss: 0.0051860264502465725\nFOLD: 0, EPOCH: 22, train_loss: 0.0078049866510007315, valid_loss: 0.004884708362321059\nFOLD: 0, EPOCH: 23, train_loss: 0.00775724950739566, valid_loss: 0.0049606007523834705\nFOLD: 0, EPOCH: 24, train_loss: 0.007730502744807917, valid_loss: 0.004788729672630628\nBeginning training for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.40661846254678335, valid_loss: 0.034029886747399964\nFOLD: 0, EPOCH: 1, train_loss: 0.026845695998738792, valid_loss: 0.020455205192168553\nFOLD: 0, EPOCH: 2, train_loss: 0.022560930975219783, valid_loss: 0.0205428684130311\nFOLD: 0, EPOCH: 3, train_loss: 0.021656508502714774, valid_loss: 0.0185420469691356\nFOLD: 0, EPOCH: 4, train_loss: 0.021282942910843036, valid_loss: 0.018247930022577446\nFOLD: 0, EPOCH: 5, train_loss: 0.021095574888236383, valid_loss: 0.018206700682640076\nFOLD: 0, EPOCH: 6, train_loss: 0.020949665895279718, valid_loss: 0.018441239992777508\nFOLD: 0, EPOCH: 7, train_loss: 0.02073900560464929, valid_loss: 0.0179165406152606\nFOLD: 0, EPOCH: 8, train_loss: 0.02068200893700123, valid_loss: 0.017750590108335018\nFOLD: 0, EPOCH: 9, train_loss: 0.020515718332984868, valid_loss: 0.0182411161561807\nFOLD: 0, EPOCH: 10, train_loss: 0.02043477796456393, valid_loss: 0.017819651092092197\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 11, train_loss: 0.020380392889766133, valid_loss: 0.01767742944260438\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 12, train_loss: 0.020074191338875714, valid_loss: 0.017451271725197632\nFOLD: 0, EPOCH: 13, train_loss: 0.019867255790706945, valid_loss: 0.017476345412433147\nFOLD: 0, EPOCH: 14, train_loss: 0.01977391875184634, valid_loss: 0.017514966738720734\nFOLD: 0, EPOCH: 15, train_loss: 0.01971384948667358, valid_loss: 0.01735091395676136\nFOLD: 0, EPOCH: 16, train_loss: 0.019664377412375283, valid_loss: 0.01736926504721244\nFOLD: 0, EPOCH: 17, train_loss: 0.019634808304117006, valid_loss: 0.01740188989788294\nFOLD: 0, EPOCH: 18, train_loss: 0.019571103265180308, valid_loss: 0.01741487377633651\nFOLD: 0, EPOCH: 19, train_loss: 0.019562060999519685, valid_loss: 0.01743130013346672\nFOLD: 0, EPOCH: 20, train_loss: 0.01952009998700198, valid_loss: 0.0174840750793616\nFOLD: 0, EPOCH: 21, train_loss: 0.019451785010888296, valid_loss: 0.01730072560409705\nFOLD: 0, EPOCH: 22, train_loss: 0.019434621886295432, valid_loss: 0.01733147942771514\nFOLD: 0, EPOCH: 23, train_loss: 0.01939644620699041, valid_loss: 0.017414358134071033\nFOLD: 0, EPOCH: 24, train_loss: 0.019364939564291167, valid_loss: 0.01732984557747841\nFOLD: 0, EPOCH: 25, train_loss: 0.019366270424250293, valid_loss: 0.01731469513227542\nFOLD: 0, EPOCH: 26, train_loss: 0.019331918152816156, valid_loss: 0.017299218724171322\nFOLD: 0, EPOCH: 27, train_loss: 0.01931145809152547, valid_loss: 0.017326231735448044\nFOLD: 0, EPOCH: 28, train_loss: 0.01928067672997713, valid_loss: 0.017360821676750977\nFOLD: 0, EPOCH: 29, train_loss: 0.019279380657655353, valid_loss: 0.01729899023969968\nFOLD: 0, EPOCH: 30, train_loss: 0.019260417396093115, valid_loss: 0.0173065522685647\nFOLD: 0, EPOCH: 31, train_loss: 0.019252013141179785, valid_loss: 0.017312961940964062\nFOLD: 0, EPOCH: 32, train_loss: 0.019230933092972812, valid_loss: 0.017307082811991375\nFOLD: 0, EPOCH: 33, train_loss: 0.019230855464497033, valid_loss: 0.017309379143019516\nFOLD: 0, EPOCH: 34, train_loss: 0.019245621090864435, valid_loss: 0.01730626883606116\nBeginning pretraining for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.7189767834018258, valid_loss: 0.6713367998600006\nFOLD: 1, EPOCH: 1, train_loss: 0.34449237913769837, valid_loss: 0.027003067856033642\nFOLD: 1, EPOCH: 2, train_loss: 0.013506616986192325, valid_loss: 0.006144621797526876\nFOLD: 1, EPOCH: 3, train_loss: 0.008812772877076092, valid_loss: 0.005379265717541178\nFOLD: 1, EPOCH: 4, train_loss: 0.008463289136724436, valid_loss: 0.005184276572739084\nFOLD: 1, EPOCH: 5, train_loss: 0.008355515269453035, valid_loss: 0.005169442621991038\nFOLD: 1, EPOCH: 6, train_loss: 0.008375071824583061, valid_loss: 0.0052913278341293335\nFOLD: 1, EPOCH: 7, train_loss: 0.008272329646655741, valid_loss: 0.004770183547710379\nFOLD: 1, EPOCH: 8, train_loss: 0.008207874934134237, valid_loss: 0.005334582567835848\nFOLD: 1, EPOCH: 9, train_loss: 0.008199128226431854, valid_loss: 0.004913769584770004\nFOLD: 1, EPOCH: 10, train_loss: 0.008141129387213904, valid_loss: 0.005714548674101631\nFOLD: 1, EPOCH: 11, train_loss: 0.008130550411913325, valid_loss: 0.00476423076664408\nFOLD: 1, EPOCH: 12, train_loss: 0.008101044962292208, valid_loss: 0.004881371045485139\nFOLD: 1, EPOCH: 13, train_loss: 0.008062048924758154, valid_loss: 0.0053512965484211845\nFOLD: 1, EPOCH: 14, train_loss: 0.008034614750238903, valid_loss: 0.004862823834021886\nFOLD: 1, EPOCH: 15, train_loss: 0.008009813862907536, valid_loss: 0.005207270383834839\nFOLD: 1, EPOCH: 16, train_loss: 0.007981244296602467, valid_loss: 0.004821036864692966\nFOLD: 1, EPOCH: 17, train_loss: 0.007948285379611394, valid_loss: 0.005109550431370735\nFOLD: 1, EPOCH: 18, train_loss: 0.007916760932215872, valid_loss: 0.004858779177690546\nFOLD: 1, EPOCH: 19, train_loss: 0.007868220481802435, valid_loss: 0.005154710883895556\nFOLD: 1, EPOCH: 20, train_loss: 0.0078555682044038, valid_loss: 0.005059804301708937\nFOLD: 1, EPOCH: 21, train_loss: 0.007823487647863872, valid_loss: 0.00511353462934494\nFOLD: 1, EPOCH: 22, train_loss: 0.00777946034555926, valid_loss: 0.0048943871321777506\nFOLD: 1, EPOCH: 23, train_loss: 0.007743580883149715, valid_loss: 0.005200650775805116\nFOLD: 1, EPOCH: 24, train_loss: 0.007705767967683428, valid_loss: 0.0050064917498578625\nBeginning training for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.4132992299821447, valid_loss: 0.035659185921152435\nFOLD: 1, EPOCH: 1, train_loss: 0.027402122943278623, valid_loss: 0.020137790280083816\nFOLD: 1, EPOCH: 2, train_loss: 0.022346510935355637, valid_loss: 0.019387916661798954\nFOLD: 1, EPOCH: 3, train_loss: 0.021392721901921666, valid_loss: 0.018198501008252304\nFOLD: 1, EPOCH: 4, train_loss: 0.021039481548702017, valid_loss: 0.017928873809675377\nFOLD: 1, EPOCH: 5, train_loss: 0.020808145951698807, valid_loss: 0.018269374035298824\nFOLD: 1, EPOCH: 6, train_loss: 0.020681357285117403, valid_loss: 0.018018444068729877\nFOLD: 1, EPOCH: 7, train_loss: 0.020585678736953175, valid_loss: 0.01772920849422614\nFOLD: 1, EPOCH: 8, train_loss: 0.02041531014530098, valid_loss: 0.017930172383785248\nFOLD: 1, EPOCH: 9, train_loss: 0.020365444340688342, valid_loss: 0.01768670758853356\nFOLD: 1, EPOCH: 10, train_loss: 0.020253113361404222, valid_loss: 0.017758987843990326\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 11, train_loss: 0.02015307589488871, valid_loss: 0.017581801861524582\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 12, train_loss: 0.019869926549932537, valid_loss: 0.017583677545189857\nFOLD: 1, EPOCH: 13, train_loss: 0.01962512737030492, valid_loss: 0.017598610060910385\nFOLD: 1, EPOCH: 14, train_loss: 0.019509817911859823, valid_loss: 0.017434786073863506\nFOLD: 1, EPOCH: 15, train_loss: 0.019467824905672493, valid_loss: 0.017306255487104256\nFOLD: 1, EPOCH: 16, train_loss: 0.01939477349686272, valid_loss: 0.017463175890346367\nFOLD: 1, EPOCH: 17, train_loss: 0.019324539886677965, valid_loss: 0.01732842189570268\nFOLD: 1, EPOCH: 18, train_loss: 0.01930346619337797, valid_loss: 0.0175759286309282\nFOLD: 1, EPOCH: 19, train_loss: 0.019277211278676987, valid_loss: 0.017388376717766125\nFOLD: 1, EPOCH: 20, train_loss: 0.019212227095575893, valid_loss: 0.017509034213920433\nFOLD: 1, EPOCH: 21, train_loss: 0.01919627293725224, valid_loss: 0.017300232624014217\nFOLD: 1, EPOCH: 22, train_loss: 0.019197619136642006, valid_loss: 0.017351520868639152\nFOLD: 1, EPOCH: 23, train_loss: 0.01911006309092045, valid_loss: 0.01725937332957983\nFOLD: 1, EPOCH: 24, train_loss: 0.019092667409602332, valid_loss: 0.017336418231328327\nFOLD: 1, EPOCH: 25, train_loss: 0.019069131299415055, valid_loss: 0.017267637886106968\nFOLD: 1, EPOCH: 26, train_loss: 0.019062666058102074, valid_loss: 0.017222854929665726\nFOLD: 1, EPOCH: 27, train_loss: 0.019032081698670107, valid_loss: 0.017266586733361084\nFOLD: 1, EPOCH: 28, train_loss: 0.01902252742472817, valid_loss: 0.017296690804262955\nFOLD: 1, EPOCH: 29, train_loss: 0.018996825551285464, valid_loss: 0.017270454205572605\nFOLD: 1, EPOCH: 30, train_loss: 0.01897729538819369, valid_loss: 0.01726701979835828\nFOLD: 1, EPOCH: 31, train_loss: 0.0189715036574532, valid_loss: 0.017262169780830543\nFOLD: 1, EPOCH: 32, train_loss: 0.018968139391611603, valid_loss: 0.017264062849183876\nFOLD: 1, EPOCH: 33, train_loss: 0.018960869651945197, valid_loss: 0.017262058643003304\nFOLD: 1, EPOCH: 34, train_loss: 0.018956001409712958, valid_loss: 0.01726682453105847\nBeginning pretraining for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.7186313306584078, valid_loss: 0.6692249774932861\nFOLD: 2, EPOCH: 1, train_loss: 0.3427031671299654, valid_loss: 0.02608806888262431\nFOLD: 2, EPOCH: 2, train_loss: 0.01336589184425333, valid_loss: 0.005941916179532806\nFOLD: 2, EPOCH: 3, train_loss: 0.00899484209880671, valid_loss: 0.005388810532167554\nFOLD: 2, EPOCH: 4, train_loss: 0.008609017206575064, valid_loss: 0.005068758735433221\nFOLD: 2, EPOCH: 5, train_loss: 0.00844475202371969, valid_loss: 0.0047337564174085855\nFOLD: 2, EPOCH: 6, train_loss: 0.008414937238044599, valid_loss: 0.0051472072179118795\nFOLD: 2, EPOCH: 7, train_loss: 0.008398723405073671, valid_loss: 0.004968419748668869\nFOLD: 2, EPOCH: 8, train_loss: 0.008325309273512924, valid_loss: 0.004621538527620335\nFOLD: 2, EPOCH: 9, train_loss: 0.008314399747177958, valid_loss: 0.004747869252848129\nFOLD: 2, EPOCH: 10, train_loss: 0.00827480195199742, valid_loss: 0.0054857609793543816\nFOLD: 2, EPOCH: 11, train_loss: 0.008259577471215059, valid_loss: 0.00453250374024113\nFOLD: 2, EPOCH: 12, train_loss: 0.008218026281717946, valid_loss: 0.005408365201825897\nFOLD: 2, EPOCH: 13, train_loss: 0.008245736305766246, valid_loss: 0.006377895517895619\nFOLD: 2, EPOCH: 14, train_loss: 0.008161468253306606, valid_loss: 0.004816592050095399\nFOLD: 2, EPOCH: 15, train_loss: 0.008119076517794062, valid_loss: 0.004760306289729972\nFOLD: 2, EPOCH: 16, train_loss: 0.008085606934721856, valid_loss: 0.004701458926623066\nFOLD: 2, EPOCH: 17, train_loss: 0.008049958248567931, valid_loss: 0.004602544514151911\nFOLD: 2, EPOCH: 18, train_loss: 0.008008897879763562, valid_loss: 0.004910936035836737\nFOLD: 2, EPOCH: 19, train_loss: 0.007980512674240506, valid_loss: 0.004649266173752646\nFOLD: 2, EPOCH: 20, train_loss: 0.007919540779445977, valid_loss: 0.004969725618138909\nFOLD: 2, EPOCH: 21, train_loss: 0.007871992214966346, valid_loss: 0.004863028492157658\nFOLD: 2, EPOCH: 22, train_loss: 0.00782749244450208, valid_loss: 0.0046797625642890734\nFOLD: 2, EPOCH: 23, train_loss: 0.0077913424602764495, valid_loss: 0.005101035504291455\nFOLD: 2, EPOCH: 24, train_loss: 0.007740838713396122, valid_loss: 0.004755621077492833\nBeginning training for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.41560900649603677, valid_loss: 0.044531639044483505\nFOLD: 2, EPOCH: 1, train_loss: 0.027209048488122577, valid_loss: 0.020882527033487957\nFOLD: 2, EPOCH: 2, train_loss: 0.022120860021780517, valid_loss: 0.024920811876654625\nFOLD: 2, EPOCH: 3, train_loss: 0.021313595640308717, valid_loss: 0.018726641933123272\nFOLD: 2, EPOCH: 4, train_loss: 0.020896885160575893, valid_loss: 0.018498753507932026\nFOLD: 2, EPOCH: 5, train_loss: 0.02066634491305141, valid_loss: 0.01849611382931471\nFOLD: 2, EPOCH: 6, train_loss: 0.02054555873003076, valid_loss: 0.0185128931577007\nFOLD: 2, EPOCH: 7, train_loss: 0.020448345088345164, valid_loss: 0.018656708610554535\nFOLD: 2, EPOCH: 8, train_loss: 0.020321262781234348, valid_loss: 0.01822736021131277\nFOLD: 2, EPOCH: 9, train_loss: 0.020257398650488433, valid_loss: 0.018496280536055565\nFOLD: 2, EPOCH: 10, train_loss: 0.02029452538665603, valid_loss: 0.01836074112604062\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 11, train_loss: 0.02011666912585497, valid_loss: 0.01832324918359518\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 12, train_loss: 0.01985355428255656, valid_loss: 0.018117710016667843\nFOLD: 2, EPOCH: 13, train_loss: 0.019661135259358323, valid_loss: 0.018190956364075344\nFOLD: 2, EPOCH: 14, train_loss: 0.019571200780132237, valid_loss: 0.018068821479876835\nFOLD: 2, EPOCH: 15, train_loss: 0.019536572782432333, valid_loss: 0.018064144998788834\nFOLD: 2, EPOCH: 16, train_loss: 0.01947025122011409, valid_loss: 0.018106413694719475\nFOLD: 2, EPOCH: 17, train_loss: 0.019436146954403204, valid_loss: 0.018093181774020195\nFOLD: 2, EPOCH: 18, train_loss: 0.019342524656916365, valid_loss: 0.018052605912089348\nFOLD: 2, EPOCH: 19, train_loss: 0.01934369357631487, valid_loss: 0.01804827929784854\nFOLD: 2, EPOCH: 20, train_loss: 0.01930264547905501, valid_loss: 0.018114288958410423\nFOLD: 2, EPOCH: 21, train_loss: 0.01925889024620547, valid_loss: 0.01802266389131546\nFOLD: 2, EPOCH: 22, train_loss: 0.019241866765215117, valid_loss: 0.01799014738450448\nFOLD: 2, EPOCH: 23, train_loss: 0.019195511663223013, valid_loss: 0.017995052350064118\nFOLD: 2, EPOCH: 24, train_loss: 0.019153051516588998, valid_loss: 0.017983931116759777\nFOLD: 2, EPOCH: 25, train_loss: 0.01916547119617462, valid_loss: 0.01804351278891166\nFOLD: 2, EPOCH: 26, train_loss: 0.019138210069607284, valid_loss: 0.018012788146734238\nFOLD: 2, EPOCH: 27, train_loss: 0.01911738290287116, valid_loss: 0.017970930164059002\nFOLD: 2, EPOCH: 28, train_loss: 0.01908853091299534, valid_loss: 0.017978812257448833\nFOLD: 2, EPOCH: 29, train_loss: 0.019069266286404693, valid_loss: 0.017973429523408413\nFOLD: 2, EPOCH: 30, train_loss: 0.019076259971103248, valid_loss: 0.01799183152616024\nFOLD: 2, EPOCH: 31, train_loss: 0.01907555096070556, valid_loss: 0.0179843179260691\nFOLD: 2, EPOCH: 32, train_loss: 0.019053605056422597, valid_loss: 0.01798429557432731\nFOLD: 2, EPOCH: 33, train_loss: 0.019030218943953514, valid_loss: 0.017980408544341724\nFOLD: 2, EPOCH: 34, train_loss: 0.019042728413992068, valid_loss: 0.0179800766830643\nBeginning pretraining for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.7186475483810201, valid_loss: 0.6693408389886221\nFOLD: 3, EPOCH: 1, train_loss: 0.3432749144952087, valid_loss: 0.02579032691816489\nFOLD: 3, EPOCH: 2, train_loss: 0.013558105592999388, valid_loss: 0.005954432068392634\nFOLD: 3, EPOCH: 3, train_loss: 0.008875810815130962, valid_loss: 0.005321209629376729\nFOLD: 3, EPOCH: 4, train_loss: 0.008559643181369585, valid_loss: 0.004991316391776006\nFOLD: 3, EPOCH: 5, train_loss: 0.00843967464478577, valid_loss: 0.005054795338461797\nFOLD: 3, EPOCH: 6, train_loss: 0.008386675871032126, valid_loss: 0.004740287782624364\nFOLD: 3, EPOCH: 7, train_loss: 0.008331477327053161, valid_loss: 0.004855697120850285\nFOLD: 3, EPOCH: 8, train_loss: 0.008296330664854716, valid_loss: 0.00471162764976422\nFOLD: 3, EPOCH: 9, train_loss: 0.008281719103893814, valid_loss: 0.004984576565523942\nFOLD: 3, EPOCH: 10, train_loss: 0.008250678437488043, valid_loss: 0.005625390137235324\nFOLD: 3, EPOCH: 11, train_loss: 0.008205647032488795, valid_loss: 0.005233125217879812\nFOLD: 3, EPOCH: 12, train_loss: 0.008198979201123995, valid_loss: 0.0049128523872544365\nFOLD: 3, EPOCH: 13, train_loss: 0.008143707657890284, valid_loss: 0.004639335985605915\nFOLD: 3, EPOCH: 14, train_loss: 0.00812734679921585, valid_loss: 0.0049125276661167545\nFOLD: 3, EPOCH: 15, train_loss: 0.00808640242115978, valid_loss: 0.0051738182082772255\nFOLD: 3, EPOCH: 16, train_loss: 0.008059768882744452, valid_loss: 0.004744723827267687\nFOLD: 3, EPOCH: 17, train_loss: 0.00802540113492047, valid_loss: 0.00518797702776889\nFOLD: 3, EPOCH: 18, train_loss: 0.007989119437030134, valid_loss: 0.00483194839519759\nFOLD: 3, EPOCH: 19, train_loss: 0.00794411709477358, valid_loss: 0.005124795017763972\nFOLD: 3, EPOCH: 20, train_loss: 0.00790248514043496, valid_loss: 0.004834207473322749\nFOLD: 3, EPOCH: 21, train_loss: 0.007870068883194643, valid_loss: 0.004777346815293034\nFOLD: 3, EPOCH: 22, train_loss: 0.007833904888042632, valid_loss: 0.00474049608844022\nFOLD: 3, EPOCH: 23, train_loss: 0.00777525276712635, valid_loss: 0.0048628379590809345\nFOLD: 3, EPOCH: 24, train_loss: 0.007735108655384358, valid_loss: 0.005058752062420051\nBeginning training for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.41195281967520714, valid_loss: 0.03573959693312645\nFOLD: 3, EPOCH: 1, train_loss: 0.0271549338255735, valid_loss: 0.020720322305957477\nFOLD: 3, EPOCH: 2, train_loss: 0.022474780733532766, valid_loss: 0.019652315725882847\nFOLD: 3, EPOCH: 3, train_loss: 0.02147184032946825, valid_loss: 0.018481009639799595\nFOLD: 3, EPOCH: 4, train_loss: 0.021159105002880096, valid_loss: 0.01914315236111482\nFOLD: 3, EPOCH: 5, train_loss: 0.020945454947650433, valid_loss: 0.018252648102740448\nFOLD: 3, EPOCH: 6, train_loss: 0.02080502498018391, valid_loss: 0.018687875630954903\nFOLD: 3, EPOCH: 7, train_loss: 0.020637002390097168, valid_loss: 0.01775593465814988\nFOLD: 3, EPOCH: 8, train_loss: 0.02055940103224095, valid_loss: 0.01788984363277753\nFOLD: 3, EPOCH: 9, train_loss: 0.020472685094265378, valid_loss: 0.017622525803744793\nFOLD: 3, EPOCH: 10, train_loss: 0.020400654250646338, valid_loss: 0.018031869394083817\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 11, train_loss: 0.020268296932472903, valid_loss: 0.01800321228802204\nFOLD: 3, EPOCH: 12, train_loss: 0.020022849442765993, valid_loss: 0.0174250981460015\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 13, train_loss: 0.01982300608035396, valid_loss: 0.017365524545311928\nFOLD: 3, EPOCH: 14, train_loss: 0.019709189422428608, valid_loss: 0.017432994209229946\nFOLD: 3, EPOCH: 15, train_loss: 0.019598626860362643, valid_loss: 0.017299249457816284\nFOLD: 3, EPOCH: 16, train_loss: 0.01955462630619021, valid_loss: 0.017141112436850865\nFOLD: 3, EPOCH: 17, train_loss: 0.019476938609252956, valid_loss: 0.01719186796496312\nFOLD: 3, EPOCH: 18, train_loss: 0.019448556882493636, valid_loss: 0.017144187663992245\nFOLD: 3, EPOCH: 19, train_loss: 0.01940158803892486, valid_loss: 0.017295838023225468\nFOLD: 3, EPOCH: 20, train_loss: 0.019387449630919623, valid_loss: 0.01716679676125447\nFOLD: 3, EPOCH: 21, train_loss: 0.01938140332041418, valid_loss: 0.017331987619400024\nFOLD: 3, EPOCH: 22, train_loss: 0.019328539424082813, valid_loss: 0.01716263343890508\nFOLD: 3, EPOCH: 23, train_loss: 0.019292069270330316, valid_loss: 0.01714765311529239\nFOLD: 3, EPOCH: 24, train_loss: 0.01924458288532846, valid_loss: 0.017110374756157398\nFOLD: 3, EPOCH: 25, train_loss: 0.019228815101087093, valid_loss: 0.017119354257980984\nFOLD: 3, EPOCH: 26, train_loss: 0.019228416714159882, valid_loss: 0.017158934536079567\nFOLD: 3, EPOCH: 27, train_loss: 0.019197364776011777, valid_loss: 0.017143180904289086\nFOLD: 3, EPOCH: 28, train_loss: 0.019197320839499727, valid_loss: 0.017109219916164875\nFOLD: 3, EPOCH: 29, train_loss: 0.019142893362132943, valid_loss: 0.017103288633128006\nFOLD: 3, EPOCH: 30, train_loss: 0.019127341151675758, valid_loss: 0.017114351813991863\nFOLD: 3, EPOCH: 31, train_loss: 0.019148885382010657, valid_loss: 0.017134221891562145\nFOLD: 3, EPOCH: 32, train_loss: 0.01913556604481795, valid_loss: 0.017104923104246456\nFOLD: 3, EPOCH: 33, train_loss: 0.019125105922713, valid_loss: 0.01711892895400524\nFOLD: 3, EPOCH: 34, train_loss: 0.019133202841176707, valid_loss: 0.017114071796337765\nBeginning pretraining for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.7187256006633534, valid_loss: 0.6693390409151713\nFOLD: 4, EPOCH: 1, train_loss: 0.34116702767855983, valid_loss: 0.026049941778182983\nFOLD: 4, EPOCH: 2, train_loss: 0.01367111676646506, valid_loss: 0.005994118827705582\nFOLD: 4, EPOCH: 3, train_loss: 0.008890351143610828, valid_loss: 0.0052626344840973616\nFOLD: 4, EPOCH: 4, train_loss: 0.008617218707085532, valid_loss: 0.005096419403950374\nFOLD: 4, EPOCH: 5, train_loss: 0.008439023294212186, valid_loss: 0.00492163832920293\nFOLD: 4, EPOCH: 6, train_loss: 0.008427455829566015, valid_loss: 0.004684649873524904\nFOLD: 4, EPOCH: 7, train_loss: 0.008305370246114977, valid_loss: 0.004714430775493383\nFOLD: 4, EPOCH: 8, train_loss: 0.008260383804821792, valid_loss: 0.004659607463205854\nFOLD: 4, EPOCH: 9, train_loss: 0.008231120170367992, valid_loss: 0.005155361257493496\nFOLD: 4, EPOCH: 10, train_loss: 0.008210797890034668, valid_loss: 0.005171934608370066\nFOLD: 4, EPOCH: 11, train_loss: 0.00820046801613096, valid_loss: 0.00480284991984566\nFOLD: 4, EPOCH: 12, train_loss: 0.00814678954124889, valid_loss: 0.004714487508560221\nFOLD: 4, EPOCH: 13, train_loss: 0.008112528087461697, valid_loss: 0.004817457248767217\nFOLD: 4, EPOCH: 14, train_loss: 0.008069643339909175, valid_loss: 0.00483259876879553\nFOLD: 4, EPOCH: 15, train_loss: 0.008051044006338891, valid_loss: 0.005039539188146591\nFOLD: 4, EPOCH: 16, train_loss: 0.00800192657419864, valid_loss: 0.004922224519153436\nFOLD: 4, EPOCH: 17, train_loss: 0.007962717811631806, valid_loss: 0.0049075346905738115\nFOLD: 4, EPOCH: 18, train_loss: 0.007930274998002192, valid_loss: 0.004686094432448347\nFOLD: 4, EPOCH: 19, train_loss: 0.007886543061912936, valid_loss: 0.004836327241112788\nFOLD: 4, EPOCH: 20, train_loss: 0.007842410889947238, valid_loss: 0.004913374160726865\nFOLD: 4, EPOCH: 21, train_loss: 0.007816599709365298, valid_loss: 0.004619903784866135\nFOLD: 4, EPOCH: 22, train_loss: 0.007749348166672622, valid_loss: 0.004936193892111381\nFOLD: 4, EPOCH: 23, train_loss: 0.0077020660152330115, valid_loss: 0.00478415369677047\nFOLD: 4, EPOCH: 24, train_loss: 0.0076685059979996265, valid_loss: 0.004955870642637213\nBeginning training for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.4014545090715675, valid_loss: 0.03767670380572478\nFOLD: 4, EPOCH: 1, train_loss: 0.026766938431298033, valid_loss: 0.020583132281899452\nFOLD: 4, EPOCH: 2, train_loss: 0.022645152776556855, valid_loss: 0.019266208944221336\nFOLD: 4, EPOCH: 3, train_loss: 0.021666333517607522, valid_loss: 0.018590884904066723\nFOLD: 4, EPOCH: 4, train_loss: 0.0212806200279909, valid_loss: 0.018377732795973618\nFOLD: 4, EPOCH: 5, train_loss: 0.020971139180747903, valid_loss: 0.018544261654218037\nFOLD: 4, EPOCH: 6, train_loss: 0.02086597072946675, valid_loss: 0.018330234413345654\nFOLD: 4, EPOCH: 7, train_loss: 0.0207132421543493, valid_loss: 0.018326047187050182\nFOLD: 4, EPOCH: 8, train_loss: 0.020556215087280554, valid_loss: 0.01792849910755952\nFOLD: 4, EPOCH: 9, train_loss: 0.02041173364747973, valid_loss: 0.018138593062758446\nFOLD: 4, EPOCH: 10, train_loss: 0.02033429324407788, valid_loss: 0.018143728375434875\nFOLD: 4, EPOCH: 11, train_loss: 0.020243228796650383, valid_loss: 0.017958315399785835\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 12, train_loss: 0.019977743956534302, valid_loss: 0.0179801220074296\nFOLD: 4, EPOCH: 13, train_loss: 0.019784091643112546, valid_loss: 0.017847370977203052\nFOLD: 4, EPOCH: 14, train_loss: 0.01969714890069821, valid_loss: 0.01781205243120591\nFOLD: 4, EPOCH: 15, train_loss: 0.019643487134838804, valid_loss: 0.017779977060854435\nFOLD: 4, EPOCH: 16, train_loss: 0.019584312651525524, valid_loss: 0.017734741792082787\nFOLD: 4, EPOCH: 17, train_loss: 0.019537072190467047, valid_loss: 0.01779054260502259\nFOLD: 4, EPOCH: 18, train_loss: 0.019513850648175266, valid_loss: 0.017812936566770077\nFOLD: 4, EPOCH: 19, train_loss: 0.019467383897041574, valid_loss: 0.017803688223163288\nFOLD: 4, EPOCH: 20, train_loss: 0.019432393012239653, valid_loss: 0.01780870712051789\nFOLD: 4, EPOCH: 21, train_loss: 0.0193991868154091, valid_loss: 0.017647740120689075\nFOLD: 4, EPOCH: 22, train_loss: 0.019362985649529624, valid_loss: 0.01781683259954055\nFOLD: 4, EPOCH: 23, train_loss: 0.019373597303295836, valid_loss: 0.017707120316723984\nFOLD: 4, EPOCH: 24, train_loss: 0.019324089400470257, valid_loss: 0.017622779744366806\nFOLD: 4, EPOCH: 25, train_loss: 0.01930228796075372, valid_loss: 0.017677028042574722\nFOLD: 4, EPOCH: 26, train_loss: 0.01926842052489519, valid_loss: 0.01769221853464842\nFOLD: 4, EPOCH: 27, train_loss: 0.01925012392594534, valid_loss: 0.01763383485376835\nFOLD: 4, EPOCH: 28, train_loss: 0.019225251039161402, valid_loss: 0.017668988245228927\nFOLD: 4, EPOCH: 29, train_loss: 0.019227211199262562, valid_loss: 0.017636371155579884\nFOLD: 4, EPOCH: 30, train_loss: 0.019221032312249437, valid_loss: 0.01764853671193123\nFOLD: 4, EPOCH: 31, train_loss: 0.019197049276793703, valid_loss: 0.01762329942236344\nFOLD: 4, EPOCH: 32, train_loss: 0.019182009622454643, valid_loss: 0.017622018853823345\nFOLD: 4, EPOCH: 33, train_loss: 0.01917750377427129, valid_loss: 0.017630883492529392\nFOLD: 4, EPOCH: 34, train_loss: 0.01920207546037786, valid_loss: 0.017625031992793083\nBeginning pretraining for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.7185351480455959, valid_loss: 0.6693183680375417\nFOLD: 5, EPOCH: 1, train_loss: 0.34321315520826506, valid_loss: 0.026854575611650944\nFOLD: 5, EPOCH: 2, train_loss: 0.013429340742090169, valid_loss: 0.0059757221800585585\nFOLD: 5, EPOCH: 3, train_loss: 0.00881401717881946, valid_loss: 0.005360335344448686\nFOLD: 5, EPOCH: 4, train_loss: 0.008441499393323766, valid_loss: 0.005019982190181811\nFOLD: 5, EPOCH: 5, train_loss: 0.008353834711563061, valid_loss: 0.00503101572394371\nFOLD: 5, EPOCH: 6, train_loss: 0.008281359585988171, valid_loss: 0.004711679803828399\nFOLD: 5, EPOCH: 7, train_loss: 0.008254990693839157, valid_loss: 0.004630316824962695\nFOLD: 5, EPOCH: 8, train_loss: 0.00821515012477689, valid_loss: 0.004874861100688577\nFOLD: 5, EPOCH: 9, train_loss: 0.008175795966321054, valid_loss: 0.0053474747110158205\nFOLD: 5, EPOCH: 10, train_loss: 0.008158253533217837, valid_loss: 0.005160463120167454\nFOLD: 5, EPOCH: 11, train_loss: 0.008108330192044377, valid_loss: 0.0055572171695530415\nFOLD: 5, EPOCH: 12, train_loss: 0.008077833075624178, valid_loss: 0.004689270397648215\nFOLD: 5, EPOCH: 13, train_loss: 0.008039401145651937, valid_loss: 0.005168910526360075\nFOLD: 5, EPOCH: 14, train_loss: 0.00800091911600355, valid_loss: 0.004854625246177117\nFOLD: 5, EPOCH: 15, train_loss: 0.007970641517792554, valid_loss: 0.004829915318017204\nFOLD: 5, EPOCH: 16, train_loss: 0.007942615652128178, valid_loss: 0.004698959644883871\nFOLD: 5, EPOCH: 17, train_loss: 0.007914468454306616, valid_loss: 0.004763935537387927\nFOLD: 5, EPOCH: 18, train_loss: 0.007892146090264706, valid_loss: 0.004945651162415743\nFOLD: 5, EPOCH: 19, train_loss: 0.007852107738418615, valid_loss: 0.0048403524948904915\nFOLD: 5, EPOCH: 20, train_loss: 0.007825296125648654, valid_loss: 0.004969512578099966\nFOLD: 5, EPOCH: 21, train_loss: 0.007804957136292668, valid_loss: 0.004958747653290629\nFOLD: 5, EPOCH: 22, train_loss: 0.0077612699447747535, valid_loss: 0.004856274463236332\nFOLD: 5, EPOCH: 23, train_loss: 0.007738094886436182, valid_loss: 0.005057234472284715\nFOLD: 5, EPOCH: 24, train_loss: 0.007689779350424514, valid_loss: 0.004760939007004102\nBeginning training for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.43025321447673964, valid_loss: 0.04584776175518831\nFOLD: 5, EPOCH: 1, train_loss: 0.028194008230724755, valid_loss: 0.0212325310955445\nFOLD: 5, EPOCH: 2, train_loss: 0.022367796422365832, valid_loss: 0.02305499588449796\nFOLD: 5, EPOCH: 3, train_loss: 0.021459569223225117, valid_loss: 0.018627368845045567\nFOLD: 5, EPOCH: 4, train_loss: 0.021067546406651243, valid_loss: 0.01873554599781831\nFOLD: 5, EPOCH: 5, train_loss: 0.020818028325105414, valid_loss: 0.018544393902023632\nFOLD: 5, EPOCH: 6, train_loss: 0.02061301250668133, valid_loss: 0.018364340998232365\nFOLD: 5, EPOCH: 7, train_loss: 0.020479959874030423, valid_loss: 0.018144968586663406\nFOLD: 5, EPOCH: 8, train_loss: 0.02040008537690429, valid_loss: 0.01821099066485961\nFOLD: 5, EPOCH: 9, train_loss: 0.020291233599624214, valid_loss: 0.01841845673819383\nFOLD: 5, EPOCH: 10, train_loss: 0.020265110439675695, valid_loss: 0.018049341005583603\nFOLD: 5, EPOCH: 11, train_loss: 0.020132985952145913, valid_loss: 0.017927090637385845\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 12, train_loss: 0.020006888162563827, valid_loss: 0.017897725105285645\nFOLD: 5, EPOCH: 13, train_loss: 0.01976422549170606, valid_loss: 0.01793006807565689\nFOLD: 5, EPOCH: 14, train_loss: 0.019610663547235375, valid_loss: 0.017901193350553513\nFOLD: 5, EPOCH: 15, train_loss: 0.01955958757111255, valid_loss: 0.017945104899505775\nFOLD: 5, EPOCH: 16, train_loss: 0.01948787403457305, valid_loss: 0.01783840482433637\nFOLD: 5, EPOCH: 17, train_loss: 0.019394963651018983, valid_loss: 0.017859545225898426\nFOLD: 5, EPOCH: 18, train_loss: 0.019322975538671017, valid_loss: 0.017795441672205925\nFOLD: 5, EPOCH: 19, train_loss: 0.019269213025622508, valid_loss: 0.017844277744491894\nFOLD: 5, EPOCH: 20, train_loss: 0.019205511164139297, valid_loss: 0.017863709169129532\nFOLD: 5, EPOCH: 21, train_loss: 0.019133891088559347, valid_loss: 0.0177329508587718\nFOLD: 5, EPOCH: 22, train_loss: 0.01907946410424569, valid_loss: 0.01774724603941043\nFOLD: 5, EPOCH: 23, train_loss: 0.018993702004937565, valid_loss: 0.017752460824946564\nFOLD: 5, EPOCH: 24, train_loss: 0.018919977249906343, valid_loss: 0.017740934776763122\nFOLD: 5, EPOCH: 25, train_loss: 0.018850972437683272, valid_loss: 0.017692827619612217\nFOLD: 5, EPOCH: 26, train_loss: 0.018787609084564096, valid_loss: 0.01772562911113103\nFOLD: 5, EPOCH: 27, train_loss: 0.018714039805619156, valid_loss: 0.01765793189406395\nFOLD: 5, EPOCH: 28, train_loss: 0.018667297492570737, valid_loss: 0.017655605139831703\nFOLD: 5, EPOCH: 29, train_loss: 0.018602330128059667, valid_loss: 0.017666008633871872\nFOLD: 5, EPOCH: 30, train_loss: 0.01854763738811016, valid_loss: 0.01766611511508624\nFOLD: 5, EPOCH: 31, train_loss: 0.018507502480026555, valid_loss: 0.017677451483905315\nFOLD: 5, EPOCH: 32, train_loss: 0.01849522310144761, valid_loss: 0.01767106323192517\nFOLD: 5, EPOCH: 33, train_loss: 0.01846671586527544, valid_loss: 0.017664815299212933\nFOLD: 5, EPOCH: 34, train_loss: 0.018447179730762455, valid_loss: 0.017662838101387024\nBeginning pretraining for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.7185614529777976, valid_loss: 0.6691081126530966\nFOLD: 6, EPOCH: 1, train_loss: 0.34209677412667694, valid_loss: 0.0259382426738739\nFOLD: 6, EPOCH: 2, train_loss: 0.013333791949074058, valid_loss: 0.005864112948377927\nFOLD: 6, EPOCH: 3, train_loss: 0.008793589741210727, valid_loss: 0.005203573033213615\nFOLD: 6, EPOCH: 4, train_loss: 0.008499556686729193, valid_loss: 0.004869908094406128\nFOLD: 6, EPOCH: 5, train_loss: 0.008339838604168856, valid_loss: 0.004776052975406249\nFOLD: 6, EPOCH: 6, train_loss: 0.008293361266088836, valid_loss: 0.004620645505686601\nFOLD: 6, EPOCH: 7, train_loss: 0.008213278851197922, valid_loss: 0.004724438302218914\nFOLD: 6, EPOCH: 8, train_loss: 0.008185259244569084, valid_loss: 0.004994184710085392\nFOLD: 6, EPOCH: 9, train_loss: 0.008150106145288138, valid_loss: 0.0045307760980601115\nFOLD: 6, EPOCH: 10, train_loss: 0.008114001467166579, valid_loss: 0.0044448811483259005\nFOLD: 6, EPOCH: 11, train_loss: 0.008087372229270199, valid_loss: 0.004554727619203429\nFOLD: 6, EPOCH: 12, train_loss: 0.008048375164542128, valid_loss: 0.0050971913151443005\nFOLD: 6, EPOCH: 13, train_loss: 0.008021410088986158, valid_loss: 0.004792475452025731\nFOLD: 6, EPOCH: 14, train_loss: 0.007982199819868101, valid_loss: 0.004718744584048788\nFOLD: 6, EPOCH: 15, train_loss: 0.007949845522970837, valid_loss: 0.004721079487353563\nFOLD: 6, EPOCH: 16, train_loss: 0.007929521886741413, valid_loss: 0.004737709804127614\nFOLD: 6, EPOCH: 17, train_loss: 0.007893783547093762, valid_loss: 0.004443408688530326\nFOLD: 6, EPOCH: 18, train_loss: 0.007866570219287978, valid_loss: 0.0048644527948151035\nFOLD: 6, EPOCH: 19, train_loss: 0.00782600398558904, valid_loss: 0.004971510032191873\nFOLD: 6, EPOCH: 20, train_loss: 0.007803836946978289, valid_loss: 0.004805378460635741\nFOLD: 6, EPOCH: 21, train_loss: 0.007764214170439278, valid_loss: 0.004975843786572416\nFOLD: 6, EPOCH: 22, train_loss: 0.007732352214481901, valid_loss: 0.004612170315037171\nFOLD: 6, EPOCH: 23, train_loss: 0.007690503535901799, valid_loss: 0.004542431561276317\nFOLD: 6, EPOCH: 24, train_loss: 0.007661270320086795, valid_loss: 0.004646197039013107\nBeginning training for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.40984062554643436, valid_loss: 0.03808977951606115\nFOLD: 6, EPOCH: 1, train_loss: 0.02715384927304352, valid_loss: 0.02057316681991021\nFOLD: 6, EPOCH: 2, train_loss: 0.022578133379711825, valid_loss: 0.018566584214568138\nFOLD: 6, EPOCH: 3, train_loss: 0.021638509762637755, valid_loss: 0.01818353372315566\nFOLD: 6, EPOCH: 4, train_loss: 0.02128456352169023, valid_loss: 0.01799278085430463\nFOLD: 6, EPOCH: 5, train_loss: 0.021055816946660772, valid_loss: 0.01780623011291027\nFOLD: 6, EPOCH: 6, train_loss: 0.020898528561434326, valid_loss: 0.01768452984591325\nFOLD: 6, EPOCH: 7, train_loss: 0.020772811956703663, valid_loss: 0.017680726014077663\nFOLD: 6, EPOCH: 8, train_loss: 0.02071864442790256, valid_loss: 0.01749773509800434\nFOLD: 6, EPOCH: 9, train_loss: 0.020575652720735353, valid_loss: 0.01784710182497899\nFOLD: 6, EPOCH: 10, train_loss: 0.02052111699081519, valid_loss: 0.017566219593087833\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 11, train_loss: 0.020403649000560537, valid_loss: 0.01772443825999896\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 12, train_loss: 0.020100983243216488, valid_loss: 0.0172483225663503\nFOLD: 6, EPOCH: 13, train_loss: 0.01989217918804463, valid_loss: 0.01744465467830499\nFOLD: 6, EPOCH: 14, train_loss: 0.019811489181045222, valid_loss: 0.017419295695920784\nFOLD: 6, EPOCH: 15, train_loss: 0.019789038609494183, valid_loss: 0.01724946250518163\nFOLD: 6, EPOCH: 16, train_loss: 0.01973257597316714, valid_loss: 0.017339063808321953\nFOLD: 6, EPOCH: 17, train_loss: 0.019670580678126392, valid_loss: 0.017151078519721825\nFOLD: 6, EPOCH: 18, train_loss: 0.019641666618340155, valid_loss: 0.01707552094012499\nFOLD: 6, EPOCH: 19, train_loss: 0.01958359679316773, valid_loss: 0.017146521247923374\nFOLD: 6, EPOCH: 20, train_loss: 0.019557340618442085, valid_loss: 0.017118929885327816\nFOLD: 6, EPOCH: 21, train_loss: 0.019519389826146996, valid_loss: 0.01719528002043565\nFOLD: 6, EPOCH: 22, train_loss: 0.019494541646803126, valid_loss: 0.017314137890934944\nFOLD: 6, EPOCH: 23, train_loss: 0.01945965046830037, valid_loss: 0.017088876105844975\nFOLD: 6, EPOCH: 24, train_loss: 0.019432070719845155, valid_loss: 0.01712796526650588\nFOLD: 6, EPOCH: 25, train_loss: 0.019400600343942642, valid_loss: 0.017114755387107532\nFOLD: 6, EPOCH: 26, train_loss: 0.01940159351729295, valid_loss: 0.017070801618198555\nFOLD: 6, EPOCH: 27, train_loss: 0.019376974607653478, valid_loss: 0.017100503978629906\nFOLD: 6, EPOCH: 28, train_loss: 0.019350851612056002, valid_loss: 0.01712203739831845\nFOLD: 6, EPOCH: 29, train_loss: 0.01934898814515156, valid_loss: 0.01707895938307047\nFOLD: 6, EPOCH: 30, train_loss: 0.019350431202089086, valid_loss: 0.017114368577798206\nFOLD: 6, EPOCH: 31, train_loss: 0.019328656332457766, valid_loss: 0.017069230787456036\nFOLD: 6, EPOCH: 32, train_loss: 0.019329594174290404, valid_loss: 0.01709278766065836\nFOLD: 6, EPOCH: 33, train_loss: 0.019326560363611755, valid_loss: 0.01709823589771986\nFOLD: 6, EPOCH: 34, train_loss: 0.019304651493097052, valid_loss: 0.017091008834540844\nBeginning pretraining for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.718931224416284, valid_loss: 0.6686531603336334\nFOLD: 0, EPOCH: 1, train_loss: 0.33981376919238004, valid_loss: 0.026248903013765812\nFOLD: 0, EPOCH: 2, train_loss: 0.01343175336061155, valid_loss: 0.005998884870981176\nFOLD: 0, EPOCH: 3, train_loss: 0.00886052638730582, valid_loss: 0.005297929824640353\nFOLD: 0, EPOCH: 4, train_loss: 0.008563677588587297, valid_loss: 0.005347455696513255\nFOLD: 0, EPOCH: 5, train_loss: 0.008428336312884794, valid_loss: 0.004869963973760605\nFOLD: 0, EPOCH: 6, train_loss: 0.008386918822960818, valid_loss: 0.004913796282683809\nFOLD: 0, EPOCH: 7, train_loss: 0.008392760379459052, valid_loss: 0.00480671141607066\nFOLD: 0, EPOCH: 8, train_loss: 0.008291125516681111, valid_loss: 0.004776674400394161\nFOLD: 0, EPOCH: 9, train_loss: 0.008259335943662068, valid_loss: 0.006550366214166085\nFOLD: 0, EPOCH: 10, train_loss: 0.008214987773338662, valid_loss: 0.004584084032103419\nFOLD: 0, EPOCH: 11, train_loss: 0.008191959603744395, valid_loss: 0.0047517089794079466\nFOLD: 0, EPOCH: 12, train_loss: 0.008144149885458104, valid_loss: 0.005281611888979872\nFOLD: 0, EPOCH: 13, train_loss: 0.008101682726512937, valid_loss: 0.0051953422371298075\nFOLD: 0, EPOCH: 14, train_loss: 0.008068305481334819, valid_loss: 0.004895046741391222\nFOLD: 0, EPOCH: 15, train_loss: 0.008035510600380161, valid_loss: 0.0048532621779789524\nFOLD: 0, EPOCH: 16, train_loss: 0.007992914480650249, valid_loss: 0.004716134862974286\nFOLD: 0, EPOCH: 17, train_loss: 0.00796500627663644, valid_loss: 0.00488418930520614\nFOLD: 0, EPOCH: 18, train_loss: 0.007922131458626074, valid_loss: 0.005042110104113817\nFOLD: 0, EPOCH: 19, train_loss: 0.007900780217502923, valid_loss: 0.004898754957442482\nFOLD: 0, EPOCH: 20, train_loss: 0.007860489011577824, valid_loss: 0.004930088529363275\nFOLD: 0, EPOCH: 21, train_loss: 0.007836080060395248, valid_loss: 0.005123051581904292\nFOLD: 0, EPOCH: 22, train_loss: 0.0077968269367428384, valid_loss: 0.004867323208600283\nFOLD: 0, EPOCH: 23, train_loss: 0.007751766523784574, valid_loss: 0.004842671022439997\nFOLD: 0, EPOCH: 24, train_loss: 0.007704999106114402, valid_loss: 0.005255601679285367\nBeginning training for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.41773687237325835, valid_loss: 0.04128877508143584\nFOLD: 0, EPOCH: 1, train_loss: 0.02748754746554529, valid_loss: 0.022014984550575416\nFOLD: 0, EPOCH: 2, train_loss: 0.022213810616556334, valid_loss: 0.01859937918682893\nFOLD: 0, EPOCH: 3, train_loss: 0.02141871555324863, valid_loss: 0.01841472772260507\nFOLD: 0, EPOCH: 4, train_loss: 0.021032851901562774, valid_loss: 0.018038998047510784\nFOLD: 0, EPOCH: 5, train_loss: 0.020854362505762017, valid_loss: 0.01839581038802862\nFOLD: 0, EPOCH: 6, train_loss: 0.02073729076587102, valid_loss: 0.01798982049028079\nFOLD: 0, EPOCH: 7, train_loss: 0.020577061702223384, valid_loss: 0.017946780038376648\nFOLD: 0, EPOCH: 8, train_loss: 0.02044881261227762, valid_loss: 0.017674974786738556\nFOLD: 0, EPOCH: 9, train_loss: 0.02034648705054732, valid_loss: 0.01757675347228845\nFOLD: 0, EPOCH: 10, train_loss: 0.020229665398159447, valid_loss: 0.01801450519512097\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 11, train_loss: 0.020213913829887614, valid_loss: 0.017692890639106434\nFOLD: 0, EPOCH: 12, train_loss: 0.01991413298117764, valid_loss: 0.017553791714211304\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 13, train_loss: 0.019796182182343566, valid_loss: 0.017485020371774834\nFOLD: 0, EPOCH: 14, train_loss: 0.01963445812682895, valid_loss: 0.01742516551166773\nFOLD: 0, EPOCH: 15, train_loss: 0.019532284699380398, valid_loss: 0.01740387051055829\nFOLD: 0, EPOCH: 16, train_loss: 0.019478864748688304, valid_loss: 0.017367270464698475\nFOLD: 0, EPOCH: 17, train_loss: 0.019391940139672336, valid_loss: 0.017506228139003117\nFOLD: 0, EPOCH: 18, train_loss: 0.01935484024750836, valid_loss: 0.017342396390934784\nFOLD: 0, EPOCH: 19, train_loss: 0.019310081925462273, valid_loss: 0.01747801434248686\nFOLD: 0, EPOCH: 20, train_loss: 0.019292379510315025, valid_loss: 0.017290934299429257\nFOLD: 0, EPOCH: 21, train_loss: 0.019270896199433243, valid_loss: 0.017322359296182793\nFOLD: 0, EPOCH: 22, train_loss: 0.019216059323619392, valid_loss: 0.01733417995274067\nFOLD: 0, EPOCH: 23, train_loss: 0.01917729209012845, valid_loss: 0.017327606057127316\nFOLD: 0, EPOCH: 24, train_loss: 0.019139136023381177, valid_loss: 0.01730606270333131\nFOLD: 0, EPOCH: 25, train_loss: 0.019126334282405236, valid_loss: 0.01728683492789666\nFOLD: 0, EPOCH: 26, train_loss: 0.01907267196870902, valid_loss: 0.017281935550272465\nFOLD: 0, EPOCH: 27, train_loss: 0.019084745086729527, valid_loss: 0.017301045358181\nFOLD: 0, EPOCH: 28, train_loss: 0.019067213158397114, valid_loss: 0.017272945183018844\nFOLD: 0, EPOCH: 29, train_loss: 0.01903821895008578, valid_loss: 0.01726679566005866\nFOLD: 0, EPOCH: 30, train_loss: 0.019043932997566813, valid_loss: 0.017267444171011448\nFOLD: 0, EPOCH: 31, train_loss: 0.019034996026140803, valid_loss: 0.01726617757230997\nFOLD: 0, EPOCH: 32, train_loss: 0.01901730621124015, valid_loss: 0.017274290323257446\nFOLD: 0, EPOCH: 33, train_loss: 0.01902374192414915, valid_loss: 0.01726203504949808\nFOLD: 0, EPOCH: 34, train_loss: 0.019023855764637974, valid_loss: 0.017259471739331882\nBeginning pretraining for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.7187837555127985, valid_loss: 0.6662418643633524\nFOLD: 1, EPOCH: 1, train_loss: 0.33997374619631204, valid_loss: 0.026461969129741192\nFOLD: 1, EPOCH: 2, train_loss: 0.013505979742416564, valid_loss: 0.006176199841623505\nFOLD: 1, EPOCH: 3, train_loss: 0.008816999410662581, valid_loss: 0.005376973267023762\nFOLD: 1, EPOCH: 4, train_loss: 0.008478702840340488, valid_loss: 0.005109020819266637\nFOLD: 1, EPOCH: 5, train_loss: 0.008369389442069566, valid_loss: 0.004994889022782445\nFOLD: 1, EPOCH: 6, train_loss: 0.008357159067493151, valid_loss: 0.004971947132920225\nFOLD: 1, EPOCH: 7, train_loss: 0.008283768753137659, valid_loss: 0.005428057552004854\nFOLD: 1, EPOCH: 8, train_loss: 0.008211706806083812, valid_loss: 0.005013054547210534\nFOLD: 1, EPOCH: 9, train_loss: 0.00820754625944092, valid_loss: 0.004953377336884539\nFOLD: 1, EPOCH: 10, train_loss: 0.008159919819958946, valid_loss: 0.005512881558388472\nFOLD: 1, EPOCH: 11, train_loss: 0.008148238829830113, valid_loss: 0.0053395159387340145\nFOLD: 1, EPOCH: 12, train_loss: 0.008094509269165643, valid_loss: 0.005202763248234987\nFOLD: 1, EPOCH: 13, train_loss: 0.008071821895154083, valid_loss: 0.004716224890823166\nFOLD: 1, EPOCH: 14, train_loss: 0.008030487566857654, valid_loss: 0.004959065544729431\nFOLD: 1, EPOCH: 15, train_loss: 0.008011105004698038, valid_loss: 0.005095273333912094\nFOLD: 1, EPOCH: 16, train_loss: 0.007971109452602617, valid_loss: 0.005109070024142663\nFOLD: 1, EPOCH: 17, train_loss: 0.00793930170509745, valid_loss: 0.004862189215297501\nFOLD: 1, EPOCH: 18, train_loss: 0.007911485510275644, valid_loss: 0.004972186327601473\nFOLD: 1, EPOCH: 19, train_loss: 0.007868231493322289, valid_loss: 0.004863683599978685\nFOLD: 1, EPOCH: 20, train_loss: 0.007822759750792208, valid_loss: 0.004870388889685273\nFOLD: 1, EPOCH: 21, train_loss: 0.007779614876627046, valid_loss: 0.005004631898676355\nFOLD: 1, EPOCH: 22, train_loss: 0.007762550828321015, valid_loss: 0.005195172581200798\nFOLD: 1, EPOCH: 23, train_loss: 0.007725294583531863, valid_loss: 0.004900468047708273\nFOLD: 1, EPOCH: 24, train_loss: 0.0076673115904936015, valid_loss: 0.004923886386677623\nBeginning training for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.41234168355517525, valid_loss: 0.03505957747499148\nFOLD: 1, EPOCH: 1, train_loss: 0.02723506306681563, valid_loss: 0.020158383684853714\nFOLD: 1, EPOCH: 2, train_loss: 0.022476860650760287, valid_loss: 0.019123411426941555\nFOLD: 1, EPOCH: 3, train_loss: 0.021540436181513703, valid_loss: 0.018199530740578968\nFOLD: 1, EPOCH: 4, train_loss: 0.02114041620755897, valid_loss: 0.018461042704681557\nFOLD: 1, EPOCH: 5, train_loss: 0.0209332986132187, valid_loss: 0.01813069824129343\nFOLD: 1, EPOCH: 6, train_loss: 0.02081683836877346, valid_loss: 0.01777209124217431\nFOLD: 1, EPOCH: 7, train_loss: 0.02066942857687964, valid_loss: 0.017791302874684334\nFOLD: 1, EPOCH: 8, train_loss: 0.020573468988432604, valid_loss: 0.017557604238390923\nFOLD: 1, EPOCH: 9, train_loss: 0.0203921653001624, valid_loss: 0.017644584799806278\nFOLD: 1, EPOCH: 10, train_loss: 0.0203453565345091, valid_loss: 0.01767238260557254\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 11, train_loss: 0.02016213465043727, valid_loss: 0.017518723383545876\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 12, train_loss: 0.019939099383704802, valid_loss: 0.017458921298384666\nFOLD: 1, EPOCH: 13, train_loss: 0.019733897903386283, valid_loss: 0.017374439164996147\nFOLD: 1, EPOCH: 14, train_loss: 0.019646086784846643, valid_loss: 0.017395225043098133\nFOLD: 1, EPOCH: 15, train_loss: 0.01957519445568323, valid_loss: 0.01742767325292031\nFOLD: 1, EPOCH: 16, train_loss: 0.019535809645757955, valid_loss: 0.017428248499830563\nFOLD: 1, EPOCH: 17, train_loss: 0.01948200330576476, valid_loss: 0.017586044346292812\nFOLD: 1, EPOCH: 18, train_loss: 0.01946672095971949, valid_loss: 0.017382592894136906\nFOLD: 1, EPOCH: 19, train_loss: 0.019436277558698374, valid_loss: 0.01740419554213683\nFOLD: 1, EPOCH: 20, train_loss: 0.019386831944917932, valid_loss: 0.017291975828508537\nFOLD: 1, EPOCH: 21, train_loss: 0.01934903032858582, valid_loss: 0.01738255036373933\nFOLD: 1, EPOCH: 22, train_loss: 0.0193281971356448, valid_loss: 0.01734702941030264\nFOLD: 1, EPOCH: 23, train_loss: 0.01928954746793298, valid_loss: 0.017439050289491814\nFOLD: 1, EPOCH: 24, train_loss: 0.019256481900811195, valid_loss: 0.017336731776595116\nFOLD: 1, EPOCH: 25, train_loss: 0.019230099778403256, valid_loss: 0.01743446880330642\nFOLD: 1, EPOCH: 26, train_loss: 0.019221684128484306, valid_loss: 0.01725768701483806\nFOLD: 1, EPOCH: 27, train_loss: 0.019161721472354495, valid_loss: 0.01728636243691047\nFOLD: 1, EPOCH: 28, train_loss: 0.01917995167348315, valid_loss: 0.017310555403431255\nFOLD: 1, EPOCH: 29, train_loss: 0.019165868596995577, valid_loss: 0.017286785878241062\nFOLD: 1, EPOCH: 30, train_loss: 0.019135832512641653, valid_loss: 0.017301809042692184\nFOLD: 1, EPOCH: 31, train_loss: 0.01913356698830338, valid_loss: 0.017287102217475574\nFOLD: 1, EPOCH: 32, train_loss: 0.019125478287391803, valid_loss: 0.017284896845618885\nFOLD: 1, EPOCH: 33, train_loss: 0.019120789187795976, valid_loss: 0.017286125260094803\nFOLD: 1, EPOCH: 34, train_loss: 0.01913338034030269, valid_loss: 0.017284965763489406\nBeginning pretraining for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.7193613069898942, valid_loss: 0.6703986326853434\nFOLD: 2, EPOCH: 1, train_loss: 0.34469547177500587, valid_loss: 0.02743788342922926\nFOLD: 2, EPOCH: 2, train_loss: 0.01357242084272644, valid_loss: 0.0058861834307511645\nFOLD: 2, EPOCH: 3, train_loss: 0.008890941109070006, valid_loss: 0.005200517519066731\nFOLD: 2, EPOCH: 4, train_loss: 0.008581301512415795, valid_loss: 0.005148766950393717\nFOLD: 2, EPOCH: 5, train_loss: 0.008462398915606387, valid_loss: 0.0050938804633915424\nFOLD: 2, EPOCH: 6, train_loss: 0.008401887600912768, valid_loss: 0.004800590531279643\nFOLD: 2, EPOCH: 7, train_loss: 0.008365632925073014, valid_loss: 0.00497446721419692\nFOLD: 2, EPOCH: 8, train_loss: 0.008350785821676254, valid_loss: 0.004536560309740405\nFOLD: 2, EPOCH: 9, train_loss: 0.008264439385932158, valid_loss: 0.004481518253063162\nFOLD: 2, EPOCH: 10, train_loss: 0.00824736216215088, valid_loss: 0.005133663071319461\nFOLD: 2, EPOCH: 11, train_loss: 0.008235515273340484, valid_loss: 0.0050199865363538265\nFOLD: 2, EPOCH: 12, train_loss: 0.008201164412586129, valid_loss: 0.004747529242498179\nFOLD: 2, EPOCH: 13, train_loss: 0.00814131463823073, valid_loss: 0.0047460012913992005\nFOLD: 2, EPOCH: 14, train_loss: 0.008108404478715622, valid_loss: 0.004926763397331039\nFOLD: 2, EPOCH: 15, train_loss: 0.008088473956484129, valid_loss: 0.004814521099130313\nFOLD: 2, EPOCH: 16, train_loss: 0.008049198440002167, valid_loss: 0.005014846023793022\nFOLD: 2, EPOCH: 17, train_loss: 0.00801254209021435, valid_loss: 0.0045472329171995325\nFOLD: 2, EPOCH: 18, train_loss: 0.007959408028160824, valid_loss: 0.004825464216992259\nFOLD: 2, EPOCH: 19, train_loss: 0.007949752664631781, valid_loss: 0.0048652528785169125\nFOLD: 2, EPOCH: 20, train_loss: 0.007905709269620916, valid_loss: 0.004713965308231612\nFOLD: 2, EPOCH: 21, train_loss: 0.00788672109517981, valid_loss: 0.004794228511552016\nFOLD: 2, EPOCH: 22, train_loss: 0.007853790515047662, valid_loss: 0.004887821773687999\nFOLD: 2, EPOCH: 23, train_loss: 0.007804023293668733, valid_loss: 0.004894401179626584\nFOLD: 2, EPOCH: 24, train_loss: 0.007763990420190727, valid_loss: 0.004760372219607234\nBeginning training for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.41573272700257163, valid_loss: 0.04536130279302597\nFOLD: 2, EPOCH: 1, train_loss: 0.027461923494496766, valid_loss: 0.022686556292076904\nFOLD: 2, EPOCH: 2, train_loss: 0.022073271191295457, valid_loss: 0.020090671566625435\nFOLD: 2, EPOCH: 3, train_loss: 0.021251974901294008, valid_loss: 0.01874390399704377\nFOLD: 2, EPOCH: 4, train_loss: 0.020987605840405998, valid_loss: 0.018790986699362595\nFOLD: 2, EPOCH: 5, train_loss: 0.02076035091543899, valid_loss: 0.018672516879936058\nFOLD: 2, EPOCH: 6, train_loss: 0.020614464329007792, valid_loss: 0.018533450551331043\nFOLD: 2, EPOCH: 7, train_loss: 0.02047204374171355, valid_loss: 0.01827813157190879\nFOLD: 2, EPOCH: 8, train_loss: 0.020392324501538977, valid_loss: 0.018651975008348625\nFOLD: 2, EPOCH: 9, train_loss: 0.020327806034508872, valid_loss: 0.01861537465204795\nFOLD: 2, EPOCH: 10, train_loss: 0.020196980906321722, valid_loss: 0.018098308704793453\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 11, train_loss: 0.020080797976868993, valid_loss: 0.018222434756656487\nFOLD: 2, EPOCH: 12, train_loss: 0.019817917169455218, valid_loss: 0.018191639023522537\nFOLD: 2, EPOCH: 13, train_loss: 0.01970005314797163, valid_loss: 0.01807795837521553\nFOLD: 2, EPOCH: 14, train_loss: 0.019606678198803875, valid_loss: 0.018147006630897522\nFOLD: 2, EPOCH: 15, train_loss: 0.019579398062299278, valid_loss: 0.018061076911787193\nFOLD: 2, EPOCH: 16, train_loss: 0.019517216338392568, valid_loss: 0.018152011558413506\nFOLD: 2, EPOCH: 17, train_loss: 0.019418263314839673, valid_loss: 0.01804981256524722\nFOLD: 2, EPOCH: 18, train_loss: 0.019388585899244335, valid_loss: 0.018059623738129933\nFOLD: 2, EPOCH: 19, train_loss: 0.01930102552561199, valid_loss: 0.01806228452672561\nFOLD: 2, EPOCH: 20, train_loss: 0.01925564491573502, valid_loss: 0.01795803103595972\nFOLD: 2, EPOCH: 21, train_loss: 0.01919656334554448, valid_loss: 0.01800071820616722\nFOLD: 2, EPOCH: 22, train_loss: 0.019089717856224847, valid_loss: 0.01808255569388469\nFOLD: 2, EPOCH: 23, train_loss: 0.01905687668305986, valid_loss: 0.018024026416242123\nFOLD: 2, EPOCH: 24, train_loss: 0.018993832609232736, valid_loss: 0.017952895412842434\nFOLD: 2, EPOCH: 25, train_loss: 0.018945372444303596, valid_loss: 0.01796734177817901\nFOLD: 2, EPOCH: 26, train_loss: 0.018890035557834542, valid_loss: 0.017934201285243034\nFOLD: 2, EPOCH: 27, train_loss: 0.018825557521160913, valid_loss: 0.01793910718212525\nFOLD: 2, EPOCH: 28, train_loss: 0.01875760855481905, valid_loss: 0.01789894172300895\nFOLD: 2, EPOCH: 29, train_loss: 0.01870380955583909, valid_loss: 0.017930667536954086\nFOLD: 2, EPOCH: 30, train_loss: 0.018645768437315437, valid_loss: 0.017973193898797035\nFOLD: 2, EPOCH: 31, train_loss: 0.018626577987828675, valid_loss: 0.017945187476774056\nFOLD: 2, EPOCH: 32, train_loss: 0.01858535978723975, valid_loss: 0.01794572826474905\nFOLD: 2, EPOCH: 33, train_loss: 0.018581837689613596, valid_loss: 0.0179433086887002\nFOLD: 2, EPOCH: 34, train_loss: 0.018583974801003933, valid_loss: 0.017940253329773743\nBeginning pretraining for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.7196012121789596, valid_loss: 0.6711952288945516\nFOLD: 3, EPOCH: 1, train_loss: 0.34356655969339256, valid_loss: 0.027552878484129906\nFOLD: 3, EPOCH: 2, train_loss: 0.013650169596076012, valid_loss: 0.006053287535905838\nFOLD: 3, EPOCH: 3, train_loss: 0.008842902641524287, valid_loss: 0.005276240175589919\nFOLD: 3, EPOCH: 4, train_loss: 0.008552564143696251, valid_loss: 0.00495022318015496\nFOLD: 3, EPOCH: 5, train_loss: 0.008487298646393944, valid_loss: 0.004819991687933604\nFOLD: 3, EPOCH: 6, train_loss: 0.008404136443620218, valid_loss: 0.0051024677231907845\nFOLD: 3, EPOCH: 7, train_loss: 0.00832406708094127, valid_loss: 0.004703890920306246\nFOLD: 3, EPOCH: 8, train_loss: 0.00827530016848708, valid_loss: 0.004582364888240893\nFOLD: 3, EPOCH: 9, train_loss: 0.008290515486698817, valid_loss: 0.0050206730763117475\nFOLD: 3, EPOCH: 10, train_loss: 0.008329215391045985, valid_loss: 0.004638211956868569\nFOLD: 3, EPOCH: 11, train_loss: 0.008250773815876421, valid_loss: 0.0048890747129917145\nFOLD: 3, EPOCH: 12, train_loss: 0.00815170506179771, valid_loss: 0.0047215428203344345\nFOLD: 3, EPOCH: 13, train_loss: 0.008134665570276626, valid_loss: 0.005149877242123087\nFOLD: 3, EPOCH: 14, train_loss: 0.008105207522235373, valid_loss: 0.00473010097630322\nFOLD: 3, EPOCH: 15, train_loss: 0.008073163835112663, valid_loss: 0.004973252847169836\nFOLD: 3, EPOCH: 16, train_loss: 0.008033240542692296, valid_loss: 0.005113249955077966\nFOLD: 3, EPOCH: 17, train_loss: 0.007995601154535133, valid_loss: 0.00481634361979862\nFOLD: 3, EPOCH: 18, train_loss: 0.007962473736637655, valid_loss: 0.0050158502999693155\nFOLD: 3, EPOCH: 19, train_loss: 0.007931563592351535, valid_loss: 0.004853120461727182\nFOLD: 3, EPOCH: 20, train_loss: 0.00787633890286088, valid_loss: 0.004816265233481924\nFOLD: 3, EPOCH: 21, train_loss: 0.00784267029067611, valid_loss: 0.004933166705692808\nFOLD: 3, EPOCH: 22, train_loss: 0.007798197117688901, valid_loss: 0.004835944933195909\nFOLD: 3, EPOCH: 23, train_loss: 0.007750929045655271, valid_loss: 0.004919249719629685\nFOLD: 3, EPOCH: 24, train_loss: 0.007700414300951011, valid_loss: 0.0048732270176212\nBeginning training for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.4230670646271285, valid_loss: 0.047992248088121414\nFOLD: 3, EPOCH: 1, train_loss: 0.02802298546713941, valid_loss: 0.021135819144546986\nFOLD: 3, EPOCH: 2, train_loss: 0.022288337021189576, valid_loss: 0.018571413432558376\nFOLD: 3, EPOCH: 3, train_loss: 0.021310557392151916, valid_loss: 0.018163415603339672\nFOLD: 3, EPOCH: 4, train_loss: 0.020900031003881905, valid_loss: 0.01791348649809758\nFOLD: 3, EPOCH: 5, train_loss: 0.02075577713549137, valid_loss: 0.017996301564077537\nFOLD: 3, EPOCH: 6, train_loss: 0.0205899153233451, valid_loss: 0.018159881544609863\nFOLD: 3, EPOCH: 7, train_loss: 0.020481205052312684, valid_loss: 0.017723320983350277\nFOLD: 3, EPOCH: 8, train_loss: 0.020372983506497216, valid_loss: 0.01794029710193475\nFOLD: 3, EPOCH: 9, train_loss: 0.020315505016375992, valid_loss: 0.017732184069852035\nFOLD: 3, EPOCH: 10, train_loss: 0.02021483879755525, valid_loss: 0.01775110699236393\nFOLD: 3, EPOCH: 11, train_loss: 0.020148278104470056, valid_loss: 0.017632016601661842\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 12, train_loss: 0.019839273820466855, valid_loss: 0.01734213096400102\nFOLD: 3, EPOCH: 13, train_loss: 0.01971704441615764, valid_loss: 0.01731341953078906\nFOLD: 3, EPOCH: 14, train_loss: 0.019598858485765317, valid_loss: 0.01717600164314111\nFOLD: 3, EPOCH: 15, train_loss: 0.019555132069131908, valid_loss: 0.017172907789548237\nFOLD: 3, EPOCH: 16, train_loss: 0.019463167690178928, valid_loss: 0.017227965717514355\nFOLD: 3, EPOCH: 17, train_loss: 0.019413903574733174, valid_loss: 0.0172574653600653\nFOLD: 3, EPOCH: 18, train_loss: 0.019366239800172692, valid_loss: 0.01728837223102649\nFOLD: 3, EPOCH: 19, train_loss: 0.019299007897429606, valid_loss: 0.017159045673906803\nFOLD: 3, EPOCH: 20, train_loss: 0.019231742138371748, valid_loss: 0.017111168553431828\nFOLD: 3, EPOCH: 21, train_loss: 0.019175761269734186, valid_loss: 0.017170917553206284\nFOLD: 3, EPOCH: 22, train_loss: 0.019097386201953188, valid_loss: 0.017202883027493954\nFOLD: 3, EPOCH: 23, train_loss: 0.019045407829039237, valid_loss: 0.017261843817929428\nFOLD: 3, EPOCH: 24, train_loss: 0.018973246983745518, valid_loss: 0.01701652631163597\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 25, train_loss: 0.018923101563225773, valid_loss: 0.0170025151843826\nFOLD: 3, EPOCH: 26, train_loss: 0.01880808870362885, valid_loss: 0.017074146928886574\nFOLD: 3, EPOCH: 27, train_loss: 0.018751113730318406, valid_loss: 0.017035212367773056\nFOLD: 3, EPOCH: 28, train_loss: 0.018739141742972767, valid_loss: 0.017031524951259296\nFOLD: 3, EPOCH: 29, train_loss: 0.01870746154557256, valid_loss: 0.017034868088861305\nFOLD: 3, EPOCH: 30, train_loss: 0.018697487026014748, valid_loss: 0.017012781153122585\nFOLD: 3, EPOCH: 31, train_loss: 0.01866126065964208, valid_loss: 0.01701406203210354\nFOLD: 3, EPOCH: 32, train_loss: 0.018653995083535418, valid_loss: 0.01701865593592326\nFOLD: 3, EPOCH: 33, train_loss: 0.018636651008444673, valid_loss: 0.017026346487303574\nFOLD: 3, EPOCH: 34, train_loss: 0.018649354522280833, valid_loss: 0.017023232765495777\nBeginning pretraining for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.7188329100608826, valid_loss: 0.6689597368240356\nFOLD: 4, EPOCH: 1, train_loss: 0.33702859413974423, valid_loss: 0.025830261098841827\nFOLD: 4, EPOCH: 2, train_loss: 0.013895523843958098, valid_loss: 0.005942370742559433\nFOLD: 4, EPOCH: 3, train_loss: 0.008860017063424867, valid_loss: 0.005256310881425937\nFOLD: 4, EPOCH: 4, train_loss: 0.008721932522295153, valid_loss: 0.005054702827086051\nFOLD: 4, EPOCH: 5, train_loss: 0.008432087132378537, valid_loss: 0.0047524844606717425\nFOLD: 4, EPOCH: 6, train_loss: 0.008340080515207612, valid_loss: 0.005044319899752736\nFOLD: 4, EPOCH: 7, train_loss: 0.008286539793891065, valid_loss: 0.004940959624946117\nFOLD: 4, EPOCH: 8, train_loss: 0.008244668777265093, valid_loss: 0.005408810296406348\nFOLD: 4, EPOCH: 9, train_loss: 0.00823932742316495, valid_loss: 0.0045901328170051174\nFOLD: 4, EPOCH: 10, train_loss: 0.008187001338228583, valid_loss: 0.00490090452755491\nFOLD: 4, EPOCH: 11, train_loss: 0.008165017605813988, valid_loss: 0.005115147214382887\nFOLD: 4, EPOCH: 12, train_loss: 0.008119076120612375, valid_loss: 0.004722436424344778\nFOLD: 4, EPOCH: 13, train_loss: 0.00805939910659457, valid_loss: 0.005329967321207126\nFOLD: 4, EPOCH: 14, train_loss: 0.008026414468665333, valid_loss: 0.005577402034153541\nFOLD: 4, EPOCH: 15, train_loss: 0.007990903070415644, valid_loss: 0.0050694486902405815\nFOLD: 4, EPOCH: 16, train_loss: 0.007952704286093222, valid_loss: 0.004824048063407342\nFOLD: 4, EPOCH: 17, train_loss: 0.007915677119265585, valid_loss: 0.004905167346199353\nFOLD: 4, EPOCH: 18, train_loss: 0.007875120787716964, valid_loss: 0.004954047656307618\nFOLD: 4, EPOCH: 19, train_loss: 0.007846108569270548, valid_loss: 0.004743640311062336\nFOLD: 4, EPOCH: 20, train_loss: 0.007802588878856862, valid_loss: 0.004790072562173009\nFOLD: 4, EPOCH: 21, train_loss: 0.007751245476195917, valid_loss: 0.0048211422593643265\nFOLD: 4, EPOCH: 22, train_loss: 0.007705213844447452, valid_loss: 0.004788225671897332\nFOLD: 4, EPOCH: 23, train_loss: 0.007664970067494056, valid_loss: 0.004818970337510109\nFOLD: 4, EPOCH: 24, train_loss: 0.007597614367328146, valid_loss: 0.004936009335021178\nBeginning training for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.42405288239174027, valid_loss: 0.0437466191748778\nFOLD: 4, EPOCH: 1, train_loss: 0.027644129743909136, valid_loss: 0.02069527345399062\nFOLD: 4, EPOCH: 2, train_loss: 0.022039611276020023, valid_loss: 0.019276475223402183\nFOLD: 4, EPOCH: 3, train_loss: 0.02123930409331532, valid_loss: 0.01840633371224006\nFOLD: 4, EPOCH: 4, train_loss: 0.020894651082070434, valid_loss: 0.01832945893208186\nFOLD: 4, EPOCH: 5, train_loss: 0.020683371516711572, valid_loss: 0.01808918019135793\nFOLD: 4, EPOCH: 6, train_loss: 0.020543068866519368, valid_loss: 0.018123346380889416\nFOLD: 4, EPOCH: 7, train_loss: 0.020445872974746367, valid_loss: 0.018053844881554443\nFOLD: 4, EPOCH: 8, train_loss: 0.020339942153762367, valid_loss: 0.017847302680214245\nFOLD: 4, EPOCH: 9, train_loss: 0.020225308013751227, valid_loss: 0.017970124259591103\nFOLD: 4, EPOCH: 10, train_loss: 0.02016317690996563, valid_loss: 0.017880975579222042\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 11, train_loss: 0.020094191819867668, valid_loss: 0.01788312227775653\nFOLD: 4, EPOCH: 12, train_loss: 0.01980447999256499, valid_loss: 0.01772443825999896\nFOLD: 4, EPOCH: 13, train_loss: 0.019679309526348814, valid_loss: 0.01778639356295268\nFOLD: 4, EPOCH: 14, train_loss: 0.019617780659566906, valid_loss: 0.017746596907575924\nFOLD: 4, EPOCH: 15, train_loss: 0.01954000580179341, valid_loss: 0.017896125713984173\nFOLD: 4, EPOCH: 16, train_loss: 0.019491868641446617, valid_loss: 0.017727491135398548\nFOLD: 4, EPOCH: 17, train_loss: 0.019417486591812444, valid_loss: 0.017725179592768352\nFOLD: 4, EPOCH: 18, train_loss: 0.019333945313359007, valid_loss: 0.017675006451706093\nFOLD: 4, EPOCH: 19, train_loss: 0.019296280272743282, valid_loss: 0.01781286175052325\nFOLD: 4, EPOCH: 20, train_loss: 0.019244817688184625, valid_loss: 0.017870507203042507\nFOLD: 4, EPOCH: 21, train_loss: 0.019207955776330304, valid_loss: 0.01753799555202325\nFOLD: 4, EPOCH: 22, train_loss: 0.01912979655625189, valid_loss: 0.017629997804760933\nFOLD: 4, EPOCH: 23, train_loss: 0.019061717369100627, valid_loss: 0.01758406311273575\nFOLD: 4, EPOCH: 24, train_loss: 0.01899470476543202, valid_loss: 0.017567420999209087\nFOLD: 4, EPOCH: 25, train_loss: 0.01892469620660824, valid_loss: 0.017559572433431942\nFOLD: 4, EPOCH: 26, train_loss: 0.018864859991213855, valid_loss: 0.017514937557280064\nFOLD: 4, EPOCH: 27, train_loss: 0.018800900810781646, valid_loss: 0.017520193941891193\nFOLD: 4, EPOCH: 28, train_loss: 0.01876342373297495, valid_loss: 0.01751813292503357\nFOLD: 4, EPOCH: 29, train_loss: 0.01868781211840756, valid_loss: 0.017502305718759697\nFOLD: 4, EPOCH: 30, train_loss: 0.018642268527080032, valid_loss: 0.01748606034864982\nFOLD: 4, EPOCH: 31, train_loss: 0.018609893672606525, valid_loss: 0.017495668803652126\nFOLD: 4, EPOCH: 32, train_loss: 0.018566137344083366, valid_loss: 0.017498154814044636\nFOLD: 4, EPOCH: 33, train_loss: 0.01856205196065061, valid_loss: 0.017491163685917854\nFOLD: 4, EPOCH: 34, train_loss: 0.018535741933566684, valid_loss: 0.017487366994222004\nBeginning pretraining for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.7194406004513011, valid_loss: 0.6713972687721252\nFOLD: 5, EPOCH: 1, train_loss: 0.3428971673197606, valid_loss: 0.026117283540467422\nFOLD: 5, EPOCH: 2, train_loss: 0.013554252273238757, valid_loss: 0.005929506771887342\nFOLD: 5, EPOCH: 3, train_loss: 0.008850815979873432, valid_loss: 0.0053968101274222136\nFOLD: 5, EPOCH: 4, train_loss: 0.008491916801123059, valid_loss: 0.004934026937310894\nFOLD: 5, EPOCH: 5, train_loss: 0.00832969359779621, valid_loss: 0.004686401225626469\nFOLD: 5, EPOCH: 6, train_loss: 0.008275553899104981, valid_loss: 0.004731907742097974\nFOLD: 5, EPOCH: 7, train_loss: 0.00821750022142249, valid_loss: 0.004669121311356624\nFOLD: 5, EPOCH: 8, train_loss: 0.008205731577404281, valid_loss: 0.005258256647114952\nFOLD: 5, EPOCH: 9, train_loss: 0.008152396075756234, valid_loss: 0.004781695626055201\nFOLD: 5, EPOCH: 10, train_loss: 0.008128330905866973, valid_loss: 0.005482011552279194\nFOLD: 5, EPOCH: 11, train_loss: 0.008083913790281205, valid_loss: 0.005064663244411349\nFOLD: 5, EPOCH: 12, train_loss: 0.008081612382631968, valid_loss: 0.005330043068776528\nFOLD: 5, EPOCH: 13, train_loss: 0.008008078206330538, valid_loss: 0.0050098958890885115\nFOLD: 5, EPOCH: 14, train_loss: 0.007977396523689522, valid_loss: 0.004942587033535044\nFOLD: 5, EPOCH: 15, train_loss: 0.007957583211143227, valid_loss: 0.005155379340673487\nFOLD: 5, EPOCH: 16, train_loss: 0.007921597495784654, valid_loss: 0.004793692768240969\nFOLD: 5, EPOCH: 17, train_loss: 0.007880949374178754, valid_loss: 0.004739456344395876\nFOLD: 5, EPOCH: 18, train_loss: 0.007867130122202285, valid_loss: 0.004902709508314729\nFOLD: 5, EPOCH: 19, train_loss: 0.007831247632994372, valid_loss: 0.005055457431202133\nFOLD: 5, EPOCH: 20, train_loss: 0.007799761178081527, valid_loss: 0.004946520552039146\nFOLD: 5, EPOCH: 21, train_loss: 0.007768592632868711, valid_loss: 0.004834521018589537\nFOLD: 5, EPOCH: 22, train_loss: 0.007742790875079877, valid_loss: 0.004906154470518231\nFOLD: 5, EPOCH: 23, train_loss: 0.007696223664371406, valid_loss: 0.004907331507032116\nFOLD: 5, EPOCH: 24, train_loss: 0.007661543676958364, valid_loss: 0.004701010417193174\nBeginning training for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.4257559035630787, valid_loss: 0.046486892427007355\nFOLD: 5, EPOCH: 1, train_loss: 0.027941224274828154, valid_loss: 0.02136596105992794\nFOLD: 5, EPOCH: 2, train_loss: 0.022465534617795664, valid_loss: 0.01938848476856947\nFOLD: 5, EPOCH: 3, train_loss: 0.02130075401681311, valid_loss: 0.019702086535592873\nFOLD: 5, EPOCH: 4, train_loss: 0.021014001987436238, valid_loss: 0.018426699563860893\nFOLD: 5, EPOCH: 5, train_loss: 0.020830743617432958, valid_loss: 0.018368260314067204\nFOLD: 5, EPOCH: 6, train_loss: 0.020610736135174248, valid_loss: 0.018740088678896427\nFOLD: 5, EPOCH: 7, train_loss: 0.02051125028554131, valid_loss: 0.018504693172872066\nFOLD: 5, EPOCH: 8, train_loss: 0.02035302225062076, valid_loss: 0.01806042653818925\nFOLD: 5, EPOCH: 9, train_loss: 0.020287489408955854, valid_loss: 0.01821728392193715\nFOLD: 5, EPOCH: 10, train_loss: 0.020167611375012818, valid_loss: 0.017939367952446144\nFOLD: 5, EPOCH: 11, train_loss: 0.02009408301947748, valid_loss: 0.017997497382263344\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 12, train_loss: 0.01999559609548134, valid_loss: 0.01781228557229042\nFOLD: 5, EPOCH: 13, train_loss: 0.01968292459188139, valid_loss: 0.018009507718185585\nFOLD: 5, EPOCH: 14, train_loss: 0.019602700191385606, valid_loss: 0.017809458697835606\nFOLD: 5, EPOCH: 15, train_loss: 0.01950785643695032, valid_loss: 0.017959170043468475\nFOLD: 5, EPOCH: 16, train_loss: 0.019431233515634257, valid_loss: 0.017896616831421852\nFOLD: 5, EPOCH: 17, train_loss: 0.019368465825477067, valid_loss: 0.017892591655254364\nFOLD: 5, EPOCH: 18, train_loss: 0.019327092039234498, valid_loss: 0.017842537723481655\nFOLD: 5, EPOCH: 19, train_loss: 0.01926082049441688, valid_loss: 0.017850234794119995\nFOLD: 5, EPOCH: 20, train_loss: 0.019170074559309903, valid_loss: 0.017821446371575195\nFOLD: 5, EPOCH: 21, train_loss: 0.01913797696504523, valid_loss: 0.017891109300156433\nFOLD: 5, EPOCH: 22, train_loss: 0.019025312955765164, valid_loss: 0.017882614706953365\nFOLD: 5, EPOCH: 23, train_loss: 0.01897683980710366, valid_loss: 0.017856370968123276\nFOLD: 5, EPOCH: 24, train_loss: 0.018884484163101983, valid_loss: 0.01773748950411876\nFOLD: 5, EPOCH: 25, train_loss: 0.01881585412603967, valid_loss: 0.017838747551043827\nFOLD: 5, EPOCH: 26, train_loss: 0.01878413640181808, valid_loss: 0.017748257455726463\nFOLD: 5, EPOCH: 27, train_loss: 0.01870995809269302, valid_loss: 0.017749592351416748\nFOLD: 5, EPOCH: 28, train_loss: 0.018611194730243263, valid_loss: 0.017740515060722828\nFOLD: 5, EPOCH: 29, train_loss: 0.018553923158084646, valid_loss: 0.017751726011435192\nFOLD: 5, EPOCH: 30, train_loss: 0.01852423379964688, valid_loss: 0.017715817938248318\nFOLD: 5, EPOCH: 31, train_loss: 0.01849421754698543, valid_loss: 0.01772197987884283\nFOLD: 5, EPOCH: 32, train_loss: 0.01844770718804177, valid_loss: 0.017711966608961422\nFOLD: 5, EPOCH: 33, train_loss: 0.018401492770542118, valid_loss: 0.01770452472070853\nFOLD: 5, EPOCH: 34, train_loss: 0.018402712419629097, valid_loss: 0.017702213488519192\nBeginning pretraining for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.7189408347887152, valid_loss: 0.6685850620269775\nFOLD: 6, EPOCH: 1, train_loss: 0.3396610869642566, valid_loss: 0.025675859302282333\nFOLD: 6, EPOCH: 2, train_loss: 0.013353823410237537, valid_loss: 0.005792873678728938\nFOLD: 6, EPOCH: 3, train_loss: 0.008752185413066079, valid_loss: 0.005184886433805029\nFOLD: 6, EPOCH: 4, train_loss: 0.008462513345019781, valid_loss: 0.005007983262961109\nFOLD: 6, EPOCH: 5, train_loss: 0.02873690949533792, valid_loss: 0.00543009831259648\nFOLD: 6, EPOCH: 6, train_loss: 0.00840800000792917, valid_loss: 0.004753505888705452\nFOLD: 6, EPOCH: 7, train_loss: 0.00822159493177691, valid_loss: 0.004579172935336828\nFOLD: 6, EPOCH: 8, train_loss: 0.008176630650482634, valid_loss: 0.004579398004959027\nFOLD: 6, EPOCH: 9, train_loss: 0.008158427745322971, valid_loss: 0.004583243435869615\nFOLD: 6, EPOCH: 10, train_loss: 0.008174598860718748, valid_loss: 0.004511405170584719\nFOLD: 6, EPOCH: 11, train_loss: 0.00813113639185972, valid_loss: 0.004547291047250231\nFOLD: 6, EPOCH: 12, train_loss: 0.008120360647273414, valid_loss: 0.0045485596638172865\nFOLD: 6, EPOCH: 13, train_loss: 0.008103093283031793, valid_loss: 0.004560257075354457\nFOLD: 6, EPOCH: 14, train_loss: 0.008071243406876046, valid_loss: 0.00451096174462388\nFOLD: 6, EPOCH: 15, train_loss: 0.008047182496417971, valid_loss: 0.004605166652860741\nFOLD: 6, EPOCH: 16, train_loss: 0.00799791279303677, valid_loss: 0.004508584039285779\nFOLD: 6, EPOCH: 17, train_loss: 0.007954604855245528, valid_loss: 0.0045153150567784905\nFOLD: 6, EPOCH: 18, train_loss: 0.00791651280322934, valid_loss: 0.004725398185352485\nFOLD: 6, EPOCH: 19, train_loss: 0.007897579015287407, valid_loss: 0.004678990924730897\nFOLD: 6, EPOCH: 20, train_loss: 0.007833427585223141, valid_loss: 0.0047121476382017136\nFOLD: 6, EPOCH: 21, train_loss: 0.007792041678091183, valid_loss: 0.004573320038616657\nFOLD: 6, EPOCH: 22, train_loss: 0.00772622087970376, valid_loss: 0.00456037597420315\nFOLD: 6, EPOCH: 23, train_loss: 0.0076468151665347464, valid_loss: 0.004649149331574638\nFOLD: 6, EPOCH: 24, train_loss: 0.007584504180533045, valid_loss: 0.004645305530478557\nBeginning training for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.39656502545318184, valid_loss: 0.030409990499416988\nFOLD: 6, EPOCH: 1, train_loss: 0.02598824894384426, valid_loss: 0.02164360291014115\nFOLD: 6, EPOCH: 2, train_loss: 0.02247385395800366, valid_loss: 0.01857869451244672\nFOLD: 6, EPOCH: 3, train_loss: 0.021640679415534523, valid_loss: 0.018039956378440063\nFOLD: 6, EPOCH: 4, train_loss: 0.02111513545627103, valid_loss: 0.018131397664546967\nFOLD: 6, EPOCH: 5, train_loss: 0.02097012140952489, valid_loss: 0.017854369866351288\nFOLD: 6, EPOCH: 6, train_loss: 0.020846233538845006, valid_loss: 0.017720714211463928\nFOLD: 6, EPOCH: 7, train_loss: 0.02071835659444332, valid_loss: 0.017727539874613285\nFOLD: 6, EPOCH: 8, train_loss: 0.02062844961662503, valid_loss: 0.017568751859168213\nFOLD: 6, EPOCH: 9, train_loss: 0.020524795276715475, valid_loss: 0.017851070500910282\nFOLD: 6, EPOCH: 10, train_loss: 0.020471908864291274, valid_loss: 0.017497568391263485\nFOLD: 6, EPOCH: 11, train_loss: 0.020382875686182696, valid_loss: 0.017523500757912796\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 12, train_loss: 0.020350845475845477, valid_loss: 0.01722828981777032\nFOLD: 6, EPOCH: 13, train_loss: 0.020043237189597943, valid_loss: 0.017301644198596478\nFOLD: 6, EPOCH: 14, train_loss: 0.019928563167067134, valid_loss: 0.01740607184668382\nFOLD: 6, EPOCH: 15, train_loss: 0.019863149205989698, valid_loss: 0.017269872439404328\nFOLD: 6, EPOCH: 16, train_loss: 0.019759889199014974, valid_loss: 0.017207680580516655\nFOLD: 6, EPOCH: 17, train_loss: 0.01974577966200955, valid_loss: 0.0171776885787646\nFOLD: 6, EPOCH: 18, train_loss: 0.019676099038299394, valid_loss: 0.017079454225798447\nFOLD: 6, EPOCH: 19, train_loss: 0.019592530148870805, valid_loss: 0.017075244647761185\nFOLD: 6, EPOCH: 20, train_loss: 0.019554640166461468, valid_loss: 0.01711593909809987\nFOLD: 6, EPOCH: 21, train_loss: 0.019525341360884553, valid_loss: 0.01700872213890155\nFOLD: 6, EPOCH: 22, train_loss: 0.019429900793029982, valid_loss: 0.01712282405545314\nFOLD: 6, EPOCH: 23, train_loss: 0.019373273696093, valid_loss: 0.01700851662705342\nFOLD: 6, EPOCH: 24, train_loss: 0.019325283465578276, valid_loss: 0.017054263812800247\nFOLD: 6, EPOCH: 25, train_loss: 0.019281987484325382, valid_loss: 0.016999791376292706\nFOLD: 6, EPOCH: 26, train_loss: 0.019195003106313592, valid_loss: 0.016959151563545067\nFOLD: 6, EPOCH: 27, train_loss: 0.01914309397997225, valid_loss: 0.016976487822830677\nFOLD: 6, EPOCH: 28, train_loss: 0.019123199943672207, valid_loss: 0.01695841768135627\nFOLD: 6, EPOCH: 29, train_loss: 0.019082635148044896, valid_loss: 0.01694876669595639\nFOLD: 6, EPOCH: 30, train_loss: 0.01897680343073957, valid_loss: 0.016882925604780514\nFOLD: 6, EPOCH: 31, train_loss: 0.018941778908757603, valid_loss: 0.016921544447541237\nFOLD: 6, EPOCH: 32, train_loss: 0.018925710800377762, valid_loss: 0.016927067190408707\nFOLD: 6, EPOCH: 33, train_loss: 0.018896307741456172, valid_loss: 0.016927836773296196\nFOLD: 6, EPOCH: 34, train_loss: 0.01892668233417413, valid_loss: 0.016924547652403515\n"
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "SEED = [0,1,2,3,4,5,6] #<-- Update\n",
    "#SEED = [0]\n",
    "oof = np.zeros((len(train), len(target_scored_cols)))\n",
    "predictions = np.zeros((len(test), len(target_scored_cols)))\n",
    "\n",
    "# mean_scored = np.mean(train[target_scored_cols].values,axis=0)\n",
    "# mean_nscored = np.mean(train[target_nscored_cols].values,axis=0)\n",
    "# pos_scored_rate = np.log(np.where(mean_scored==0, 1e-8, mean_scored))\n",
    "# pos_nscored_rate = np.log(np.where(mean_nscored==0, 1e-8, mean_nscored))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_scored_cols] = oof\n",
    "test[target_scored_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV log_loss:  0.01576428037781396\n"
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_scored_cols).merge(train[['sig_id']+target_scored_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "y_true = train_targets_scored[target_scored_cols].values\n",
    "y_pred = valid_results[target_scored_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_scored_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target_scored.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6478278870126901\n"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline CV log_loss:  0.01622662672176626 - 0.6261338781888004\n",
    "\n",
    "#preepoch 25, epoch 35 CV log_loss:  0.01618806192622041 - 0.6330132853505958\n",
    "#preepoch 15, epoch 35 CV log_loss:  0.01619329827462154 - 0.6293014385190406\n",
    "\n",
    "#preepoch 25, epoch 35 CV log_loss:  0.01605941986493062 - 0.6458518945990771\n",
    "\n",
    "# hidden_sizes = [1100,1000,1000]\n",
    "# dropout_rates = [0.2619422201258426,0.2619422201258426]\n",
    "#preepoch 25, epoch 35 CV log_loss:  0.016014125974596623 - 0.6518603281029931\n",
    "\n",
    "# hidden_sizes = [1200,1000,1000]\n",
    "# dropout_rates = [0.2619422201258426,0.2619422201258426]\n",
    "#preepoch 25, epoch 35 CV log_loss:  0.016027521551404262 - 0.6514822673689723\n",
    "\n",
    "\n",
    "# hidden_sizes = [1100,1000,1000]\n",
    "# dropout_rates = [0.2619422201258426,0.2619422201258426]\n",
    "#batch_size 256\n",
    "#CV log_loss:  0.015968929447358168 - 0.6539676986145084\n",
    "\n",
    "\n",
    "# hidden_sizes = [1100,1000,1000]\n",
    "# dropout_rates = [0.2619422201258426,0.2619422201258426]\n",
    "#batch_size 512\n",
    "#CV log_loss:  0.01592896796356105 - 0.6531892020245663\n",
    "\n",
    "# hidden_sizes = [1100,900,1000]\n",
    "# dropout_rates = [0.2619422201258426,0.2619422201258426]\n",
    "#batch_size 512\n",
    "#CV log_loss:  0.015922683115357255 - 0.6463843503736969\n",
    "\n",
    "# hidden_sizes = [1100,900]\n",
    "# dropout_rates = [0.2619422201258426,0.25]\n",
    "#CV log_loss:  0.015909999946531972 - 0.6516116485729425\n",
    "\n",
    "\n",
    "# hidden_sizes = [1100,900]\n",
    "# dropout_rates = [0.25,0.25]\n",
    "#CV log_loss:  0.01589648288882382 - 0.6508185055047814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"model_1bis_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}