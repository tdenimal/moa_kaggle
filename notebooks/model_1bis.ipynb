{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1605127178604",
   "display_name": "Python 3.7.9 64-bit ('moa_kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['train_targets_scored.csv',\n 'sample_submission.csv',\n '.gitkeep',\n 'train_drug.csv',\n 'train_features.csv',\n 'test_features.csv',\n 'train_targets_nonscored.csv']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "data_dir = '../data/01_raw'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "no_ctl = True\n",
    "ncompo_genes = 600\n",
    "ncompo_cells = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(data_dir+'/train_features.csv')\n",
    "train_targets_scored = pd.read_csv(data_dir+'/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(data_dir+'/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv(data_dir+'/test_features.csv')\n",
    "sample_submission = pd.read_csv(data_dir+'/sample_submission.csv')\n",
    "drug = pd.read_csv(data_dir+'/train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_scored = train_targets_scored.columns[1:]\n",
    "scored = train_targets_scored.merge(drug, on='sig_id', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "not_ctl\n"
    }
   ],
   "source": [
    "if no_ctl:\n",
    "    # cp_type == ctl_vehicle\n",
    "    print(\"not_ctl\")\n",
    "    train_features = train_features[train_features[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
    "    test_features = test_features[test_features[\"cp_type\"] != \"ctl_vehicle\"].reset_index(drop=True)\n",
    "    train_targets_scored = train_targets_scored.iloc[train_features.index]\n",
    "    train_targets_nonscored = train_targets_nonscored.iloc[train_features.index]\n",
    "    train_features.reset_index(drop = True, inplace = True)\n",
    "    train_features.reset_index(drop = True, inplace = True)\n",
    "    train_targets_scored.reset_index(drop = True, inplace = True)\n",
    "    train_targets_nonscored.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Indiquer si valeur dans le range max, min\n",
    "\n",
    "# import seaborn as sns\n",
    "# data = pd.concat([train_features,test_features],axis=0)\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"ctl_vehicle\"][\"c-4\"],label=\"normal\")\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"trt_cp\"][\"c-4\"],label=\"treated\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets = train_targets_scored[[c for c in train_targets_scored.columns if (c != \"sig_id\")]].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_inhibitor\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_activator\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_agonist\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_antagonist\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# train_features[GENES].apply(lambda x : stats.moment(x,moment=5),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True gauss rank\n",
    "import cupy as cp\n",
    "from cupyx.scipy.special import erfinv\n",
    "epsilon = 1e-6\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES + CELLS]), pd.DataFrame(test_features[GENES + CELLS])])\n",
    "\n",
    "for k in (GENES + CELLS):\n",
    "    r_gpu = cp.array(data.loc[:,k])\n",
    "    r_gpu = r_gpu.argsort().argsort()\n",
    "    r_gpu = (r_gpu/r_gpu.max()-0.5)*2 \n",
    "    r_gpu = cp.clip(r_gpu,-1+epsilon,1-epsilon)\n",
    "    r_gpu = erfinv(r_gpu) \n",
    "    data.loc[:,k] = cp.asnumpy( r_gpu * np.sqrt(2) )\n",
    "\n",
    "train_features[GENES + CELLS] = data[:train_features.shape[0]]; test_features[GENES + CELLS] = data[-test_features.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RankGauss\n",
    "\n",
    "# for col in (GENES + CELLS):\n",
    "#     transformer = QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n",
    "#     vec_len = len(train_features[col].values)\n",
    "#     vec_len_test = len(test_features[col].values)\n",
    "#     raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "#     transformer.fit(raw_vec)\n",
    "\n",
    "#     train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "#     test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENES\n",
    "n_comp = ncompo_genes \n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELLS\n",
    "n_comp = ncompo_cells\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(21948, 1526)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = GENES\n",
    "    features_c = CELLS\n",
    "    \n",
    "    for df in train, test:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1) ## <==\n",
    "        # df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1) ## <==\n",
    "        # df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        #df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        # df['g_q25'] = df[features_g].quantile(q=.25,axis = 1)\n",
    "        # df['g_q50'] = df[features_g].quantile(q=.5,axis = 1)\n",
    "        # df['g_q75'] = df[features_g].quantile(q=.75,axis = 1)\n",
    "        #df['g_var'] = df[features_g].apply(axis=1,func=stats.variation)\n",
    "        # df['g_mad'] = df[features_g].mad(axis = 1)\n",
    "\n",
    "\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1) ## <==\n",
    "        # df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1) ## <==\n",
    "        # df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        #df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        # df['c_q25'] = df[features_c].quantile(q=.25,axis = 1)\n",
    "        # df['c_q50'] = df[features_c].quantile(q=.5,axis = 1)\n",
    "        # df['c_q75'] = df[features_c].quantile(q=.75,axis = 1)\n",
    "        # df['c_var'] = df[features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['c_mad'] = df[features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1) ## <==\n",
    "        # df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        # df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        # df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        # df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        # df['gc_q25'] = df[features_g + features_c].quantile(q=.25,axis = 1)\n",
    "        # df['gc_q50'] = df[features_g + features_c].quantile(q=.5,axis = 1)\n",
    "        # df['gc_q75'] = df[features_g + features_c].quantile(q=.75,axis = 1)\n",
    "        # df['gc_var'] = df[features_g + features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['gc_mad'] = df[features_g + features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "train_features,test_features=fe_stats(train_features,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features_gc = train_features[[\"sig_id\"]+GENES+CELLS].copy()\n",
    "# test_features_gc = test_features[[\"sig_id\"]+GENES+CELLS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(21948, 1049)"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "var_thresh = VarianceThreshold(0.8)  #<-- Update\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# def fe_cluster2(train, test, n_clusters = 3, SEED = 42):\n",
    "    \n",
    "\n",
    "#     def create_cluster(train, test, n_clusters = n_clusters):\n",
    "#         train_ = train.copy()\n",
    "#         test_ = test.copy()\n",
    "#         data = pd.concat([train_, test_], axis = 0)\n",
    "#         kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data[[c for c in data.columns if c not in [\"sig_id\",\"cp_type\",\"cp_dose\",\"cp_time\"]]])\n",
    "#         train['cluster'] = kmeans.labels_[:train.shape[0]]\n",
    "#         test['cluster'] = kmeans.labels_[train.shape[0]:]\n",
    "#         train = pd.get_dummies(train, columns = ['cluster'])\n",
    "#         test = pd.get_dummies(test, columns = ['cluster'])\n",
    "#         return train, test\n",
    "    \n",
    "#     train, test = create_cluster(train, test, n_clusters = n_clusters)\n",
    "#     return train, test\n",
    "\n",
    "\n",
    "\n",
    "# train_features,test_features=fe_cluster2(train_features,test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([train1, test1], axis = 0)\n",
    "\n",
    "# distortion = []\n",
    "# for k in range(1,10):\n",
    "#     kmeans = KMeans(n_clusters = k, random_state = 42).fit(data)\n",
    "#     distortion += [kmeans.inertia_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(1,10),distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = train_features.merge(train_features_gc.loc[:,[col for col in train_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")\n",
    "# test_features = test_features.merge(test_features_gc.loc[:,[col for col in test_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_targets_nonscored, on='sig_id')\n",
    "# train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "# test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target_scored = train[train_targets_scored.columns]\n",
    "target_nscored = train[train_targets_nonscored.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test_features.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scored_cols = target_scored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "target_nscored_cols = target_nscored.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds = train.copy()\n",
    "\n",
    "# mskf = MultilabelStratifiedKFold(n_splits=7)\n",
    "\n",
    "# for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target_scored)):\n",
    "#     folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "# folds['kfold'] = folds['kfold'].astype(int)\n",
    "# folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float),          \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, \n",
    "                 num_targets, \n",
    "                 hidden_sizes,\n",
    "                 dropout_rates):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_sizes[0]))\n",
    "        self.activation1 = torch.nn.PReLU(num_parameters = hidden_sizes[0], init = 1.0)\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_sizes[0])\n",
    "        self.dropout2 = nn.Dropout(dropout_rates[0])\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_sizes[0], hidden_sizes[1]))\n",
    "        self.activation2 = torch.nn.PReLU(num_parameters = hidden_sizes[1], init = 1.0)\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_sizes[1])\n",
    "        self.dropout3 = nn.Dropout(dropout_rates[1])\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_sizes[1], num_targets))\n",
    "\n",
    "    def init_bias(self,pos_scored_rate,pos_nscored_rate):\n",
    "        self.dense3.bias.data = nn.Parameter(torch.tensor(pos_scored_rate, dtype=torch.float))\n",
    "    \n",
    "    def recalibrate_layer(self, layer):\n",
    "        if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "            print ('recalibrate layer.weight_v')\n",
    "            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "        if(torch.isnan(layer.weight).sum() > 0):\n",
    "            print ('recalibrate layer.weight')\n",
    "            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "            layer.weight += 1e-7\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        self.recalibrate_layer(self.dense1)\n",
    "        x = self.activation1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        self.recalibrate_layer(self.dense2)\n",
    "        x = self.activation2(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        self.recalibrate_layer(self.dense3)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PREEPOCHS = 25\n",
    "EPOCHS = 60\n",
    "#EPOCHS = 300 #200\n",
    "PATIENCE=40\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7           \n",
    "EARLY_STOPPING_STEPS = PATIENCE+5\n",
    "EARLY_STOP = False\n",
    "\n",
    "#hidden_size=1500\n",
    "hidden_sizes = [1300,900]\n",
    "dropout_rates = [0.25,0.25]\n",
    "#dropout_rate = 0.2619422201258426\n",
    "#dropout_rate = 0.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCATE DRUGS\n",
    "vc = scored.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc<=18].index.sort_values()\n",
    "vc2 = vc.loc[vc>18].index.sort_values()\n",
    "\n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}; dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = scored.groupby('drug_id')[targets_scored].mean().loc[vc1]\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_scored])):\n",
    "    dd = {k:fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "\n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_scored])):\n",
    "    dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "# ASSIGN FOLDS\n",
    "folds = train.merge(drug,on=\"sig_id\")\n",
    "folds['fold'] = folds.drug_id.map(dct1)\n",
    "folds.loc[folds.fold.isna(),'fold'] =\\\n",
    "    folds.loc[folds.fold.isna(),'sig_id'].map(dct2)\n",
    "folds.fold = folds.fold.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1050"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "feature_cols = [c for c in process_data(train).columns if c not in (target_scored_cols + target_nscored_cols)]\n",
    "feature_cols = [c for c in feature_cols if c not in ['fold','sig_id']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=len(feature_cols)\n",
    "num_targets_scored=len(target_scored_cols)\n",
    "num_targets_nscored=len(target_nscored_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(20228, 1656)\n(20228, 1658)\n(3624, 1048)\n(20228, 207)\n(20228, 403)\n(3982, 207)\n"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target_scored.shape)\n",
    "print(target_nscored.shape)\n",
    "print(sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, seed, preTrain=True):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "\n",
    "    \n",
    "    trn_idx = train[train['fold'] != fold].index\n",
    "    val_idx = train[train['fold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['fold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    x_train, y_scored_train  = train_df[feature_cols].values, train_df[target_scored_cols].values\n",
    "    x_valid, y_scored_valid  =  valid_df[feature_cols].values, valid_df[target_scored_cols].values\n",
    "\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "\n",
    "\n",
    "    \n",
    "    if preTrain:\n",
    "        print(f\"Beginning pretraining for fold {fold}\")\n",
    "        y_nscored_train = train_df[target_nscored_cols].values\n",
    "        y_nscored_valid = valid_df[target_nscored_cols].values\n",
    "\n",
    "        train_dataset = TrainDataset(x_train, y_nscored_train)\n",
    "        valid_dataset = TrainDataset(x_valid, y_nscored_train)\n",
    "\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        num_targets=len(target_nscored_cols)\n",
    "\n",
    "\n",
    "\n",
    "        model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "        )\n",
    "\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,pct_start=0.1, div_factor=1e4, final_div_factor=1e5,\n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "\n",
    "\n",
    "        early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "        early_step = 0\n",
    "        best_loss = np.inf\n",
    "    \n",
    "        #Main pretrain loop\n",
    "        for epoch in range(PREEPOCHS):\n",
    "        \n",
    "            train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "            print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "            #scheduler.step(valid_loss)\n",
    "        \n",
    "            if valid_loss < best_loss:\n",
    "            \n",
    "                best_loss = valid_loss\n",
    "                torch.save(model.state_dict(), f\"preFOLD{fold}_.pth\")\n",
    "        \n",
    "            elif(EARLY_STOP == True):\n",
    "            \n",
    "                early_step += 1\n",
    "                if (early_step >= early_stopping_steps):\n",
    "                    break\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    if preTrain:\n",
    "        model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=len(target_nscored_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(torch.load(f\"preFOLD{fold}_.pth\"))\n",
    "        model.dense3 = nn.utils.weight_norm(nn.Linear(hidden_sizes[1], len(target_scored_cols)))\n",
    "    else:\n",
    "        model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=len(target_scored_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "        )\n",
    "\n",
    "\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,pct_start=0.1, div_factor=1e4, final_div_factor=1e5,\n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "\n",
    "    train_dataset = TrainDataset(x_train, y_scored_train)\n",
    "    valid_dataset = TrainDataset(x_valid, y_scored_valid)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "   \n",
    "    oof = np.zeros((len(train), target_scored.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    #Main train loop\n",
    "    print(f\"Beginning training for fold {fold}\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "        #scheduler.step(valid_loss)\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=len(target_scored_cols),\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target_scored.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_scored_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_scored_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Beginning pretraining for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.7292999800513772, valid_loss: 0.6968241035938263\nFOLD: 0, EPOCH: 1, train_loss: 0.6139652781626758, valid_loss: 0.34454473356405896\nFOLD: 0, EPOCH: 2, train_loss: 0.11343786602511126, valid_loss: 0.014605309658994278\nFOLD: 0, EPOCH: 3, train_loss: 0.012103462591767311, valid_loss: 0.006552839108432333\nFOLD: 0, EPOCH: 4, train_loss: 0.00917671637280899, valid_loss: 0.005604881172378858\nFOLD: 0, EPOCH: 5, train_loss: 0.0086847870501087, valid_loss: 0.005171009572222829\nFOLD: 0, EPOCH: 6, train_loss: 0.00851425706573269, valid_loss: 0.005013505617777507\nFOLD: 0, EPOCH: 7, train_loss: 0.008418362204204588, valid_loss: 0.00491558985474209\nFOLD: 0, EPOCH: 8, train_loss: 0.008398253306308213, valid_loss: 0.0048799031258871155\nFOLD: 0, EPOCH: 9, train_loss: 0.008338926524362144, valid_loss: 0.004842489026486874\nFOLD: 0, EPOCH: 10, train_loss: 0.008281434940941194, valid_loss: 0.00497948417129616\nFOLD: 0, EPOCH: 11, train_loss: 0.008267428712261951, valid_loss: 0.005087765632197261\nFOLD: 0, EPOCH: 12, train_loss: 0.008220596868983087, valid_loss: 0.004777400366341074\nFOLD: 0, EPOCH: 13, train_loss: 0.008181618362226906, valid_loss: 0.004765879052380721\nFOLD: 0, EPOCH: 14, train_loss: 0.008137738934772857, valid_loss: 0.004833883761117856\nFOLD: 0, EPOCH: 15, train_loss: 0.008105658118010443, valid_loss: 0.004730530160789688\nFOLD: 0, EPOCH: 16, train_loss: 0.0080776521251263, valid_loss: 0.004987099363158147\nFOLD: 0, EPOCH: 17, train_loss: 0.008071465335567208, valid_loss: 0.004828408282871048\nFOLD: 0, EPOCH: 18, train_loss: 0.008036796003580093, valid_loss: 0.00499474451256295\nFOLD: 0, EPOCH: 19, train_loss: 0.008022352779174553, valid_loss: 0.0047757867723703384\nFOLD: 0, EPOCH: 20, train_loss: 0.008019045707495773, valid_loss: 0.00518401968292892\nFOLD: 0, EPOCH: 21, train_loss: 0.008009797701721682, valid_loss: 0.004737649345770478\nFOLD: 0, EPOCH: 22, train_loss: 0.007998223115197, valid_loss: 0.00513727815511326\nFOLD: 0, EPOCH: 23, train_loss: 0.008000046535230735, valid_loss: 0.004860857113574942\nFOLD: 0, EPOCH: 24, train_loss: 0.007981727461275808, valid_loss: 0.004619816473374764\nBeginning training for fold 0\nFOLD: 0, EPOCH: 0, train_loss: 0.5164189535905334, valid_loss: 0.1529132972160975\nFOLD: 0, EPOCH: 1, train_loss: 0.053290445028859025, valid_loss: 0.022333419571320217\nFOLD: 0, EPOCH: 2, train_loss: 0.023962608045514894, valid_loss: 0.019781123846769333\nFOLD: 0, EPOCH: 3, train_loss: 0.022047855014748433, valid_loss: 0.01860586243371169\nFOLD: 0, EPOCH: 4, train_loss: 0.021403809351956144, valid_loss: 0.01853579158584277\nFOLD: 0, EPOCH: 5, train_loss: 0.02116479932823602, valid_loss: 0.018431841085354488\nFOLD: 0, EPOCH: 6, train_loss: 0.020842006156111464, valid_loss: 0.018378304317593575\nFOLD: 0, EPOCH: 7, train_loss: 0.02072404998847667, valid_loss: 0.018706314265727997\nFOLD: 0, EPOCH: 8, train_loss: 0.020568387308979735, valid_loss: 0.017835902360578377\nFOLD: 0, EPOCH: 9, train_loss: 0.020467270110898158, valid_loss: 0.01797145853439967\nFOLD: 0, EPOCH: 10, train_loss: 0.020473996396450436, valid_loss: 0.017665765869120758\nFOLD: 0, EPOCH: 11, train_loss: 0.020333436098607147, valid_loss: 0.01762637992699941\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 12, train_loss: 0.02016551557051785, valid_loss: 0.01736597002794345\nFOLD: 0, EPOCH: 13, train_loss: 0.019808347117813194, valid_loss: 0.01733596095194419\nFOLD: 0, EPOCH: 14, train_loss: 0.019716340555425954, valid_loss: 0.017385664706428845\nFOLD: 0, EPOCH: 15, train_loss: 0.019670705804053473, valid_loss: 0.017609815423687298\nFOLD: 0, EPOCH: 16, train_loss: 0.019599882940597394, valid_loss: 0.01753464372207721\nFOLD: 0, EPOCH: 17, train_loss: 0.019538333091665718, valid_loss: 0.01744257565587759\nFOLD: 0, EPOCH: 18, train_loss: 0.01951745048384456, valid_loss: 0.0172515536348025\nFOLD: 0, EPOCH: 19, train_loss: 0.01947177524733193, valid_loss: 0.017684581068654854\nFOLD: 0, EPOCH: 20, train_loss: 0.01945098363520468, valid_loss: 0.01740450815608104\nFOLD: 0, EPOCH: 21, train_loss: 0.019431503434829852, valid_loss: 0.017291766591370106\nFOLD: 0, EPOCH: 22, train_loss: 0.019391772008555776, valid_loss: 0.017204327508807182\nFOLD: 0, EPOCH: 23, train_loss: 0.01939249745405772, valid_loss: 0.017486602067947388\nFOLD: 0, EPOCH: 24, train_loss: 0.01938102103988914, valid_loss: 0.01743679866194725\nFOLD: 0, EPOCH: 25, train_loss: 0.01932779387296999, valid_loss: 0.017446773126721382\nFOLD: 0, EPOCH: 26, train_loss: 0.019333105315180385, valid_loss: 0.017518790438771248\nFOLD: 0, EPOCH: 27, train_loss: 0.019305002163438237, valid_loss: 0.017357850757737953\nFOLD: 0, EPOCH: 28, train_loss: 0.01927772180779892, valid_loss: 0.017429142879943054\nFOLD: 0, EPOCH: 29, train_loss: 0.019260986269835162, valid_loss: 0.01726337584356467\nFOLD: 0, EPOCH: 30, train_loss: 0.01923681209411691, valid_loss: 0.017312707379460335\nFOLD: 0, EPOCH: 31, train_loss: 0.019207211540025824, valid_loss: 0.01742077215264241\nFOLD: 0, EPOCH: 32, train_loss: 0.019207817666670856, valid_loss: 0.01742328144609928\nFOLD: 0, EPOCH: 33, train_loss: 0.01920774480437531, valid_loss: 0.01734947816779216\nFOLD: 0, EPOCH: 34, train_loss: 0.019158477347125027, valid_loss: 0.017496092555423576\nFOLD: 0, EPOCH: 35, train_loss: 0.01914649166385917, valid_loss: 0.01733548225214084\nFOLD: 0, EPOCH: 36, train_loss: 0.019126065951936385, valid_loss: 0.01744476364304622\nFOLD: 0, EPOCH: 37, train_loss: 0.019084634642828915, valid_loss: 0.01736153227587541\nFOLD: 0, EPOCH: 38, train_loss: 0.019078816944623694, valid_loss: 0.017368235314885776\nFOLD: 0, EPOCH: 39, train_loss: 0.019070385133518893, valid_loss: 0.017403675243258476\nFOLD: 0, EPOCH: 40, train_loss: 0.019056671518175042, valid_loss: 0.017415440641343594\nFOLD: 0, EPOCH: 41, train_loss: 0.019035030813778147, valid_loss: 0.017363015251855057\nFOLD: 0, EPOCH: 42, train_loss: 0.01900910914820783, valid_loss: 0.01726991341759761\nFOLD: 0, EPOCH: 43, train_loss: 0.01896219963536543, valid_loss: 0.017261553555727005\nFOLD: 0, EPOCH: 44, train_loss: 0.018971113214159712, valid_loss: 0.017250696818033855\nFOLD: 0, EPOCH: 45, train_loss: 0.01897134889355477, valid_loss: 0.017273616977036\nFOLD: 0, EPOCH: 46, train_loss: 0.018949343548978075, valid_loss: 0.01733018395801385\nFOLD: 0, EPOCH: 47, train_loss: 0.018922006985282198, valid_loss: 0.017296588669220608\nFOLD: 0, EPOCH: 48, train_loss: 0.018900221487616792, valid_loss: 0.01727352725962798\nFOLD: 0, EPOCH: 49, train_loss: 0.01890128418145811, valid_loss: 0.017245887778699398\nFOLD: 0, EPOCH: 50, train_loss: 0.01888687311507323, valid_loss: 0.017273968396087486\nFOLD: 0, EPOCH: 51, train_loss: 0.01886333738837172, valid_loss: 0.017251754490037758\nFOLD: 0, EPOCH: 52, train_loss: 0.018867629087146592, valid_loss: 0.017248987530668575\nFOLD: 0, EPOCH: 53, train_loss: 0.018811898908632642, valid_loss: 0.01728117497016986\nFOLD: 0, EPOCH: 54, train_loss: 0.01883336830445949, valid_loss: 0.017268420507510502\nFOLD: 0, EPOCH: 55, train_loss: 0.01884080732569975, valid_loss: 0.017254704609513283\nFOLD: 0, EPOCH: 56, train_loss: 0.018821719759965643, valid_loss: 0.017255653627216816\nFOLD: 0, EPOCH: 57, train_loss: 0.018832836519269383, valid_loss: 0.017262835676471393\nFOLD: 0, EPOCH: 58, train_loss: 0.018797730643521336, valid_loss: 0.017259357186655205\nFOLD: 0, EPOCH: 59, train_loss: 0.018801975830951157, valid_loss: 0.01725791860371828\nBeginning pretraining for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.7291774451732635, valid_loss: 0.6947248578071594\nFOLD: 1, EPOCH: 1, train_loss: 0.6115937618648305, valid_loss: 0.34239529073238373\nFOLD: 1, EPOCH: 2, train_loss: 0.11207968095207915, valid_loss: 0.014666392467916012\nFOLD: 1, EPOCH: 3, train_loss: 0.01193172753076343, valid_loss: 0.0065595638783027726\nFOLD: 1, EPOCH: 4, train_loss: 0.009108060395673793, valid_loss: 0.005633131290475528\nFOLD: 1, EPOCH: 5, train_loss: 0.008588375468902728, valid_loss: 0.005317470291629434\nFOLD: 1, EPOCH: 6, train_loss: 0.008420388556688148, valid_loss: 0.005247310424844424\nFOLD: 1, EPOCH: 7, train_loss: 0.00835237790392164, valid_loss: 0.005107219719017546\nFOLD: 1, EPOCH: 8, train_loss: 0.008331081966924317, valid_loss: 0.004941704450175166\nFOLD: 1, EPOCH: 9, train_loss: 0.008299617918536943, valid_loss: 0.005080589636539419\nFOLD: 1, EPOCH: 10, train_loss: 0.008228473009212929, valid_loss: 0.004796733458836873\nFOLD: 1, EPOCH: 11, train_loss: 0.00821340162142673, valid_loss: 0.005014830424139897\nFOLD: 1, EPOCH: 12, train_loss: 0.008163770181400812, valid_loss: 0.005118703314413627\nFOLD: 1, EPOCH: 13, train_loss: 0.00812635084559374, valid_loss: 0.004711187367017071\nFOLD: 1, EPOCH: 14, train_loss: 0.008089348043808165, valid_loss: 0.0052334094264854985\nFOLD: 1, EPOCH: 15, train_loss: 0.008063529960482436, valid_loss: 0.004948399495333433\nFOLD: 1, EPOCH: 16, train_loss: 0.008045330273864023, valid_loss: 0.004800975555554032\nFOLD: 1, EPOCH: 17, train_loss: 0.008029529756373343, valid_loss: 0.004930325395738085\nFOLD: 1, EPOCH: 18, train_loss: 0.008006358943770038, valid_loss: 0.004808423419793447\nFOLD: 1, EPOCH: 19, train_loss: 0.007982931305270861, valid_loss: 0.004891388040656845\nFOLD: 1, EPOCH: 20, train_loss: 0.007972958675750038, valid_loss: 0.004820691576848428\nFOLD: 1, EPOCH: 21, train_loss: 0.007956861052662134, valid_loss: 0.004821146683146556\nFOLD: 1, EPOCH: 22, train_loss: 0.007950943812508793, valid_loss: 0.0051390368801852064\nFOLD: 1, EPOCH: 23, train_loss: 0.00795407296048806, valid_loss: 0.004943796743949254\nFOLD: 1, EPOCH: 24, train_loss: 0.007944254464853336, valid_loss: 0.005081830080598593\nBeginning training for fold 1\nFOLD: 1, EPOCH: 0, train_loss: 0.5402604613234016, valid_loss: 0.18033824861049652\nFOLD: 1, EPOCH: 1, train_loss: 0.059587217483888656, valid_loss: 0.02174877778937419\nFOLD: 1, EPOCH: 2, train_loss: 0.02351423813139691, valid_loss: 0.019391455687582493\nFOLD: 1, EPOCH: 3, train_loss: 0.021697586183162296, valid_loss: 0.01827220742901166\nFOLD: 1, EPOCH: 4, train_loss: 0.021199134561945412, valid_loss: 0.01798529488344987\nFOLD: 1, EPOCH: 5, train_loss: 0.02088755467797027, valid_loss: 0.017825720831751823\nFOLD: 1, EPOCH: 6, train_loss: 0.02066767149988343, valid_loss: 0.018119020387530327\nFOLD: 1, EPOCH: 7, train_loss: 0.020517302896170056, valid_loss: 0.01844634022563696\nFOLD: 1, EPOCH: 8, train_loss: 0.02040482230265351, valid_loss: 0.0176900972922643\nFOLD: 1, EPOCH: 9, train_loss: 0.02031796102357261, valid_loss: 0.017775732713441055\nFOLD: 1, EPOCH: 10, train_loss: 0.02033182408879785, valid_loss: 0.017536722123622894\nFOLD: 1, EPOCH: 11, train_loss: 0.020221835878842017, valid_loss: 0.01770595305909713\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 12, train_loss: 0.019972476615187, valid_loss: 0.017568431173761685\nFOLD: 1, EPOCH: 13, train_loss: 0.01978618755717488, valid_loss: 0.017552020649115246\nFOLD: 1, EPOCH: 14, train_loss: 0.019725504331290722, valid_loss: 0.017575806006789207\nFOLD: 1, EPOCH: 15, train_loss: 0.01959506274365327, valid_loss: 0.017446233270068962\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 16, train_loss: 0.019537341890527922, valid_loss: 0.017461890665193398\nFOLD: 1, EPOCH: 17, train_loss: 0.019421520259450462, valid_loss: 0.017423783428967\nFOLD: 1, EPOCH: 18, train_loss: 0.01931112681460731, valid_loss: 0.017521412732700508\nFOLD: 1, EPOCH: 19, train_loss: 0.019254024688373592, valid_loss: 0.01735622777293126\nFOLD: 1, EPOCH: 20, train_loss: 0.019223507165032273, valid_loss: 0.017334566141168278\nFOLD: 1, EPOCH: 21, train_loss: 0.01916827283361379, valid_loss: 0.017433227350314457\nFOLD: 1, EPOCH: 22, train_loss: 0.01912873232846751, valid_loss: 0.0173009450857838\nFOLD: 1, EPOCH: 23, train_loss: 0.01910701531040318, valid_loss: 0.017465231319268543\nFOLD: 1, EPOCH: 24, train_loss: 0.019115453585982323, valid_loss: 0.017357662009696167\nFOLD: 1, EPOCH: 25, train_loss: 0.019053796799305606, valid_loss: 0.017364061747988064\nFOLD: 1, EPOCH: 26, train_loss: 0.019045101314344826, valid_loss: 0.017470782933135826\nFOLD: 1, EPOCH: 27, train_loss: 0.019019346300731686, valid_loss: 0.017523000948131084\nFOLD: 1, EPOCH: 28, train_loss: 0.01898462451337015, valid_loss: 0.01730198971927166\nFOLD: 1, EPOCH: 29, train_loss: 0.018983348437091884, valid_loss: 0.01728889811784029\nFOLD: 1, EPOCH: 30, train_loss: 0.018950350473032278, valid_loss: 0.017383793989817303\nFOLD: 1, EPOCH: 31, train_loss: 0.01892573254949906, valid_loss: 0.017412976051370304\nFOLD: 1, EPOCH: 32, train_loss: 0.01889918793869369, valid_loss: 0.017346438020467758\nFOLD: 1, EPOCH: 33, train_loss: 0.018865890472250825, valid_loss: 0.017204339616000652\nFOLD: 1, EPOCH: 34, train_loss: 0.018875653965069968, valid_loss: 0.017328172301252682\nFOLD: 1, EPOCH: 35, train_loss: 0.018836085848948535, valid_loss: 0.017336274807651837\nFOLD: 1, EPOCH: 36, train_loss: 0.018843736937817407, valid_loss: 0.017309606385727722\nFOLD: 1, EPOCH: 37, train_loss: 0.018789296367150897, valid_loss: 0.017340599248806637\nFOLD: 1, EPOCH: 38, train_loss: 0.018758889178142828, valid_loss: 0.017225941953559715\nFOLD: 1, EPOCH: 39, train_loss: 0.018745368017869836, valid_loss: 0.01732374293108781\nFOLD: 1, EPOCH: 40, train_loss: 0.01872160504846012, valid_loss: 0.017348083357016247\nFOLD: 1, EPOCH: 41, train_loss: 0.01871991338317885, valid_loss: 0.01726520713418722\nFOLD: 1, EPOCH: 42, train_loss: 0.018671445493750712, valid_loss: 0.01722101680934429\nFOLD: 1, EPOCH: 43, train_loss: 0.018657242714920464, valid_loss: 0.017195118591189384\nFOLD: 1, EPOCH: 44, train_loss: 0.018640531829613095, valid_loss: 0.017209783506890137\nFOLD: 1, EPOCH: 45, train_loss: 0.018582949250498238, valid_loss: 0.0172257199883461\nFOLD: 1, EPOCH: 46, train_loss: 0.01858833755421288, valid_loss: 0.01716548080245654\nFOLD: 1, EPOCH: 47, train_loss: 0.018563637400374693, valid_loss: 0.017166313094397385\nFOLD: 1, EPOCH: 48, train_loss: 0.018548102994613788, valid_loss: 0.01721770130097866\nFOLD: 1, EPOCH: 49, train_loss: 0.0185314494130366, valid_loss: 0.01719847818215688\nFOLD: 1, EPOCH: 50, train_loss: 0.018518281717072513, valid_loss: 0.01719746955980857\nFOLD: 1, EPOCH: 51, train_loss: 0.018500450505491567, valid_loss: 0.0171764912083745\nFOLD: 1, EPOCH: 52, train_loss: 0.018523393965819302, valid_loss: 0.017202688691516716\nFOLD: 1, EPOCH: 53, train_loss: 0.018471983535324827, valid_loss: 0.01718577432135741\nFOLD: 1, EPOCH: 54, train_loss: 0.018489701125551674, valid_loss: 0.017205148314436276\nFOLD: 1, EPOCH: 55, train_loss: 0.018455562236554483, valid_loss: 0.017194263637065887\nFOLD: 1, EPOCH: 56, train_loss: 0.018478676238480735, valid_loss: 0.017181734864910442\nFOLD: 1, EPOCH: 57, train_loss: 0.018467931898639482, valid_loss: 0.017184766630331676\nFOLD: 1, EPOCH: 58, train_loss: 0.01848125014015857, valid_loss: 0.017185467916230362\nFOLD: 1, EPOCH: 59, train_loss: 0.018501444555380765, valid_loss: 0.01718422615279754\nBeginning pretraining for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.7294187387999367, valid_loss: 0.6958232820034027\nFOLD: 2, EPOCH: 1, train_loss: 0.6137140261776307, valid_loss: 0.3434138000011444\nFOLD: 2, EPOCH: 2, train_loss: 0.11242545615224277, valid_loss: 0.014478202598790327\nFOLD: 2, EPOCH: 3, train_loss: 0.011956816895262283, valid_loss: 0.006344936943302552\nFOLD: 2, EPOCH: 4, train_loss: 0.009180658387348932, valid_loss: 0.00548128744897743\nFOLD: 2, EPOCH: 5, train_loss: 0.008689960300484124, valid_loss: 0.004967988235875964\nFOLD: 2, EPOCH: 6, train_loss: 0.00851718322647845, valid_loss: 0.0047037289865935845\nFOLD: 2, EPOCH: 7, train_loss: 0.008448124753639978, valid_loss: 0.005457233637571335\nFOLD: 2, EPOCH: 8, train_loss: 0.008385259356787977, valid_loss: 0.005077531561255455\nFOLD: 2, EPOCH: 9, train_loss: 0.008333737071713103, valid_loss: 0.004661795722010235\nFOLD: 2, EPOCH: 10, train_loss: 0.00826937161550364, valid_loss: 0.00485307932831347\nFOLD: 2, EPOCH: 11, train_loss: 0.008246176355682752, valid_loss: 0.00466632788690428\nFOLD: 2, EPOCH: 12, train_loss: 0.008194189683041152, valid_loss: 0.004979095887392759\nFOLD: 2, EPOCH: 13, train_loss: 0.00818159561330343, valid_loss: 0.005124127181867759\nFOLD: 2, EPOCH: 14, train_loss: 0.00814088887315901, valid_loss: 0.004974199614177148\nFOLD: 2, EPOCH: 15, train_loss: 0.008100037148003192, valid_loss: 0.005194629930580656\nFOLD: 2, EPOCH: 16, train_loss: 0.008060517885229167, valid_loss: 0.00493090048742791\nFOLD: 2, EPOCH: 17, train_loss: 0.008053287356982337, valid_loss: 0.004901295372595389\nFOLD: 2, EPOCH: 18, train_loss: 0.008039905443130171, valid_loss: 0.004561901519385477\nFOLD: 2, EPOCH: 19, train_loss: 0.008029887274674633, valid_loss: 0.0049536411340037985\nFOLD: 2, EPOCH: 20, train_loss: 0.008025795947212507, valid_loss: 0.004751133072810869\nFOLD: 2, EPOCH: 21, train_loss: 0.008012278772452298, valid_loss: 0.004676187372145553\nFOLD: 2, EPOCH: 22, train_loss: 0.0080178965239183, valid_loss: 0.004568945189627509\nFOLD: 2, EPOCH: 23, train_loss: 0.00801143589813043, valid_loss: 0.004733004917701085\nFOLD: 2, EPOCH: 24, train_loss: 0.00800295123446952, valid_loss: 0.005046036948139469\nBeginning training for fold 2\nFOLD: 2, EPOCH: 0, train_loss: 0.5333810987717965, valid_loss: 0.16240333020687103\nFOLD: 2, EPOCH: 1, train_loss: 0.056556577958604866, valid_loss: 0.022542282318075497\nFOLD: 2, EPOCH: 2, train_loss: 0.023649736307561398, valid_loss: 0.0207363481943806\nFOLD: 2, EPOCH: 3, train_loss: 0.021854215246789595, valid_loss: 0.019183052082856495\nFOLD: 2, EPOCH: 4, train_loss: 0.021316401167389226, valid_loss: 0.019770444991687935\nFOLD: 2, EPOCH: 5, train_loss: 0.021011691156993893, valid_loss: 0.01859007744739453\nFOLD: 2, EPOCH: 6, train_loss: 0.020815575221443876, valid_loss: 0.018932470430930454\nFOLD: 2, EPOCH: 7, train_loss: 0.020700668861322543, valid_loss: 0.01843247065941493\nFOLD: 2, EPOCH: 8, train_loss: 0.020548125345479038, valid_loss: 0.018337023134032886\nFOLD: 2, EPOCH: 9, train_loss: 0.020400821779142406, valid_loss: 0.018418499268591404\nFOLD: 2, EPOCH: 10, train_loss: 0.0203186648843043, valid_loss: 0.01832031396528085\nFOLD: 2, EPOCH: 11, train_loss: 0.020228871966109556, valid_loss: 0.01830108215411504\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 12, train_loss: 0.020102720378952867, valid_loss: 0.01804420289893945\nFOLD: 2, EPOCH: 13, train_loss: 0.01974790296791231, valid_loss: 0.018173145440717537\nFOLD: 2, EPOCH: 14, train_loss: 0.019645544645540854, valid_loss: 0.018479264962176483\nFOLD: 2, EPOCH: 15, train_loss: 0.01961465793497422, valid_loss: 0.01811727819343408\nFOLD: 2, EPOCH: 16, train_loss: 0.019519737812087816, valid_loss: 0.018085932669540245\nFOLD: 2, EPOCH: 17, train_loss: 0.019461861318525148, valid_loss: 0.018298265834649403\nFOLD: 2, EPOCH: 18, train_loss: 0.019462848794372642, valid_loss: 0.01808181529243787\nFOLD: 2, EPOCH: 19, train_loss: 0.019411304965615273, valid_loss: 0.018154371840258438\nFOLD: 2, EPOCH: 20, train_loss: 0.019386681180228207, valid_loss: 0.018206654426952202\nFOLD: 2, EPOCH: 21, train_loss: 0.019364143557408276, valid_loss: 0.018071612653632958\nFOLD: 2, EPOCH: 22, train_loss: 0.01932648426907904, valid_loss: 0.01808448260029157\nFOLD: 2, EPOCH: 23, train_loss: 0.01930210947552148, valid_loss: 0.018004491925239563\nFOLD: 2, EPOCH: 24, train_loss: 0.019253608825452188, valid_loss: 0.01808026432991028\nFOLD: 2, EPOCH: 25, train_loss: 0.01925574308809112, valid_loss: 0.01793731314440568\nFOLD: 2, EPOCH: 26, train_loss: 0.01924782925668885, valid_loss: 0.018272771189610165\nFOLD: 2, EPOCH: 27, train_loss: 0.019241118748836657, valid_loss: 0.018215916740397613\nFOLD: 2, EPOCH: 28, train_loss: 0.019184937098008746, valid_loss: 0.017985279361406963\nFOLD: 2, EPOCH: 29, train_loss: 0.01921714562922716, valid_loss: 0.018099029858907063\nFOLD: 2, EPOCH: 30, train_loss: 0.01919554639607668, valid_loss: 0.018338697962462902\nFOLD: 2, EPOCH: 31, train_loss: 0.019171273993218645, valid_loss: 0.017983755407234032\nFOLD: 2, EPOCH: 32, train_loss: 0.01914453670820769, valid_loss: 0.018075187380115192\nFOLD: 2, EPOCH: 33, train_loss: 0.019115903524353224, valid_loss: 0.01806116693963607\nFOLD: 2, EPOCH: 34, train_loss: 0.01909895706921816, valid_loss: 0.018060369106630485\nFOLD: 2, EPOCH: 35, train_loss: 0.0191158855553059, valid_loss: 0.018002210184931755\nFOLD: 2, EPOCH: 36, train_loss: 0.019066240145441365, valid_loss: 0.0179812911277016\nFOLD: 2, EPOCH: 37, train_loss: 0.019076920443159694, valid_loss: 0.017963506591816742\nFOLD: 2, EPOCH: 38, train_loss: 0.01902768454130958, valid_loss: 0.018024278183778126\nFOLD: 2, EPOCH: 39, train_loss: 0.01900070418110665, valid_loss: 0.01796234492212534\nFOLD: 2, EPOCH: 40, train_loss: 0.019015724332455325, valid_loss: 0.018043027569850285\nFOLD: 2, EPOCH: 41, train_loss: 0.018990656306200168, valid_loss: 0.017968522384762764\nFOLD: 2, EPOCH: 42, train_loss: 0.01895511763937333, valid_loss: 0.017997111504276592\nFOLD: 2, EPOCH: 43, train_loss: 0.018918129505918306, valid_loss: 0.017976267884174984\nFOLD: 2, EPOCH: 44, train_loss: 0.01892322805874488, valid_loss: 0.017988293431699276\nFOLD: 2, EPOCH: 45, train_loss: 0.018907217966283068, valid_loss: 0.017949120762447517\nFOLD: 2, EPOCH: 46, train_loss: 0.018907253904377714, valid_loss: 0.017926335334777832\nFOLD: 2, EPOCH: 47, train_loss: 0.0188701824449441, valid_loss: 0.017948631818095844\nFOLD: 2, EPOCH: 48, train_loss: 0.018845225300859, valid_loss: 0.017946639098227024\nFOLD: 2, EPOCH: 49, train_loss: 0.018854077850632808, valid_loss: 0.017946554347872734\nFOLD: 2, EPOCH: 50, train_loss: 0.0188210034633384, valid_loss: 0.0179305129374067\nFOLD: 2, EPOCH: 51, train_loss: 0.01881807730259264, valid_loss: 0.017965239162246387\nFOLD: 2, EPOCH: 52, train_loss: 0.01881532688789508, valid_loss: 0.01795875933021307\nFOLD: 2, EPOCH: 53, train_loss: 0.018797977772705695, valid_loss: 0.01793772696206967\nFOLD: 2, EPOCH: 54, train_loss: 0.01879531068398672, valid_loss: 0.017944018356502056\nFOLD: 2, EPOCH: 55, train_loss: 0.018773548305034637, valid_loss: 0.017945483947793644\nFOLD: 2, EPOCH: 56, train_loss: 0.01879615807796226, valid_loss: 0.017949598841369152\nFOLD: 2, EPOCH: 57, train_loss: 0.01877504663870615, valid_loss: 0.017948185404141743\nFOLD: 2, EPOCH: 58, train_loss: 0.018790585920214653, valid_loss: 0.017945264155666035\nFOLD: 2, EPOCH: 59, train_loss: 0.018774571992895183, valid_loss: 0.01794461254030466\nBeginning pretraining for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.7294178570018095, valid_loss: 0.6955376366774241\nFOLD: 3, EPOCH: 1, train_loss: 0.6125447373179829, valid_loss: 0.3387097269296646\nFOLD: 3, EPOCH: 2, train_loss: 0.1116842277457609, valid_loss: 0.014398312196135521\nFOLD: 3, EPOCH: 3, train_loss: 0.012210739584749235, valid_loss: 0.006435493007302284\nFOLD: 3, EPOCH: 4, train_loss: 0.009135400109431323, valid_loss: 0.005382696632295847\nFOLD: 3, EPOCH: 5, train_loss: 0.008666779161156976, valid_loss: 0.0051745214344312744\nFOLD: 3, EPOCH: 6, train_loss: 0.008493385907700834, valid_loss: 0.005080574036886294\nFOLD: 3, EPOCH: 7, train_loss: 0.00843529899001998, valid_loss: 0.004971520431960623\nFOLD: 3, EPOCH: 8, train_loss: 0.008378402247805805, valid_loss: 0.00509163667447865\nFOLD: 3, EPOCH: 9, train_loss: 0.008314425591379404, valid_loss: 0.004898487823083997\nFOLD: 3, EPOCH: 10, train_loss: 0.008298885679858573, valid_loss: 0.00486464883821706\nFOLD: 3, EPOCH: 11, train_loss: 0.008233350646846434, valid_loss: 0.005201784272988637\nFOLD: 3, EPOCH: 12, train_loss: 0.008194847059819628, valid_loss: 0.005221462498108546\nFOLD: 3, EPOCH: 13, train_loss: 0.008177899002261898, valid_loss: 0.005248920448745291\nFOLD: 3, EPOCH: 14, train_loss: 0.008146881057387766, valid_loss: 0.004759026613707344\nFOLD: 3, EPOCH: 15, train_loss: 0.008126137695987435, valid_loss: 0.005019168136641383\nFOLD: 3, EPOCH: 16, train_loss: 0.008101389318814172, valid_loss: 0.004735873236010472\nFOLD: 3, EPOCH: 17, train_loss: 0.008078792502227075, valid_loss: 0.004596274734164278\nFOLD: 3, EPOCH: 18, train_loss: 0.008065688232069506, valid_loss: 0.004928809047366182\nFOLD: 3, EPOCH: 19, train_loss: 0.008051855352652423, valid_loss: 0.004886167511964838\nFOLD: 3, EPOCH: 20, train_loss: 0.008036522509749322, valid_loss: 0.004664413010080655\nFOLD: 3, EPOCH: 21, train_loss: 0.00803450636072632, valid_loss: 0.004886787384748459\nFOLD: 3, EPOCH: 22, train_loss: 0.008020388236378921, valid_loss: 0.004907823400571942\nFOLD: 3, EPOCH: 23, train_loss: 0.008027660249568084, valid_loss: 0.004873428959399462\nFOLD: 3, EPOCH: 24, train_loss: 0.00803306616623612, valid_loss: 0.004793886328116059\nBeginning training for fold 3\nFOLD: 3, EPOCH: 0, train_loss: 0.5383404462653048, valid_loss: 0.161064679423968\nFOLD: 3, EPOCH: 1, train_loss: 0.057658562874969316, valid_loss: 0.02201878000050783\nFOLD: 3, EPOCH: 2, train_loss: 0.02371912696124876, valid_loss: 0.01992259236673514\nFOLD: 3, EPOCH: 3, train_loss: 0.02192976336707087, valid_loss: 0.018776696485777695\nFOLD: 3, EPOCH: 4, train_loss: 0.021353181506342748, valid_loss: 0.019360044971108437\nFOLD: 3, EPOCH: 5, train_loss: 0.02110529165057575, valid_loss: 0.018489232907692593\nFOLD: 3, EPOCH: 6, train_loss: 0.02097554345998694, valid_loss: 0.01802973325053851\nFOLD: 3, EPOCH: 7, train_loss: 0.020803330192232832, valid_loss: 0.01785849624623855\nFOLD: 3, EPOCH: 8, train_loss: 0.02066571807817501, valid_loss: 0.018186541584630806\nFOLD: 3, EPOCH: 9, train_loss: 0.020573249360656038, valid_loss: 0.017954573966562748\nFOLD: 3, EPOCH: 10, train_loss: 0.020511215607471326, valid_loss: 0.01778315007686615\nFOLD: 3, EPOCH: 11, train_loss: 0.020373833365738392, valid_loss: 0.01768973997483651\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 12, train_loss: 0.02023170899380656, valid_loss: 0.017490547771255176\nFOLD: 3, EPOCH: 13, train_loss: 0.019895523512626394, valid_loss: 0.017377398287256558\nFOLD: 3, EPOCH: 14, train_loss: 0.019792790031608415, valid_loss: 0.017428066891928513\nFOLD: 3, EPOCH: 15, train_loss: 0.019695291569566026, valid_loss: 0.017256205901503563\nFOLD: 3, EPOCH: 16, train_loss: 0.019661821151042685, valid_loss: 0.017396917566657066\nFOLD: 3, EPOCH: 17, train_loss: 0.019625685725580242, valid_loss: 0.017292405478656292\nFOLD: 3, EPOCH: 18, train_loss: 0.019586217754027423, valid_loss: 0.017326108490427334\nFOLD: 3, EPOCH: 19, train_loss: 0.01954147488097934, valid_loss: 0.017355629863838356\nFOLD: 3, EPOCH: 20, train_loss: 0.019524072187350076, valid_loss: 0.017286959414680798\nFOLD: 3, EPOCH: 21, train_loss: 0.019490823204464772, valid_loss: 0.017346075425545376\nFOLD: 3, EPOCH: 22, train_loss: 0.019455079153618392, valid_loss: 0.017264170572161674\nFOLD: 3, EPOCH: 23, train_loss: 0.019442183951682904, valid_loss: 0.01723311065385739\nFOLD: 3, EPOCH: 24, train_loss: 0.01940396647242939, valid_loss: 0.017421964245537918\nFOLD: 3, EPOCH: 25, train_loss: 0.019406758029671276, valid_loss: 0.017255725028614204\nFOLD: 3, EPOCH: 26, train_loss: 0.019381910452947897, valid_loss: 0.017345579341053963\nFOLD: 3, EPOCH: 27, train_loss: 0.019369267146377003, valid_loss: 0.01729529133687417\nFOLD: 3, EPOCH: 28, train_loss: 0.01934674705433495, valid_loss: 0.017214572988450527\nFOLD: 3, EPOCH: 29, train_loss: 0.01931614921811749, valid_loss: 0.017278430983424187\nFOLD: 3, EPOCH: 30, train_loss: 0.01929652296444949, valid_loss: 0.017111883188287418\nFOLD: 3, EPOCH: 31, train_loss: 0.019260639763053727, valid_loss: 0.01720066647976637\nFOLD: 3, EPOCH: 32, train_loss: 0.019237654721912217, valid_loss: 0.017275923552612465\nFOLD: 3, EPOCH: 33, train_loss: 0.019232784562251148, valid_loss: 0.017276120992998283\nFOLD: 3, EPOCH: 34, train_loss: 0.019230571904164905, valid_loss: 0.01721318221340577\nFOLD: 3, EPOCH: 35, train_loss: 0.019222431980511722, valid_loss: 0.01718618782858054\nFOLD: 3, EPOCH: 36, train_loss: 0.019150915939141724, valid_loss: 0.017241752706468105\nFOLD: 3, EPOCH: 37, train_loss: 0.019159591099356905, valid_loss: 0.017174212262034416\nFOLD: 3, EPOCH: 38, train_loss: 0.01914043850539362, valid_loss: 0.01720783890535434\nFOLD: 3, EPOCH: 39, train_loss: 0.019105047425803018, valid_loss: 0.017205616769691307\nFOLD: 3, EPOCH: 40, train_loss: 0.019132814644014135, valid_loss: 0.01713205687701702\nFOLD: 3, EPOCH: 41, train_loss: 0.019096102675094324, valid_loss: 0.01713673211634159\nFOLD: 3, EPOCH: 42, train_loss: 0.019067729987642345, valid_loss: 0.01710907183587551\nFOLD: 3, EPOCH: 43, train_loss: 0.01904068438007551, valid_loss: 0.017205475208659966\nFOLD: 3, EPOCH: 44, train_loss: 0.019035842543577448, valid_loss: 0.017179963799814384\nFOLD: 3, EPOCH: 45, train_loss: 0.01901327829588862, valid_loss: 0.017068164112667244\nFOLD: 3, EPOCH: 46, train_loss: 0.018999809289679807, valid_loss: 0.017054379917681217\nFOLD: 3, EPOCH: 47, train_loss: 0.018995365182704785, valid_loss: 0.01724926196038723\nFOLD: 3, EPOCH: 48, train_loss: 0.018955419387887504, valid_loss: 0.017125280263523262\nFOLD: 3, EPOCH: 49, train_loss: 0.018950554377892437, valid_loss: 0.017124708431462448\nFOLD: 3, EPOCH: 50, train_loss: 0.018913075108738506, valid_loss: 0.017077612380186718\nFOLD: 3, EPOCH: 51, train_loss: 0.018923932029043928, valid_loss: 0.017109873704612255\nFOLD: 3, EPOCH: 52, train_loss: 0.018886893768520915, valid_loss: 0.01709985639899969\nFOLD: 3, EPOCH: 53, train_loss: 0.01889926107490764, valid_loss: 0.017097380943596363\nFOLD: 3, EPOCH: 54, train_loss: 0.018890435478704816, valid_loss: 0.017099624499678612\nFOLD: 3, EPOCH: 55, train_loss: 0.018883173080051645, valid_loss: 0.017078439394632976\nFOLD: 3, EPOCH: 56, train_loss: 0.018866566393305275, valid_loss: 0.017083202799161274\nFOLD: 3, EPOCH: 57, train_loss: 0.018854865037343082, valid_loss: 0.017082476367553074\nFOLD: 3, EPOCH: 58, train_loss: 0.018886327250477147, valid_loss: 0.017085382714867592\nFOLD: 3, EPOCH: 59, train_loss: 0.018847233287113553, valid_loss: 0.017084763074914616\nBeginning pretraining for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.7295852882020614, valid_loss: 0.6958296597003937\nFOLD: 4, EPOCH: 1, train_loss: 0.6140079673598794, valid_loss: 0.3406508465607961\nFOLD: 4, EPOCH: 2, train_loss: 0.112222516854458, valid_loss: 0.014653512587149939\nFOLD: 4, EPOCH: 3, train_loss: 0.012092597837395528, valid_loss: 0.006405306980013847\nFOLD: 4, EPOCH: 4, train_loss: 0.009153056451502968, valid_loss: 0.005356759453813235\nFOLD: 4, EPOCH: 5, train_loss: 0.008640995166976662, valid_loss: 0.00519214547239244\nFOLD: 4, EPOCH: 6, train_loss: 0.008460368070860995, valid_loss: 0.005179625547801455\nFOLD: 4, EPOCH: 7, train_loss: 0.008387126466807197, valid_loss: 0.004769923475881417\nFOLD: 4, EPOCH: 8, train_loss: 0.008314755485009621, valid_loss: 0.004863445647060871\nFOLD: 4, EPOCH: 9, train_loss: 0.008273295921218745, valid_loss: 0.004739365540444851\nFOLD: 4, EPOCH: 10, train_loss: 0.008271672393140547, valid_loss: 0.004720408391828339\nFOLD: 4, EPOCH: 11, train_loss: 0.008218845078612076, valid_loss: 0.004632105973238747\nFOLD: 4, EPOCH: 12, train_loss: 0.008146205574602765, valid_loss: 0.004651490754137437\nFOLD: 4, EPOCH: 13, train_loss: 0.008124559076831621, valid_loss: 0.004628026935582359\nFOLD: 4, EPOCH: 14, train_loss: 0.008096275755259045, valid_loss: 0.004704567526156704\nFOLD: 4, EPOCH: 15, train_loss: 0.008055375368498704, valid_loss: 0.005199728766456246\nFOLD: 4, EPOCH: 16, train_loss: 0.00801930682021467, valid_loss: 0.004786966523776452\nFOLD: 4, EPOCH: 17, train_loss: 0.008009289459818426, valid_loss: 0.004886246751993895\nFOLD: 4, EPOCH: 18, train_loss: 0.007992236025850563, valid_loss: 0.005426851566880941\nFOLD: 4, EPOCH: 19, train_loss: 0.007978552719578147, valid_loss: 0.004842726979404688\nFOLD: 4, EPOCH: 20, train_loss: 0.007971166797420558, valid_loss: 0.004819561416904132\nFOLD: 4, EPOCH: 21, train_loss: 0.007962119957322584, valid_loss: 0.004683522041887045\nFOLD: 4, EPOCH: 22, train_loss: 0.007966207888196497, valid_loss: 0.004734352386246125\nFOLD: 4, EPOCH: 23, train_loss: 0.007958796354668105, valid_loss: 0.004791926282147567\nFOLD: 4, EPOCH: 24, train_loss: 0.00795262267210466, valid_loss: 0.004808091480905811\nBeginning training for fold 4\nFOLD: 4, EPOCH: 0, train_loss: 0.547352708876133, valid_loss: 0.16721549133459726\nFOLD: 4, EPOCH: 1, train_loss: 0.06021066689315964, valid_loss: 0.02220358078678449\nFOLD: 4, EPOCH: 2, train_loss: 0.02338644675910473, valid_loss: 0.02054523614545663\nFOLD: 4, EPOCH: 3, train_loss: 0.02159300577991149, valid_loss: 0.020492285179595154\nFOLD: 4, EPOCH: 4, train_loss: 0.021094833719818032, valid_loss: 0.018243991459409397\nFOLD: 4, EPOCH: 5, train_loss: 0.020848298226209247, valid_loss: 0.0181909433255593\nFOLD: 4, EPOCH: 6, train_loss: 0.02061048522591591, valid_loss: 0.01834863455345233\nFOLD: 4, EPOCH: 7, train_loss: 0.02051970236660803, valid_loss: 0.018222771895428497\nFOLD: 4, EPOCH: 8, train_loss: 0.02041040563627201, valid_loss: 0.017969776255389053\nFOLD: 4, EPOCH: 9, train_loss: 0.020231803660007083, valid_loss: 0.017889668854574364\nFOLD: 4, EPOCH: 10, train_loss: 0.020180293084943995, valid_loss: 0.0180609164138635\nFOLD: 4, EPOCH: 11, train_loss: 0.02010193417834885, valid_loss: 0.01786313485354185\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 12, train_loss: 0.019856616525965577, valid_loss: 0.01792066606382529\nFOLD: 4, EPOCH: 13, train_loss: 0.019589152789729482, valid_loss: 0.01767100083331267\nFOLD: 4, EPOCH: 14, train_loss: 0.019457105601973394, valid_loss: 0.01778119895607233\nFOLD: 4, EPOCH: 15, train_loss: 0.019411768380771664, valid_loss: 0.017576520331203938\nFOLD: 4, EPOCH: 16, train_loss: 0.01935692936839426, valid_loss: 0.017600630099574726\nFOLD: 4, EPOCH: 17, train_loss: 0.01929350652019767, valid_loss: 0.017789790406823158\nFOLD: 4, EPOCH: 18, train_loss: 0.019252404515795848, valid_loss: 0.017757063110669453\nFOLD: 4, EPOCH: 19, train_loss: 0.019234240274218953, valid_loss: 0.017666129705806572\nFOLD: 4, EPOCH: 20, train_loss: 0.019165352315587157, valid_loss: 0.01763876744856437\nFOLD: 4, EPOCH: 21, train_loss: 0.019149793311953545, valid_loss: 0.01774518998960654\nFOLD: 4, EPOCH: 22, train_loss: 0.019150380045175552, valid_loss: 0.017699002288281918\nFOLD: 4, EPOCH: 23, train_loss: 0.01909565569504219, valid_loss: 0.017632472639282543\nFOLD: 4, EPOCH: 24, train_loss: 0.019046892302439493, valid_loss: 0.017607624952991802\nFOLD: 4, EPOCH: 25, train_loss: 0.019038625499781442, valid_loss: 0.017640909180045128\nFOLD: 4, EPOCH: 26, train_loss: 0.019035943838603357, valid_loss: 0.0176234341536959\nFOLD: 4, EPOCH: 27, train_loss: 0.01901128362206852, valid_loss: 0.017564036572972935\nFOLD: 4, EPOCH: 28, train_loss: 0.01898782283944242, valid_loss: 0.01754191145300865\nFOLD: 4, EPOCH: 29, train_loss: 0.018953607581994113, valid_loss: 0.01761225952456395\nFOLD: 4, EPOCH: 30, train_loss: 0.0189362948979525, valid_loss: 0.01760272557536761\nFOLD: 4, EPOCH: 31, train_loss: 0.01893259968389483, valid_loss: 0.01767503594358762\nFOLD: 4, EPOCH: 32, train_loss: 0.01895709313890513, valid_loss: 0.017662265338003635\nFOLD: 4, EPOCH: 33, train_loss: 0.01890726086190518, valid_loss: 0.017707056365907192\nFOLD: 4, EPOCH: 34, train_loss: 0.018853202955249476, valid_loss: 0.01752168219536543\nFOLD: 4, EPOCH: 35, train_loss: 0.01881653070449829, valid_loss: 0.017621535807847977\nFOLD: 4, EPOCH: 36, train_loss: 0.018812292584163302, valid_loss: 0.017598222320278484\nFOLD: 4, EPOCH: 37, train_loss: 0.01878357645781601, valid_loss: 0.017529580742120743\nFOLD: 4, EPOCH: 38, train_loss: 0.018756118986536476, valid_loss: 0.017549364206691582\nFOLD: 4, EPOCH: 39, train_loss: 0.018749810536118114, valid_loss: 0.01763836604853471\nFOLD: 4, EPOCH: 40, train_loss: 0.018754362566944432, valid_loss: 0.01759117313971122\nFOLD: 4, EPOCH: 41, train_loss: 0.018718329039128387, valid_loss: 0.017563780148824055\nFOLD: 4, EPOCH: 42, train_loss: 0.018703165957156348, valid_loss: 0.017545343687136967\nFOLD: 4, EPOCH: 43, train_loss: 0.018655435510856265, valid_loss: 0.017501361357669037\nFOLD: 4, EPOCH: 44, train_loss: 0.018669213332674083, valid_loss: 0.017574056672553223\nFOLD: 4, EPOCH: 45, train_loss: 0.018647653927259585, valid_loss: 0.0174894571925203\nFOLD: 4, EPOCH: 46, train_loss: 0.01862402178127976, valid_loss: 0.017495568220814068\nFOLD: 4, EPOCH: 47, train_loss: 0.018598888617228058, valid_loss: 0.017550046555697918\nFOLD: 4, EPOCH: 48, train_loss: 0.018610379220369982, valid_loss: 0.017468415821592014\nFOLD: 4, EPOCH: 49, train_loss: 0.018565266940961864, valid_loss: 0.01747671173264583\nFOLD: 4, EPOCH: 50, train_loss: 0.018572430564638448, valid_loss: 0.017491964312891167\nFOLD: 4, EPOCH: 51, train_loss: 0.018542092457851943, valid_loss: 0.017504945397377014\nFOLD: 4, EPOCH: 52, train_loss: 0.018531239427187863, valid_loss: 0.017485857009887695\nFOLD: 4, EPOCH: 53, train_loss: 0.018504671259399724, valid_loss: 0.01746967062354088\nFOLD: 4, EPOCH: 54, train_loss: 0.018503402852836776, valid_loss: 0.01747901551425457\nFOLD: 4, EPOCH: 55, train_loss: 0.018509753048419952, valid_loss: 0.01748467329889536\nFOLD: 4, EPOCH: 56, train_loss: 0.018480670035761947, valid_loss: 0.017479486763477325\nFOLD: 4, EPOCH: 57, train_loss: 0.0184766106745776, valid_loss: 0.017480756466587383\nFOLD: 4, EPOCH: 58, train_loss: 0.018485528307364267, valid_loss: 0.01747674277673165\nFOLD: 4, EPOCH: 59, train_loss: 0.01848068888134816, valid_loss: 0.017474878889818985\nBeginning pretraining for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.7292912567363066, valid_loss: 0.6968927880128225\nFOLD: 5, EPOCH: 1, train_loss: 0.6119405846385395, valid_loss: 0.3411096930503845\nFOLD: 5, EPOCH: 2, train_loss: 0.1116663021966815, valid_loss: 0.014648799318820238\nFOLD: 5, EPOCH: 3, train_loss: 0.011973985525615075, valid_loss: 0.006460150548567374\nFOLD: 5, EPOCH: 4, train_loss: 0.009079004884423578, valid_loss: 0.0055644698441028595\nFOLD: 5, EPOCH: 5, train_loss: 0.008588947273571701, valid_loss: 0.005173183589552839\nFOLD: 5, EPOCH: 6, train_loss: 0.0084122112568687, valid_loss: 0.0047420545015484095\nFOLD: 5, EPOCH: 7, train_loss: 0.00836706446374164, valid_loss: 0.004920289774114887\nFOLD: 5, EPOCH: 8, train_loss: 0.008275897803661577, valid_loss: 0.004781343586121996\nFOLD: 5, EPOCH: 9, train_loss: 0.00822913201580591, valid_loss: 0.006818136277919014\nFOLD: 5, EPOCH: 10, train_loss: 0.008185630568357952, valid_loss: 0.005046028643846512\nFOLD: 5, EPOCH: 11, train_loss: 0.008212518248268786, valid_loss: 0.004880927037447691\nFOLD: 5, EPOCH: 12, train_loss: 0.008114512338686515, valid_loss: 0.005012967390939593\nFOLD: 5, EPOCH: 13, train_loss: 0.008068717673749608, valid_loss: 0.004820932634174824\nFOLD: 5, EPOCH: 14, train_loss: 0.008052344447659218, valid_loss: 0.005303807090967894\nFOLD: 5, EPOCH: 15, train_loss: 0.00804881244788275, valid_loss: 0.004912897789229949\nFOLD: 5, EPOCH: 16, train_loss: 0.007995617658118992, valid_loss: 0.004827047465369105\nFOLD: 5, EPOCH: 17, train_loss: 0.007951995152432252, valid_loss: 0.00510421108144025\nFOLD: 5, EPOCH: 18, train_loss: 0.007947693934992832, valid_loss: 0.005120838759467006\nFOLD: 5, EPOCH: 19, train_loss: 0.007941694587797803, valid_loss: 0.004703945480287075\nFOLD: 5, EPOCH: 20, train_loss: 0.007925801431102789, valid_loss: 0.0049448267091065645\nFOLD: 5, EPOCH: 21, train_loss: 0.007932841805193354, valid_loss: 0.005034906168778737\nFOLD: 5, EPOCH: 22, train_loss: 0.007913779344081003, valid_loss: 0.004800089402124286\nFOLD: 5, EPOCH: 23, train_loss: 0.007895510479369584, valid_loss: 0.004869084882860382\nFOLD: 5, EPOCH: 24, train_loss: 0.007901416119078504, valid_loss: 0.00510324848194917\nBeginning training for fold 5\nFOLD: 5, EPOCH: 0, train_loss: 0.5355793242068851, valid_loss: 0.1617403601606687\nFOLD: 5, EPOCH: 1, train_loss: 0.056411336450015795, valid_loss: 0.022696002076069515\nFOLD: 5, EPOCH: 2, train_loss: 0.023698813884573823, valid_loss: 0.019977619250615437\nFOLD: 5, EPOCH: 3, train_loss: 0.02194572163417059, valid_loss: 0.019309138568739097\nFOLD: 5, EPOCH: 4, train_loss: 0.021408598815255305, valid_loss: 0.018954743010302384\nFOLD: 5, EPOCH: 5, train_loss: 0.021084848353091407, valid_loss: 0.01883729516218106\nFOLD: 5, EPOCH: 6, train_loss: 0.020853550830746397, valid_loss: 0.018295415366689365\nFOLD: 5, EPOCH: 7, train_loss: 0.02067306950030958, valid_loss: 0.018308038512865703\nFOLD: 5, EPOCH: 8, train_loss: 0.020603557774687514, valid_loss: 0.018091293672720592\nFOLD: 5, EPOCH: 9, train_loss: 0.020498686744009748, valid_loss: 0.018218715364734333\nFOLD: 5, EPOCH: 10, train_loss: 0.0203835654675084, valid_loss: 0.018163302602867287\nFOLD: 5, EPOCH: 11, train_loss: 0.020225545062738305, valid_loss: 0.017993684237202007\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 12, train_loss: 0.020025712164009318, valid_loss: 0.01783746760338545\nFOLD: 5, EPOCH: 13, train_loss: 0.019751462318441448, valid_loss: 0.01779653411358595\nFOLD: 5, EPOCH: 14, train_loss: 0.019698192968088037, valid_loss: 0.018266102609535057\nFOLD: 5, EPOCH: 15, train_loss: 0.019608816515435192, valid_loss: 0.017830202355980873\nFOLD: 5, EPOCH: 16, train_loss: 0.01959056215470328, valid_loss: 0.018106309076150257\nFOLD: 5, EPOCH: 17, train_loss: 0.019532372353269774, valid_loss: 0.01781735507150491\nFOLD: 5, EPOCH: 18, train_loss: 0.019459704539793378, valid_loss: 0.01778990433861812\nFOLD: 5, EPOCH: 19, train_loss: 0.019458089023828506, valid_loss: 0.017794442052642506\nFOLD: 5, EPOCH: 20, train_loss: 0.01944550937589477, valid_loss: 0.017853932455182076\nFOLD: 5, EPOCH: 21, train_loss: 0.01940709040226305, valid_loss: 0.01770944365610679\nFOLD: 5, EPOCH: 22, train_loss: 0.019394251737086213, valid_loss: 0.017764220635096233\nFOLD: 5, EPOCH: 23, train_loss: 0.01934901745442082, valid_loss: 0.017655376655360062\nFOLD: 5, EPOCH: 24, train_loss: 0.01931352518937167, valid_loss: 0.01778780637929837\nFOLD: 5, EPOCH: 25, train_loss: 0.019299249876947963, valid_loss: 0.01766987486431996\nFOLD: 5, EPOCH: 26, train_loss: 0.019281214157886365, valid_loss: 0.017925746738910675\nFOLD: 5, EPOCH: 27, train_loss: 0.019270522465162417, valid_loss: 0.01770591891060273\nFOLD: 5, EPOCH: 28, train_loss: 0.019252386985017973, valid_loss: 0.018002013986309368\nFOLD: 5, EPOCH: 29, train_loss: 0.0192047478083302, valid_loss: 0.017747584730386734\nFOLD: 5, EPOCH: 30, train_loss: 0.019242633079342982, valid_loss: 0.017790253274142742\nFOLD: 5, EPOCH: 31, train_loss: 0.019219998653758976, valid_loss: 0.01787625625729561\nFOLD: 5, EPOCH: 32, train_loss: 0.019176654134164837, valid_loss: 0.017701819228629272\nFOLD: 5, EPOCH: 33, train_loss: 0.019135712262462166, valid_loss: 0.017644586352010567\nFOLD: 5, EPOCH: 34, train_loss: 0.019149325021049556, valid_loss: 0.017797273273269337\nFOLD: 5, EPOCH: 35, train_loss: 0.01909667220624054, valid_loss: 0.017618874087929726\nFOLD: 5, EPOCH: 36, train_loss: 0.01911025308072567, valid_loss: 0.017720627288023632\nFOLD: 5, EPOCH: 37, train_loss: 0.019083104151136735, valid_loss: 0.017712649578849476\nFOLD: 5, EPOCH: 38, train_loss: 0.019090447465286535, valid_loss: 0.01768818652878205\nFOLD: 5, EPOCH: 39, train_loss: 0.019052252447342172, valid_loss: 0.017646355864902336\nFOLD: 5, EPOCH: 40, train_loss: 0.019033854936852175, valid_loss: 0.01758406311273575\nFOLD: 5, EPOCH: 41, train_loss: 0.01901663642595796, valid_loss: 0.017610278601447742\nFOLD: 5, EPOCH: 42, train_loss: 0.019003093954833114, valid_loss: 0.017679929733276367\nFOLD: 5, EPOCH: 43, train_loss: 0.018975962994291502, valid_loss: 0.017735669389367104\nFOLD: 5, EPOCH: 44, train_loss: 0.018953751389156368, valid_loss: 0.017673382225135963\nFOLD: 5, EPOCH: 45, train_loss: 0.018944941570653635, valid_loss: 0.017723646325369675\nFOLD: 5, EPOCH: 46, train_loss: 0.018935809076270637, valid_loss: 0.017677324824035168\nFOLD: 5, EPOCH: 47, train_loss: 0.018887794795720017, valid_loss: 0.017598089451591175\nFOLD: 5, EPOCH: 48, train_loss: 0.018882099045988393, valid_loss: 0.017629235051572323\nFOLD: 5, EPOCH: 49, train_loss: 0.018897605950341505, valid_loss: 0.017638441485663254\nFOLD: 5, EPOCH: 50, train_loss: 0.018852887839517173, valid_loss: 0.017625168586770695\nFOLD: 5, EPOCH: 51, train_loss: 0.018848410478847867, valid_loss: 0.017590186248222988\nFOLD: 5, EPOCH: 52, train_loss: 0.01884623748414657, valid_loss: 0.01763172633945942\nFOLD: 5, EPOCH: 53, train_loss: 0.018828948137953, valid_loss: 0.0175991989672184\nFOLD: 5, EPOCH: 54, train_loss: 0.01881037449792904, valid_loss: 0.01761892717331648\nFOLD: 5, EPOCH: 55, train_loss: 0.01881642924512134, valid_loss: 0.01761340784529845\nFOLD: 5, EPOCH: 56, train_loss: 0.01878761357682593, valid_loss: 0.017618737804392975\nFOLD: 5, EPOCH: 57, train_loss: 0.018827194074059233, valid_loss: 0.017617331196864445\nFOLD: 5, EPOCH: 58, train_loss: 0.018816053100368556, valid_loss: 0.017615583414832752\nFOLD: 5, EPOCH: 59, train_loss: 0.018803102895617485, valid_loss: 0.01760888161758582\nBeginning pretraining for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.7293419382151436, valid_loss: 0.6959552963574728\nFOLD: 6, EPOCH: 1, train_loss: 0.6127038063371882, valid_loss: 0.34074899057547253\nFOLD: 6, EPOCH: 2, train_loss: 0.1116049084264566, valid_loss: 0.014337006180236736\nFOLD: 6, EPOCH: 3, train_loss: 0.012012952144312508, valid_loss: 0.006307019929712017\nFOLD: 6, EPOCH: 4, train_loss: 0.009094937348409611, valid_loss: 0.005387895662958424\nFOLD: 6, EPOCH: 5, train_loss: 0.008556456579005016, valid_loss: 0.004913001321256161\nFOLD: 6, EPOCH: 6, train_loss: 0.008374229498098002, valid_loss: 0.004752583258474867\nFOLD: 6, EPOCH: 7, train_loss: 0.008307126515051898, valid_loss: 0.00463368014122049\nFOLD: 6, EPOCH: 8, train_loss: 0.008245184853234711, valid_loss: 0.004532006258765857\nFOLD: 6, EPOCH: 9, train_loss: 0.008199893266839139, valid_loss: 0.005049875083689888\nFOLD: 6, EPOCH: 10, train_loss: 0.008168041788260727, valid_loss: 0.004616795340552926\nFOLD: 6, EPOCH: 11, train_loss: 0.008109585149213672, valid_loss: 0.0047124958752344055\nFOLD: 6, EPOCH: 12, train_loss: 0.008075569194796331, valid_loss: 0.004842403422420223\nFOLD: 6, EPOCH: 13, train_loss: 0.008027799824690995, valid_loss: 0.004848361248150468\nFOLD: 6, EPOCH: 14, train_loss: 0.008008013205493198, valid_loss: 0.004705719572181503\nFOLD: 6, EPOCH: 15, train_loss: 0.007990229203749229, valid_loss: 0.004978191650783022\nFOLD: 6, EPOCH: 16, train_loss: 0.007961225531557026, valid_loss: 0.004580941361685594\nFOLD: 6, EPOCH: 17, train_loss: 0.007940104258630206, valid_loss: 0.004880819392080109\nFOLD: 6, EPOCH: 18, train_loss: 0.007922323461731566, valid_loss: 0.0045668187861641245\nFOLD: 6, EPOCH: 19, train_loss: 0.00792519620838849, valid_loss: 0.0046789882859836025\nFOLD: 6, EPOCH: 20, train_loss: 0.007896897793911836, valid_loss: 0.004666116709510486\nFOLD: 6, EPOCH: 21, train_loss: 0.007913563140284489, valid_loss: 0.004632336863627036\nFOLD: 6, EPOCH: 22, train_loss: 0.007899723125292975, valid_loss: 0.00457352624895672\nFOLD: 6, EPOCH: 23, train_loss: 0.007909375790725736, valid_loss: 0.004892543191090226\nFOLD: 6, EPOCH: 24, train_loss: 0.007913251503315918, valid_loss: 0.004566218703985214\nBeginning training for fold 6\nFOLD: 6, EPOCH: 0, train_loss: 0.5590728376718128, valid_loss: 0.19081935534874597\nFOLD: 6, EPOCH: 1, train_loss: 0.06462573008063961, valid_loss: 0.02429551227639119\nFOLD: 6, EPOCH: 2, train_loss: 0.023813228780294165, valid_loss: 0.02000295091420412\nFOLD: 6, EPOCH: 3, train_loss: 0.021708879455485764, valid_loss: 0.019431973497072857\nFOLD: 6, EPOCH: 4, train_loss: 0.021095039980376467, valid_loss: 0.01880427139500777\nFOLD: 6, EPOCH: 5, train_loss: 0.020879883100004756, valid_loss: 0.01818686289091905\nFOLD: 6, EPOCH: 6, train_loss: 0.020543016602887827, valid_loss: 0.01757850870490074\nFOLD: 6, EPOCH: 7, train_loss: 0.020411690751857618, valid_loss: 0.01762498077005148\nFOLD: 6, EPOCH: 8, train_loss: 0.02032135027077268, valid_loss: 0.01733464685579141\nFOLD: 6, EPOCH: 9, train_loss: 0.020198874997303766, valid_loss: 0.017491352123518784\nFOLD: 6, EPOCH: 10, train_loss: 0.020204452195150012, valid_loss: 0.017734013808270294\nFOLD: 6, EPOCH: 11, train_loss: 0.020120815757442925, valid_loss: 0.017476575449109077\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 12, train_loss: 0.02006611879914999, valid_loss: 0.017211657327910263\nFOLD: 6, EPOCH: 13, train_loss: 0.019759754212025332, valid_loss: 0.0170768890529871\nFOLD: 6, EPOCH: 14, train_loss: 0.019569992306916154, valid_loss: 0.017654080875217915\nFOLD: 6, EPOCH: 15, train_loss: 0.019513867795467377, valid_loss: 0.017104720696806908\nFOLD: 6, EPOCH: 16, train_loss: 0.019476577146526647, valid_loss: 0.01725658494979143\nFOLD: 6, EPOCH: 17, train_loss: 0.019410113037070808, valid_loss: 0.01716448025157054\nFOLD: 6, EPOCH: 18, train_loss: 0.019402327837751192, valid_loss: 0.01709790714085102\nFOLD: 6, EPOCH: 19, train_loss: 0.019291500177453545, valid_loss: 0.017530908808112144\nFOLD: 6, EPOCH: 20, train_loss: 0.01927839553750613, valid_loss: 0.017147439221541088\nFOLD: 6, EPOCH: 21, train_loss: 0.01924981050850714, valid_loss: 0.0170405146976312\nFOLD: 6, EPOCH: 22, train_loss: 0.01923581344239852, valid_loss: 0.017126174333194893\nFOLD: 6, EPOCH: 23, train_loss: 0.01915699616074562, valid_loss: 0.017239383111397426\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 24, train_loss: 0.019155836773707587, valid_loss: 0.016991359181702137\nFOLD: 6, EPOCH: 25, train_loss: 0.018921409678809783, valid_loss: 0.017089003697037697\nFOLD: 6, EPOCH: 26, train_loss: 0.01888398497420199, valid_loss: 0.017058505366245907\nFOLD: 6, EPOCH: 27, train_loss: 0.01881035220097093, valid_loss: 0.017098914210995037\nFOLD: 6, EPOCH: 28, train_loss: 0.018755709807224134, valid_loss: 0.017054495091239612\nFOLD: 6, EPOCH: 29, train_loss: 0.018727426033686188, valid_loss: 0.01691309455782175\nFOLD: 6, EPOCH: 30, train_loss: 0.018651502645191026, valid_loss: 0.01710886290917794\nFOLD: 6, EPOCH: 31, train_loss: 0.018642618540017045, valid_loss: 0.017050322766105335\nFOLD: 6, EPOCH: 32, train_loss: 0.018620419918614274, valid_loss: 0.01703469455242157\nFOLD: 6, EPOCH: 33, train_loss: 0.018605294034761542, valid_loss: 0.017036380246281624\nFOLD: 6, EPOCH: 34, train_loss: 0.01855401475639904, valid_loss: 0.016934373416006565\nFOLD: 6, EPOCH: 35, train_loss: 0.018529491115580585, valid_loss: 0.01688784857590993\nFOLD: 6, EPOCH: 36, train_loss: 0.018486776443965295, valid_loss: 0.016895529193182785\nFOLD: 6, EPOCH: 37, train_loss: 0.018452284090659198, valid_loss: 0.016833683165411156\nFOLD: 6, EPOCH: 38, train_loss: 0.018430133733679268, valid_loss: 0.01694078091531992\nFOLD: 6, EPOCH: 39, train_loss: 0.018431178841959026, valid_loss: 0.016955281607806683\nFOLD: 6, EPOCH: 40, train_loss: 0.018389038248535466, valid_loss: 0.016923711945613224\nFOLD: 6, EPOCH: 41, train_loss: 0.01840122274177916, valid_loss: 0.016997320267061394\nFOLD: 6, EPOCH: 42, train_loss: 0.018332506036933732, valid_loss: 0.01691144083937009\nFOLD: 6, EPOCH: 43, train_loss: 0.01830761057927328, valid_loss: 0.01685988251119852\nFOLD: 6, EPOCH: 44, train_loss: 0.0182868997730753, valid_loss: 0.016892694557706516\nFOLD: 6, EPOCH: 45, train_loss: 0.018242236226797104, valid_loss: 0.016912016396721203\nFOLD: 6, EPOCH: 46, train_loss: 0.018215797622414195, valid_loss: 0.01687289463977019\nFOLD: 6, EPOCH: 47, train_loss: 0.018212609540890243, valid_loss: 0.01681762918209036\nFOLD: 6, EPOCH: 48, train_loss: 0.01818752392907353, valid_loss: 0.016862138795355957\nFOLD: 6, EPOCH: 49, train_loss: 0.018172689001349843, valid_loss: 0.016829710453748703\nFOLD: 6, EPOCH: 50, train_loss: 0.018143230063073775, valid_loss: 0.016890608395139377\nFOLD: 6, EPOCH: 51, train_loss: 0.01813174899229232, valid_loss: 0.016849454026669264\nFOLD: 6, EPOCH: 52, train_loss: 0.018110917607212767, valid_loss: 0.01683720201253891\nFOLD: 6, EPOCH: 53, train_loss: 0.0181222198640599, valid_loss: 0.016838194336742163\nFOLD: 6, EPOCH: 54, train_loss: 0.018104653093306458, valid_loss: 0.016837520990520716\nFOLD: 6, EPOCH: 55, train_loss: 0.018079299315371933, valid_loss: 0.016849078548451264\nFOLD: 6, EPOCH: 56, train_loss: 0.0180837793604416, valid_loss: 0.01682964339852333\nFOLD: 6, EPOCH: 57, train_loss: 0.01806504323202021, valid_loss: 0.01684628085543712\nFOLD: 6, EPOCH: 58, train_loss: 0.018072710591642296, valid_loss: 0.016840153994659584\nFOLD: 6, EPOCH: 59, train_loss: 0.018079826279598123, valid_loss: 0.016833005007356405\n"
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "#SEED = [0,1,2,3,4,5,6] #<-- Update\n",
    "SEED = [0]\n",
    "oof = np.zeros((len(train), len(target_scored_cols)))\n",
    "predictions = np.zeros((len(test), len(target_scored_cols)))\n",
    "\n",
    "# mean_scored = np.mean(train[target_scored_cols].values,axis=0)\n",
    "# mean_nscored = np.mean(train[target_nscored_cols].values,axis=0)\n",
    "# pos_scored_rate = np.log(np.where(mean_scored==0, 1e-8, mean_scored))\n",
    "# pos_nscored_rate = np.log(np.where(mean_nscored==0, 1e-8, mean_nscored))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_scored_cols] = oof\n",
    "test[target_scored_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV log_loss:  0.01585970108953215\n"
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_scored_cols).merge(train[['sig_id']+target_scored_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "y_true = train_targets_scored[target_scored_cols].values\n",
    "y_pred = valid_results[target_scored_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_scored_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target_scored.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6563924422632783\n"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline - CV log_loss:  0.01592192106436285 - 0.6481409205142853\n",
    "#CV log_loss:  0.01592196808239733 - 0.6467970096134422\n",
    "#CV log_loss:  0.015910805622295764 - 0.6536475888580155 epoch 40\n",
    "#CV log_loss:  0.01589769241561799 - 0.6557716465832143 epoch 45\n",
    "#CV log_loss:  0.015891434988748915 - 0.6606290483745609 epoch 50\n",
    "#CV log_loss:  0.01588715803509738 - 0.6583277267355544 epoch 55\n",
    "#CV log_loss:  0.01588569318599064 - 0.6628654461934201 epoch 60\n",
    "#CV log_loss:  0.01590611275187904 - 0.6557353505850225 epoch 65\n",
    "\n",
    "\n",
    "#CV log_loss:  0.01586760697874445 - 0.6593208836774367 epochj 60  [1200,900]\n",
    "#CV log_loss:  0.01585970108953215 - 0.6563924422632783 epoch 60 [1300,900]\n",
    "\n",
    "\n",
    "#CV log_loss:  0.01589648288882382 - 0.6508185055047814 (qnatile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"model_1bis_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}