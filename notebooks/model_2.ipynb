{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1604087787070",
   "display_name": "Python 3.7.9 64-bit ('moa_kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['train_targets_scored.csv',\n 'sample_submission.csv',\n '.gitkeep',\n 'train_features.csv',\n 'test_features.csv',\n 'train_targets_nonscored.csv']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "data_dir = '../data/01_raw'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(data_dir+'/train_features.csv')\n",
    "train_targets_scored = pd.read_csv(data_dir+'/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(data_dir+'/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv(data_dir+'/test_features.csv')\n",
    "sample_submission = pd.read_csv(data_dir+'/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Indiquer si valeur dans le range max, min\n",
    "\n",
    "# import seaborn as sns\n",
    "# data = pd.concat([train_features,test_features],axis=0)\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"ctl_vehicle\"][\"c-4\"],label=\"normal\")\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"trt_cp\"][\"c-4\"],label=\"treated\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets = train_targets_scored[[c for c in train_targets_scored.columns if (c != \"sig_id\")]].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_inhibitor\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_activator\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_agonist\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_antagonist\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# train_features[GENES].apply(lambda x : stats.moment(x,moment=5),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RankGauss\n",
    "\n",
    "for col in (GENES + CELLS):\n",
    "\n",
    "    transformer = QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n",
    "    vec_len = len(train_features[col].values)\n",
    "    vec_len_test = len(test_features[col].values)\n",
    "    raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENES\n",
    "n_comp = 600  #<--Update\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELLS\n",
    "n_comp = 50  #<--Update\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(23814, 1526)"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = GENES\n",
    "    features_c = CELLS\n",
    "    \n",
    "    for df in train, test:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1) ## <==\n",
    "        # df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1) ## <==\n",
    "        # df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        #df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        # df['g_q25'] = df[features_g].quantile(q=.25,axis = 1)\n",
    "        # df['g_q50'] = df[features_g].quantile(q=.5,axis = 1)\n",
    "        # df['g_q75'] = df[features_g].quantile(q=.75,axis = 1)\n",
    "        #df['g_var'] = df[features_g].apply(axis=1,func=stats.variation)\n",
    "        # df['g_mad'] = df[features_g].mad(axis = 1)\n",
    "\n",
    "\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1) ## <==\n",
    "        # df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1) ## <==\n",
    "        # df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        #df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        # df['c_q25'] = df[features_c].quantile(q=.25,axis = 1)\n",
    "        # df['c_q50'] = df[features_c].quantile(q=.5,axis = 1)\n",
    "        # df['c_q75'] = df[features_c].quantile(q=.75,axis = 1)\n",
    "        # df['c_var'] = df[features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['c_mad'] = df[features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1) ## <==\n",
    "        # df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        # df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        # df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        # df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        # df['gc_q25'] = df[features_g + features_c].quantile(q=.25,axis = 1)\n",
    "        # df['gc_q50'] = df[features_g + features_c].quantile(q=.5,axis = 1)\n",
    "        # df['gc_q75'] = df[features_g + features_c].quantile(q=.75,axis = 1)\n",
    "        # df['gc_var'] = df[features_g + features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['gc_mad'] = df[features_g + features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "train_features,test_features=fe_stats(train_features,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features_gc = train_features[[\"sig_id\"]+GENES+CELLS].copy()\n",
    "# test_features_gc = test_features[[\"sig_id\"]+GENES+CELLS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(23814, 1043)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "var_thresh = VarianceThreshold(0.8)  #<-- Update\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# def fe_cluster(train, test, n_clusters_g = 15, n_clusters_c = 5, SEED = 42):\n",
    "    \n",
    "#     features_g = GENES\n",
    "#     features_c = CELLS\n",
    "    \n",
    "#     def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n",
    "#         train_ = train[features].copy()\n",
    "#         test_ = test[features].copy()\n",
    "#         data = pd.concat([train_, test_], axis = 0)\n",
    "#         kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n",
    "#         train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n",
    "#         test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n",
    "#         train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n",
    "#         test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n",
    "#         return train, test\n",
    "    \n",
    "#     #train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n",
    "#     train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n",
    "#     return train, test\n",
    "\n",
    "# train_features_gc ,test_features_gc=fe_cluster(train_features_gc,test_features_gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "def fe_cluster2(train, test, n_clusters = 3, SEED = 42):\n",
    "    \n",
    "\n",
    "    def create_cluster(train, test, n_clusters = n_clusters):\n",
    "        train_ = train.copy()\n",
    "        test_ = test.copy()\n",
    "        data = pd.concat([train_, test_], axis = 0)\n",
    "        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data[[c for c in data.columns if c not in [\"sig_id\",\"cp_type\",\"cp_dose\",\"cp_time\"]]])\n",
    "        train['cluster'] = kmeans.labels_[:train.shape[0]]\n",
    "        test['cluster'] = kmeans.labels_[train.shape[0]:]\n",
    "        train = pd.get_dummies(train, columns = ['cluster'])\n",
    "        test = pd.get_dummies(test, columns = ['cluster'])\n",
    "        return train, test\n",
    "    \n",
    "    train, test = create_cluster(train, test, n_clusters = n_clusters)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "\n",
    "train_features,test_features=fe_cluster2(train_features,test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([train1, test1], axis = 0)\n",
    "\n",
    "# distortion = []\n",
    "# for k in range(1,10):\n",
    "#     kmeans = KMeans(n_clusters = k, random_state = 42).fit(data)\n",
    "#     distortion += [kmeans.inertia_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(1,10),distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = train_features.merge(train_features_gc.loc[:,[col for col in train_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")\n",
    "# test_features = test_features.merge(test_features_gc.loc[:,[col for col in test_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(23814, 1046)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train.merge(train_targets_nonscored, on='sig_id')\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target_scored = train[train_targets_scored.columns]\n",
    "target_nscored = train[train_targets_nonscored.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scored_cols = target_scored.drop('sig_id', axis=1).columns.values.tolist()\n",
    "target_nscored_cols = target_nscored.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "#from pytorch_tabnet import tab_network\n",
    "from pytorch_tabnet.multiclass_utils import unique_labels\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from pytorch_tabnet.utils import (PredictDataset,\n",
    "                                  create_explain_matrix)\n",
    "from sklearn.base import BaseEstimator\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import io\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "def log_loss_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n",
    "    return results.mean()\n",
    "\n",
    "def log_loss_score(actual, predicted,  eps=1e-15):\n",
    "\n",
    "        \"\"\"\n",
    "        :param predicted:   The predicted probabilities as floats between 0-1\n",
    "        :param actual:      The binary labels. Either 0 or 1.\n",
    "        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n",
    "        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        p1 = actual * np.log(predicted+eps)\n",
    "        p0 = (1-actual) * np.log(1-predicted+eps)\n",
    "        loss = p0 + p1\n",
    "\n",
    "        return -loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Format for numpy array\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        The input matrix\n",
    "    y : 2D array\n",
    "        The one-hot encoded target\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y1,y2=None):\n",
    "        self.x = x\n",
    "        self.y1 = y1\n",
    "        self.y2 = y2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y1,y2 = self.x[index], self.y1[index], self.y2[index]\n",
    "        return x, y1,y2\n",
    "\n",
    "class ValidDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Format for numpy array\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        The input matrix\n",
    "    y : 2D array\n",
    "        The one-hot encoded target\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y1):\n",
    "        self.x = x\n",
    "        self.y1 = y1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y1 = self.x[index], self.y1[index]\n",
    "        return x, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    X_train, y_scored_train,y_nscored_train, X_valid,y_valid, weights, batch_size, num_workers, drop_last, pin_memory=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Create dataloaders with or wihtout subsampling depending on weights and balanced.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.ndarray\n",
    "        Training data\n",
    "    y_train : np.array\n",
    "        Mapped Training targets\n",
    "    eval_set : list of tuple\n",
    "        List of eval tuple set (X, y)\n",
    "    weights : either 0, 1, dict or iterable\n",
    "        if 0 (default) : no weights will be applied\n",
    "        if 1 : classification only, will balanced class with inverse frequency\n",
    "        if dict : keys are corresponding class values are sample weights\n",
    "        if iterable : list or np array must be of length equal to nb elements\n",
    "                      in the training set\n",
    "    batch_size : int\n",
    "        how many samples per batch to load\n",
    "    num_workers : int\n",
    "        how many subprocesses to use for data loading. 0 means that the data\n",
    "        will be loaded in the main process\n",
    "    drop_last : bool\n",
    "        set to True to drop the last incomplete batch, if the dataset size is not\n",
    "        divisible by the batch size. If False and the size of dataset is not\n",
    "        divisible by the batch size, then the last batch will be smaller\n",
    "    pin_memory : bool\n",
    "        Whether to pin GPU memory during training\n",
    "    Returns\n",
    "    -------\n",
    "    train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "        Training and validation dataloaders\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(weights, int):\n",
    "        if weights == 0:\n",
    "            need_shuffle = True\n",
    "            sampler = None\n",
    "        elif weights == 1:\n",
    "            need_shuffle = False\n",
    "            class_sample_count = np.array(\n",
    "                [len(np.where(y_train == t)[0]) for t in np.unique(y_scored_train)]\n",
    "            )\n",
    "\n",
    "            weights = 1.0 / class_sample_count\n",
    "\n",
    "            samples_weight = np.array([weights[t] for t in y_scored_train])\n",
    "\n",
    "            samples_weight = torch.from_numpy(samples_weight)\n",
    "            samples_weight = samples_weight.double()\n",
    "            sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        else:\n",
    "            raise ValueError(\"Weights should be either 0, 1, dictionnary or list.\")\n",
    "    elif isinstance(weights, dict):\n",
    "        # custom weights per class\n",
    "        need_shuffle = False\n",
    "        samples_weight = np.array([weights[t] for t in y_scored_train])\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    else:\n",
    "        # custom weights\n",
    "        if len(weights) != len(y_scored_train):\n",
    "            raise ValueError(\"Custom weights should match number of train samples.\")\n",
    "        need_shuffle = False\n",
    "        samples_weight = np.array(weights)\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        TrainDataset(X_train.astype(np.float32), y_scored_train, y_nscored_train),\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        shuffle=need_shuffle,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=drop_last,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    # valid_dataloaders = []\n",
    "    # for X, y in [(X_valid,y_valid)]:\n",
    "    #     valid_dataloaders.append(\n",
    "    #         DataLoader(\n",
    "    #             ValidDataset(X.astype(np.float32), y),\n",
    "    #             batch_size=batch_size,\n",
    "    #             shuffle=False,\n",
    "    #             num_workers=num_workers,\n",
    "    #             pin_memory=pin_memory\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    valid_dataloaders = DataLoader(\n",
    "        ValidDataset(X_valid.astype(np.float32), y_valid),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "\n",
    "    return train_dataloader, valid_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU\n",
    "import numpy as np\n",
    "from pytorch_tabnet import sparsemax\n",
    "\n",
    "\n",
    "def initialize_non_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(4*input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "def initialize_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "class GBN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Ghost Batch Normalization\n",
    "        https://arxiv.org/abs/1705.08741\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, virtual_batch_size=128, momentum=0.01):\n",
    "        super(GBN, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.bn = BatchNorm1d(self.input_dim, momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
    "        res = [self.bn(x_) for x_ in chunks]\n",
    "\n",
    "        return torch.cat(res, dim=0)\n",
    "\n",
    "\n",
    "class TabNetNoEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim1,output_dim2,\n",
    "                 n_d=8, n_a=8,\n",
    "                 n_steps=3, gamma=1.3,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,\n",
    "                 virtual_batch_size=128, momentum=0.02,\n",
    "                 mask_type=\"sparsemax\"):\n",
    "        \"\"\"\n",
    "        Defines main part of the TabNet network without the embedding layers.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Number of features\n",
    "        output_dim : int or list of int for multi task classification\n",
    "            Dimension of network output\n",
    "            examples : one for regression, 2 for binary classification etc...\n",
    "        n_d : int\n",
    "            Dimension of the prediction  layer (usually between 4 and 64)\n",
    "        n_a : int\n",
    "            Dimension of the attention  layer (usually between 4 and 64)\n",
    "        n_steps : int\n",
    "            Number of sucessive steps in the newtork (usually betwenn 3 and 10)\n",
    "        gamma : float\n",
    "            Float above 1, scaling factor for attention updates (usually betwenn 1.0 to 2.0)\n",
    "        n_independent : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        n_shared : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        epsilon : float\n",
    "            Avoid log(0), this should be kept very low\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in all batch norm\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(TabNetNoEmbeddings, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "        self.is_multi_task = isinstance(output_dim1, list)\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.mask_type = mask_type\n",
    "        self.initial_bn = BatchNorm1d(self.input_dim, momentum=0.01)\n",
    "\n",
    "        if self.n_shared > 0:\n",
    "            shared_feat_transform = torch.nn.ModuleList()\n",
    "            for i in range(self.n_shared):\n",
    "                if i == 0:\n",
    "                    shared_feat_transform.append(Linear(self.input_dim,\n",
    "                                                        2*(n_d + n_a),\n",
    "                                                        bias=False))\n",
    "                else:\n",
    "                    shared_feat_transform.append(Linear(n_d + n_a, 2*(n_d + n_a), bias=False))\n",
    "\n",
    "        else:\n",
    "            shared_feat_transform = None\n",
    "\n",
    "        self.initial_splitter = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
    "                                                n_glu_independent=self.n_independent,\n",
    "                                                virtual_batch_size=self.virtual_batch_size,\n",
    "                                                momentum=momentum)\n",
    "\n",
    "        self.feat_transformers = torch.nn.ModuleList()\n",
    "        self.att_transformers = torch.nn.ModuleList()\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            transformer = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
    "                                          n_glu_independent=self.n_independent,\n",
    "                                          virtual_batch_size=self.virtual_batch_size,\n",
    "                                          momentum=momentum)\n",
    "            attention = AttentiveTransformer(n_a, self.input_dim,\n",
    "                                             virtual_batch_size=self.virtual_batch_size,\n",
    "                                             momentum=momentum,\n",
    "                                             mask_type=self.mask_type)\n",
    "            self.feat_transformers.append(transformer)\n",
    "            self.att_transformers.append(attention)\n",
    "\n",
    "        if self.is_multi_task:\n",
    "            self.multi_task_mappings1 = torch.nn.ModuleList()\n",
    "            for task_dim in output_dim1:\n",
    "                task_mapping = Linear(n_d, task_dim, bias=False)\n",
    "                initialize_non_glu(task_mapping, n_d, task_dim)\n",
    "                self.multi_task_mappings1.append(task_mapping)\n",
    "\n",
    "            self.multi_task_mappings2 = torch.nn.ModuleList()\n",
    "            for task_dim in output_dim2:\n",
    "                task_mapping = Linear(n_d, task_dim, bias=False)\n",
    "                initialize_non_glu(task_mapping, n_d, task_dim)\n",
    "                self.multi_task_mappings2.append(task_mapping)\n",
    "        else:\n",
    "            self.final_mapping1 = Linear(n_d, output_dim1, bias=False)\n",
    "            initialize_non_glu(self.final_mapping1, n_d, output_dim1)\n",
    "            self.final_mapping2 = Linear(n_d, output_dim2, bias=False)\n",
    "            initialize_non_glu(self.final_mapping2, n_d, output_dim2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = 0\n",
    "        x = self.initial_bn(x)\n",
    "\n",
    "        prior = torch.ones(x.shape).to(x.device)\n",
    "        M_loss = 0\n",
    "        att = self.initial_splitter(x)[:, self.n_d:]\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            M = self.att_transformers[step](prior, att)\n",
    "            M_loss += torch.mean(torch.sum(torch.mul(M, torch.log(M+self.epsilon)),\n",
    "                                           dim=1))\n",
    "            # update prior\n",
    "            prior = torch.mul(self.gamma - M, prior)\n",
    "            # output\n",
    "            masked_x = torch.mul(M, x)\n",
    "            out = self.feat_transformers[step](masked_x)\n",
    "            d = ReLU()(out[:, :self.n_d])\n",
    "            res = torch.add(res, d)\n",
    "            # update attention\n",
    "            att = out[:, self.n_d:]\n",
    "\n",
    "        M_loss /= self.n_steps\n",
    "\n",
    "        if self.is_multi_task:\n",
    "            # Result will be in list format\n",
    "            out1 = []\n",
    "            for task_mapping in self.multi_task_mappings1:\n",
    "                out1.append(task_mapping(res))\n",
    "            out2 = []\n",
    "            for task_mapping in self.multi_task_mappings2:\n",
    "                out2.append(task_mapping(res))\n",
    "        else:\n",
    "            out1 = self.final_mapping1(res)\n",
    "            out2 = self.final_mapping2(res)\n",
    "        return out1,out2, M_loss\n",
    "\n",
    "    def forward_masks(self, x):\n",
    "        x = self.initial_bn(x)\n",
    "\n",
    "        prior = torch.ones(x.shape).to(x.device)\n",
    "        M_explain = torch.zeros(x.shape).to(x.device)\n",
    "        att = self.initial_splitter(x)[:, self.n_d:]\n",
    "        masks = {}\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            M = self.att_transformers[step](prior, att)\n",
    "            masks[step] = M\n",
    "            # update prior\n",
    "            prior = torch.mul(self.gamma - M, prior)\n",
    "            # output\n",
    "            masked_x = torch.mul(M, x)\n",
    "            out = self.feat_transformers[step](masked_x)\n",
    "            d = ReLU()(out[:, :self.n_d])\n",
    "            # explain\n",
    "            step_importance = torch.sum(d, dim=1)\n",
    "            M_explain += torch.mul(M, step_importance.unsqueeze(dim=1))\n",
    "            # update attention\n",
    "            att = out[:, self.n_d:]\n",
    "\n",
    "        return M_explain, masks\n",
    "\n",
    "\n",
    "class TabNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim1,output_dim2, n_d=8, n_a=8,\n",
    "                 n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,\n",
    "                 virtual_batch_size=128, momentum=0.02, device_name='auto',\n",
    "                 mask_type=\"sparsemax\"):\n",
    "        \"\"\"\n",
    "        Defines TabNet network\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Initial number of features\n",
    "        output_dim : int\n",
    "            Dimension of network output\n",
    "            examples : one for regression, 2 for binary classification etc...\n",
    "        n_d : int\n",
    "            Dimension of the prediction  layer (usually between 4 and 64)\n",
    "        n_a : int\n",
    "            Dimension of the attention  layer (usually between 4 and 64)\n",
    "        n_steps : int\n",
    "            Number of sucessive steps in the newtork (usually betwenn 3 and 10)\n",
    "        gamma : float\n",
    "            Float above 1, scaling factor for attention updates (usually betwenn 1.0 to 2.0)\n",
    "        cat_idxs : list of int\n",
    "            Index of each categorical column in the dataset\n",
    "        cat_dims : list of int\n",
    "            Number of categories in each categorical column\n",
    "        cat_emb_dim : int or list of int\n",
    "            Size of the embedding of categorical features\n",
    "            if int, all categorical features will have same embedding size\n",
    "            if list of int, every corresponding feature will have specific size\n",
    "        n_independent : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        n_shared : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        epsilon : float\n",
    "            Avoid log(0), this should be kept very low\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in all batch norm\n",
    "        device_name : {'auto', 'cuda', 'cpu'}\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(TabNet, self).__init__()\n",
    "        self.cat_idxs = cat_idxs or []\n",
    "        self.cat_dims = cat_dims or []\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.mask_type = mask_type\n",
    "\n",
    "        if self.n_steps <= 0:\n",
    "            raise ValueError(\"n_steps should be a positive integer.\")\n",
    "        if self.n_independent == 0 and self.n_shared == 0:\n",
    "            raise ValueError(\"n_shared and n_independant can't be both zero.\")\n",
    "\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.embedder = EmbeddingGenerator(input_dim, cat_dims, cat_idxs, cat_emb_dim)\n",
    "        self.post_embed_dim = self.embedder.post_embed_dim\n",
    "        self.tabnet = TabNetNoEmbeddings(self.post_embed_dim, output_dim1,output_dim2, n_d, n_a, n_steps,\n",
    "                                         gamma, n_independent, n_shared, epsilon,\n",
    "                                         virtual_batch_size, momentum, mask_type)\n",
    "\n",
    "        # Defining device\n",
    "        if device_name == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device_name = 'cuda'\n",
    "            else:\n",
    "                device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedder(x)\n",
    "        return self.tabnet(x)\n",
    "\n",
    "    def forward_masks(self, x):\n",
    "        x = self.embedder(x)\n",
    "        return self.tabnet.forward_masks(x)\n",
    "\n",
    "\n",
    "class AttentiveTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 virtual_batch_size=128,\n",
    "                 momentum=0.02,\n",
    "                 mask_type=\"sparsemax\"):\n",
    "        \"\"\"\n",
    "        Initialize an attention transformer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Input size\n",
    "        output_dim : int\n",
    "            Outpu_size\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.fc = Linear(input_dim, output_dim, bias=False)\n",
    "        initialize_non_glu(self.fc, input_dim, output_dim)\n",
    "        self.bn = GBN(output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "        if mask_type == \"sparsemax\":\n",
    "            # Sparsemax\n",
    "            self.selector = sparsemax.Sparsemax(dim=-1)\n",
    "        elif mask_type == \"entmax\":\n",
    "            # Entmax\n",
    "            self.selector = sparsemax.Entmax15(dim=-1)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Please choose either sparsemax\" +\n",
    "                                      \"or entmax as masktype\")\n",
    "\n",
    "    def forward(self, priors, processed_feat):\n",
    "        x = self.fc(processed_feat)\n",
    "        x = self.bn(x)\n",
    "        x = torch.mul(x, priors)\n",
    "        x = self.selector(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, shared_layers, n_glu_independent,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(FeatTransformer, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize a feature transformer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Input size\n",
    "        output_dim : int\n",
    "            Outpu_size\n",
    "        shared_layers : torch.nn.ModuleList\n",
    "            The shared block that should be common to every step\n",
    "        n_glu_independant : int\n",
    "            Number of independent GLU layers\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization within GLU block(s)\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            'n_glu': n_glu_independent,\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        if shared_layers is None:\n",
    "            # no shared layers\n",
    "            self.shared = torch.nn.Identity()\n",
    "            is_first = True\n",
    "        else:\n",
    "            self.shared = GLU_Block(input_dim, output_dim,\n",
    "                                    first=True,\n",
    "                                    shared_layers=shared_layers,\n",
    "                                    n_glu=len(shared_layers),\n",
    "                                    virtual_batch_size=virtual_batch_size,\n",
    "                                    momentum=momentum)\n",
    "            is_first = False\n",
    "\n",
    "        if n_glu_independent == 0:\n",
    "            # no independent layers\n",
    "            self.specifics = torch.nn.Identity()\n",
    "        else:\n",
    "            spec_input_dim = input_dim if is_first else output_dim\n",
    "            self.specifics = GLU_Block(spec_input_dim, output_dim,\n",
    "                                       first=is_first,\n",
    "                                       **params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        x = self.specifics(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Block(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Independant GLU block, specific to each step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, n_glu=2, first=False, shared_layers=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Block, self).__init__()\n",
    "        self.first = first\n",
    "        self.shared_layers = shared_layers\n",
    "        self.n_glu = n_glu\n",
    "        self.glu_layers = torch.nn.ModuleList()\n",
    "\n",
    "        params = {\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        fc = shared_layers[0] if shared_layers else None\n",
    "        self.glu_layers.append(GLU_Layer(input_dim, output_dim,\n",
    "                                         fc=fc,\n",
    "                                         **params))\n",
    "        for glu_id in range(1, self.n_glu):\n",
    "            fc = shared_layers[glu_id] if shared_layers else None\n",
    "            self.glu_layers.append(GLU_Layer(output_dim, output_dim,\n",
    "                                             fc=fc,\n",
    "                                             **params))\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = torch.sqrt(torch.FloatTensor([0.5]).to(x.device))\n",
    "        if self.first:  # the first layer of the block has no scale multiplication\n",
    "            x = self.glu_layers[0](x)\n",
    "            layers_left = range(1, self.n_glu)\n",
    "        else:\n",
    "            layers_left = range(self.n_glu)\n",
    "\n",
    "        for glu_id in layers_left:\n",
    "            x = torch.add(x, self.glu_layers[glu_id](x))\n",
    "            x = x*scale\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, fc=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Layer, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        if fc:\n",
    "            self.fc = fc\n",
    "        else:\n",
    "            self.fc = Linear(input_dim, 2*output_dim, bias=False)\n",
    "        initialize_glu(self.fc, input_dim, 2*output_dim)\n",
    "\n",
    "        self.bn = GBN(2*output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        out = torch.mul(x[:, :self.output_dim], torch.sigmoid(x[:, self.output_dim:]))\n",
    "        return out\n",
    "\n",
    "\n",
    "class EmbeddingGenerator(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Classical embeddings generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, cat_dims, cat_idxs, cat_emb_dim):\n",
    "        \"\"\" This is an embedding module for an entier set of features\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Number of features coming as input (number of columns)\n",
    "        cat_dims : list of int\n",
    "            Number of modalities for each categorial features\n",
    "            If the list is empty, no embeddings will be done\n",
    "        cat_idxs : list of int\n",
    "            Positional index for each categorical features in inputs\n",
    "        cat_emb_dim : int or list of int\n",
    "            Embedding dimension for each categorical features\n",
    "            If int, the same embdeding dimension will be used for all categorical features\n",
    "        \"\"\"\n",
    "        super(EmbeddingGenerator, self).__init__()\n",
    "        if cat_dims == [] or cat_idxs == []:\n",
    "            self.skip_embedding = True\n",
    "            self.post_embed_dim = input_dim\n",
    "            return\n",
    "\n",
    "        self.skip_embedding = False\n",
    "        if isinstance(cat_emb_dim, int):\n",
    "            self.cat_emb_dims = [cat_emb_dim]*len(cat_idxs)\n",
    "        else:\n",
    "            self.cat_emb_dims = cat_emb_dim\n",
    "\n",
    "        # check that all embeddings are provided\n",
    "        if len(self.cat_emb_dims) != len(cat_dims):\n",
    "            msg = \"\"\" cat_emb_dim and cat_dims must be lists of same length, got {len(self.cat_emb_dims)}\n",
    "                      and {len(cat_dims)}\"\"\"\n",
    "            raise ValueError(msg)\n",
    "        self.post_embed_dim = int(input_dim + np.sum(self.cat_emb_dims) - len(self.cat_emb_dims))\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleList()\n",
    "\n",
    "        # Sort dims by cat_idx\n",
    "        sorted_idxs = np.argsort(cat_idxs)\n",
    "        cat_dims = [cat_dims[i] for i in sorted_idxs]\n",
    "        self.cat_emb_dims = [self.cat_emb_dims[i] for i in sorted_idxs]\n",
    "\n",
    "        for cat_dim, emb_dim in zip(cat_dims, self.cat_emb_dims):\n",
    "            self.embeddings.append(torch.nn.Embedding(cat_dim, emb_dim))\n",
    "\n",
    "        # record continuous indices\n",
    "        self.continuous_idx = torch.ones(input_dim, dtype=torch.bool)\n",
    "        self.continuous_idx[cat_idxs] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply embdeddings to inputs\n",
    "        Inputs should be (batch_size, input_dim)\n",
    "        Outputs will be of size (batch_size, self.post_embed_dim)\n",
    "        \"\"\"\n",
    "        if self.skip_embedding:\n",
    "            # no embeddings required\n",
    "            return x\n",
    "\n",
    "        cols = []\n",
    "        cat_feat_counter = 0\n",
    "        for feat_init_idx, is_continuous in enumerate(self.continuous_idx):\n",
    "            # Enumerate through continuous idx boolean mask to apply embeddings\n",
    "            if is_continuous:\n",
    "                cols.append(x[:, feat_init_idx].float().view(-1, 1))\n",
    "            else:\n",
    "                cols.append(self.embeddings[cat_feat_counter](x[:, feat_init_idx].long()))\n",
    "                cat_feat_counter += 1\n",
    "        # concat\n",
    "        post_embeddings = torch.cat(cols, dim=1)\n",
    "        return post_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabModel(BaseEstimator):\n",
    "    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n",
    "                 lambda_sparse=1e-3, seed=0,\n",
    "                 clip_value=1, verbose=1,\n",
    "                 optimizer_fn=torch.optim.Adam,\n",
    "                 optimizer_params=dict(lr=2e-2),\n",
    "                 scheduler_params=None, scheduler_fn=None,\n",
    "                 mask_type=\"sparsemax\",\n",
    "                 input_dim=None, output_dim1=None,output_dim2=None,\n",
    "                 device_name='auto'):\n",
    "        \"\"\" Class for TabNet model\n",
    "        Parameters\n",
    "        ----------\n",
    "            device_name: str\n",
    "                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.cat_idxs = cat_idxs\n",
    "        self.cat_dims = cat_dims\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.clip_value = clip_value\n",
    "        self.verbose = verbose\n",
    "        self.optimizer_fn = optimizer_fn\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.device_name = device_name\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "        self.mask_type = mask_type\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "\n",
    "        #self.batch_size = 1024\n",
    "        self.batch_size = 2048\n",
    "\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        # Defining device\n",
    "        if device_name == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device_name = 'cuda'\n",
    "            else:\n",
    "                device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        print(f\"Device used : {self.device}\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def construct_loaders(self, X_train, y_scored_train,y_nscored_train, X_valid, y_valid,\n",
    "                          weights, batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define construct_loaders to use this base class')\n",
    "\n",
    "    def init_network(\n",
    "                     self,\n",
    "                     input_dim,\n",
    "                     output_dim1,\n",
    "                     output_dim2,\n",
    "                     n_d,\n",
    "                     n_a,\n",
    "                     n_steps,\n",
    "                     gamma,\n",
    "                     cat_idxs,\n",
    "                     cat_dims,\n",
    "                     cat_emb_dim,\n",
    "                     n_independent,\n",
    "                     n_shared,\n",
    "                     epsilon,\n",
    "                     virtual_batch_size,\n",
    "                     momentum,\n",
    "                     device_name,\n",
    "                     mask_type,\n",
    "                     ):\n",
    "        self.network = TabNet(\n",
    "            input_dim,\n",
    "            output_dim1,\n",
    "            output_dim2,\n",
    "            n_d=n_d,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=cat_emb_dim,\n",
    "            n_independent=n_independent,\n",
    "            n_shared=n_shared,\n",
    "            epsilon=epsilon,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            momentum=momentum,\n",
    "            device_name=device_name,\n",
    "            mask_type=mask_type).to(self.device)\n",
    "\n",
    "        self.reducing_matrix = create_explain_matrix(\n",
    "            self.network.input_dim,\n",
    "            self.network.cat_emb_dim,\n",
    "            self.network.cat_idxs,\n",
    "            self.network.post_embed_dim)\n",
    "\n",
    "    def fit(self, X_train, y_scored_train, y_nscored_train, X_valid=None, y_valid=None, loss_fn=None,loss_tr=None,\n",
    "            weights=0, max_epochs=100, patience=10, batch_size=1024,\n",
    "            virtual_batch_size=128, num_workers=0, drop_last=False):\n",
    "        \"\"\"Train a neural network stored in self.network\n",
    "        Using train_dataloader for training data and\n",
    "        valid_dataloader for validation.\n",
    "        Parameters\n",
    "        ----------\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            weights : bool or dictionnary\n",
    "                0 for no balancing\n",
    "                1 for automated balancing\n",
    "                dict for custom weights per class\n",
    "            max_epochs : int\n",
    "                Maximum number of epochs during training\n",
    "            patience : int\n",
    "                Number of consecutive non improving epoch before early stopping\n",
    "            batch_size : int\n",
    "                Training batch size\n",
    "            virtual_batch_size : int\n",
    "                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n",
    "            num_workers : int\n",
    "                Number of workers used in torch.utils.data.DataLoader\n",
    "            drop_last : bool\n",
    "                Whether to drop last batch during training\n",
    "        \"\"\"\n",
    "        # update model name\n",
    "\n",
    "        self.update_fit_params(X_train, y_scored_train,y_nscored_train, X_valid, y_valid, loss_fn,loss_tr,\n",
    "                               weights, max_epochs, patience, batch_size,virtual_batch_size, num_workers, drop_last)\n",
    "\n",
    "\n",
    "        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n",
    "                                                                    y_scored_train,\n",
    "                                                                    y_nscored_train,\n",
    "                                                                    X_valid,\n",
    "                                                                    y_valid,\n",
    "                                                                    self.updated_weights,\n",
    "                                                                    self.batch_size,\n",
    "                                                                    self.num_workers,\n",
    "                                                                    self.drop_last)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim1=self.output_dim1,\n",
    "            output_dim2=self.output_dim2,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=self.virtual_batch_size,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "\n",
    "        self.optimizer = self.optimizer_fn(self.network.parameters(),\n",
    "                                           **self.optimizer_params)\n",
    "\n",
    "        if self.scheduler_fn:\n",
    "            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "        self.losses_train = []\n",
    "        self.losses_valid = []\n",
    "        self.learning_rates = []\n",
    "        self.metrics_train = []\n",
    "        self.metrics_valid = []\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(\"Will train until validation stopping metric\",\n",
    "                  f\"hasn't improved in {self.patience} rounds.\")\n",
    "            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n",
    "            print('---------------------------------------')\n",
    "            print(msg_epoch)\n",
    "\n",
    "        total_time = 0\n",
    "        while (self.epoch < self.max_epochs and\n",
    "               self.patience_counter < self.patience):\n",
    "            starting_time = time.time()\n",
    "            # updates learning rate history\n",
    "            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n",
    "\n",
    "            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n",
    "\n",
    "            # leaving it here, may be used for callbacks later\n",
    "            self.losses_train.append(fit_metrics['train']['loss_avg'])\n",
    "            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n",
    "            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n",
    "            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n",
    "\n",
    "            stopping_loss = fit_metrics['valid']['stopping_loss']\n",
    "            if stopping_loss < self.best_cost:\n",
    "                self.best_cost = stopping_loss\n",
    "                self.patience_counter = 0\n",
    "                # Saving model\n",
    "                self.best_network = deepcopy(self.network)\n",
    "                has_improved = True\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                has_improved=False\n",
    "            self.epoch += 1\n",
    "            total_time += time.time() - starting_time\n",
    "            if self.verbose > 0:\n",
    "                if self.epoch % self.verbose == 0:\n",
    "                    separator = \"|\"\n",
    "                    msg_epoch = f\"| {self.epoch:<5} | \"\n",
    "                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n",
    "                    msg_epoch += f\" {has_improved}\"\n",
    "                    print(msg_epoch)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            if self.patience_counter == self.patience:\n",
    "                print(f\"Early stopping occured at epoch {self.epoch}\")\n",
    "            print(f\"Training done in {total_time:.3f} seconds.\")\n",
    "            print('---------------------------------------')\n",
    "\n",
    "        self.history = {\"train\": {\"loss\": self.losses_train,\n",
    "                                  \"metric\": self.metrics_train,\n",
    "                                  \"lr\": self.learning_rates},\n",
    "                        \"valid\": {\"loss\": self.losses_valid,\n",
    "                                  \"metric\": self.metrics_valid}}\n",
    "        # load best models post training\n",
    "        self.load_best_model()\n",
    "\n",
    "        # compute feature importance once the best model is defined\n",
    "        self._compute_feature_importances(train_dataloader)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saving model with two distinct files.\n",
    "        \"\"\"\n",
    "        saved_params = {}\n",
    "        for key, val in self.get_params().items():\n",
    "            if isinstance(val, type):\n",
    "                # Don't save torch specific params\n",
    "                continue\n",
    "            else:\n",
    "                saved_params[key] = val\n",
    "\n",
    "        # Create folder\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save models params\n",
    "        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(saved_params, f)\n",
    "\n",
    "        # Save state_dict\n",
    "        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n",
    "        shutil.make_archive(path, 'zip', path)\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Successfully saved model at {path}.zip\")\n",
    "        return f\"{path}.zip\"\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                with zipfile.ZipFile(filepath) as z:\n",
    "                    with z.open(\"model_params.json\") as f:\n",
    "                        loaded_params = json.load(f)\n",
    "                    with z.open(\"network.pt\") as f:\n",
    "                        try:\n",
    "                            saved_state_dict = torch.load(f)\n",
    "                        except io.UnsupportedOperation:\n",
    "                            # In Python <3.7, the returned file object is not seekable (which at least\n",
    "                            # some versions of PyTorch require) - so we'll try buffering it in to a\n",
    "                            # BytesIO instead:\n",
    "                            saved_state_dict = torch.load(io.BytesIO(f.read()))\n",
    "                            \n",
    "            except:\n",
    "                with open(os.path.join(filepath, \"model_params.json\")) as f:\n",
    "                        loaded_params = json.load(f)\n",
    "\n",
    "                saved_state_dict = torch.load(os.path.join(filepath, \"network.pt\"), map_location=\"cpu\")\n",
    " \n",
    "        except KeyError:\n",
    "            raise KeyError(\"Your zip file is missing at least one component\")\n",
    "\n",
    "        #print(loaded_params)\n",
    "        if torch.cuda.is_available():\n",
    "            device_name = 'cuda'\n",
    "        else:\n",
    "            device_name = 'cpu'\n",
    "        loaded_params[\"device_name\"] = device_name\n",
    "        self.__init__(**loaded_params)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim1=self.output_dim1,\n",
    "            output_dim2=self.output_dim2,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=1024,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "        self.network.load_state_dict(saved_state_dict)\n",
    "        self.network.eval()\n",
    "        return\n",
    "\n",
    "    def fit_epoch(self, train_dataloader, valid_dataloader):\n",
    "        \"\"\"\n",
    "        Evaluates and updates network for one epoch.\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with valid set\n",
    "        \"\"\"\n",
    "        train_metrics = self.train_epoch(train_dataloader)\n",
    "        valid_metrics = self.predict_epoch(valid_dataloader)\n",
    "\n",
    "        fit_metrics = {'train': train_metrics,\n",
    "                       'valid': valid_metrics}\n",
    "\n",
    "        return fit_metrics\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_batch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_batch to use this base class')\n",
    "\n",
    "    def load_best_model(self):\n",
    "        if self.best_network is not None:\n",
    "            self.network = self.best_network\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem or the last class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict to use this base class')\n",
    "\n",
    "    def explain(self, X):\n",
    "        \"\"\"\n",
    "        Return local explanation\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            M_explain: matrix\n",
    "                Importance per sample, per columns.\n",
    "            masks: matrix\n",
    "                Sparse matrix showing attention masks used by network.\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            for key, value in masks.items():\n",
    "                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n",
    "                                            self.reducing_matrix)\n",
    "\n",
    "            if batch_nb == 0:\n",
    "                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                             self.reducing_matrix)\n",
    "                res_masks = masks\n",
    "            else:\n",
    "                res_explain = np.vstack([res_explain,\n",
    "                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                                        self.reducing_matrix)])\n",
    "                for key, value in masks.items():\n",
    "                    res_masks[key] = np.vstack([res_masks[key], value])\n",
    "        return res_explain, res_masks\n",
    "\n",
    "    def _compute_feature_importances(self, loader):\n",
    "        self.network.eval()\n",
    "        feature_importances_ = np.zeros((self.network.post_embed_dim))\n",
    "        for data, targets,_ in loader:\n",
    "            data = data.to(self.device).float()\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n",
    "\n",
    "        feature_importances_ = csc_matrix.dot(feature_importances_,\n",
    "                                              self.reducing_matrix)\n",
    "        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n",
    "        \n",
    "\n",
    "\n",
    "class TabNetRegressor(TabModel):\n",
    "\n",
    "    def construct_loaders(self, X_train, y_scored_train,y_nscored_train, X_valid, y_valid, weights,\n",
    "                          batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if isinstance(weights, int):\n",
    "            if weights == 1:\n",
    "                raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "        if isinstance(weights, dict):\n",
    "            raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "\n",
    "\n",
    "\n",
    "        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n",
    "                                                                y_scored_train,\n",
    "                                                                y_nscored_train,\n",
    "                                                                X_valid,\n",
    "                                                                y_valid,\n",
    "                                                                weights,\n",
    "                                                                batch_size,\n",
    "                                                                num_workers,\n",
    "                                                                drop_last)\n",
    "        return train_dataloader, valid_dataloader\n",
    "\n",
    "    def update_fit_params(self, X_train, y_scored_train, y_nscored_train, X_valid, y_valid, loss_fn, loss_tr,\n",
    "                          weights, max_epochs, patience,batch_size, virtual_batch_size, num_workers, drop_last):\n",
    "\n",
    "\n",
    "        if loss_fn is None:\n",
    "            self.loss_fn = torch.nn.functional.mse_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "            self.loss_tr = loss_tr\n",
    "\n",
    "        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n",
    "        self.input_dim = X_train.shape[1]\n",
    "\n",
    "        if len(y_scored_train.shape) == 1:\n",
    "            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n",
    "                                if doing single regression.\"\"\")\n",
    "        assert y_scored_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n",
    "        self.output_dim1 = y_scored_train.shape[1]\n",
    "        self.output_dim2 = y_nscored_train.shape[1]\n",
    "\n",
    "        self.updated_weights = weights\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        # Initialize counters and histories.\n",
    "        self.patience_counter = 0\n",
    "        self.epoch = 0\n",
    "        self.best_cost = np.inf\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.train()\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets_scored, targets_nscored in train_loader:\n",
    "            batch_outs = self.train_batch(data, targets_scored, targets_nscored)\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "        # stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n",
    "        total_loss = total_loss / len(train_loader)\n",
    "\n",
    "        epoch_metrics = {'loss_avg': total_loss,\n",
    "                         'stopping_loss': total_loss,\n",
    "                         }\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "        return epoch_metrics\n",
    "\n",
    "    def train_batch(self, data, targets_scored, targets_nscored):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        self.network.train()\n",
    "        data = data.to(self.device).float()\n",
    "\n",
    "        \n",
    "\n",
    "        targets_scored = targets_scored.to(self.device).float()\n",
    "        targets_nscored = targets_nscored.to(self.device).float()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output1,output2, M_loss = self.network(data)\n",
    "\n",
    "        loss1 = self.loss_fn(output1, targets_scored)\n",
    "        loss2 = self.loss_fn(output2, targets_nscored)\n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "\n",
    "        loss.backward()\n",
    "        if self.clip_value:\n",
    "            clip_grad_norm_(self.network.parameters(), self.clip_value)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output1,\n",
    "                      'y': targets_scored}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        self.network.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in loader:\n",
    "            batch_outs = self.predict_batch(data, targets)\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        stopping_loss = log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "\n",
    "        total_loss = total_loss / len(loader)\n",
    "        epoch_metrics = {'total_loss': total_loss,\n",
    "                         'stopping_loss': stopping_loss}\n",
    "\n",
    "        return epoch_metrics\n",
    "\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        data = data.to(self.device).float()\n",
    "        targets = targets.to(self.device).float()\n",
    "\n",
    "        output,_, M_loss = self.network(data)\n",
    "       \n",
    "        loss = self.loss_fn(output, targets)\n",
    "        #print(self.loss_fn, loss)\n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "        #print(loss)\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        results = []\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            output,_, M_loss = self.network(data)\n",
    "            predictions = output.cpu().detach().numpy()\n",
    "            results.append(predictions)\n",
    "        res = np.vstack(results)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "SPLITS = 5\n",
    "EPOCHS = 2\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7 \n",
    "\n",
    "cat_cols = ['cluster_0',\"cluster_1\",'cluster_2','cp_time_24', 'cp_time_48', 'cp_time_72', 'cp_dose_D1', 'cp_dose_D2']\n",
    "\n",
    "\n",
    "# Parameters model\n",
    "cat_emb_dim=[1] * len(cat_cols) #to choose\n",
    "cats_idx = [1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046]\n",
    "cat_dims = [2] * len(cat_cols) #to choose\n",
    "\n",
    "\n",
    "save_name = \"../data/tabnet-weights-public/tabnet-raw-public-step1/tabnet_raw_step1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             sig_id cp_time cp_dose         0         1         2         3  \\\n0      id_000644bb2      24      D1  1.134849  0.907687 -0.416385 -0.966814   \n1      id_000779bfc      72      D1  0.119282  0.681738  0.272399  0.080113   \n2      id_000a6266a      48      D1  0.779973  0.946463  1.425350 -0.132928   \n3      id_0015fd391      48      D1 -0.734910 -0.274641 -0.438509  0.759097   \n4      id_001626bd3      72      D2 -0.452718 -0.477513  0.972316  0.970731   \n...             ...     ...     ...       ...       ...       ...       ...   \n21943  id_fff8c2444      72      D1  0.237856 -1.228203  0.218376 -0.365976   \n21944  id_fffb1ceed      24      D2  0.209361 -0.022389 -0.235888 -0.796989   \n21945  id_fffb70c0c      24      D2 -1.911021  0.587228 -0.588417  1.296405   \n21946  id_fffcb9e7c      24      D1  0.816407  0.417618  0.431631  0.300617   \n21947  id_ffffdd77b      72      D1 -1.243096  1.567730 -0.269573  1.083636   \n\n              4         5         6  ...  \\\n0     -0.254723 -1.017473 -1.364787  ...   \n1      1.205169  0.686517  0.313396  ...   \n2     -0.006122  1.492493  0.235577  ...   \n3      2.346330 -0.858153 -2.288417  ...   \n4      1.463427 -0.869555 -0.375501  ...   \n...         ...       ...       ...  ...   \n21943 -0.330177  0.569243 -0.150978  ...   \n21944 -0.674009  0.919312  0.735603  ...   \n21945 -1.002640  0.850589 -0.304313  ...   \n21946  1.070346 -0.024189  0.048942  ...   \n21947 -0.511235 -2.099634 -1.622462  ...   \n\n       vesicular_monoamine_transporter_inhibitor  vitamin_k_antagonist  \\\n0                                              0                     0   \n1                                              0                     0   \n2                                              0                     0   \n3                                              0                     0   \n4                                              0                     0   \n...                                          ...                   ...   \n21943                                          0                     0   \n21944                                          0                     0   \n21945                                          0                     0   \n21946                                          0                     0   \n21947                                          0                     0   \n\n       voltage-gated_calcium_channel_ligand  \\\n0                                         0   \n1                                         0   \n2                                         0   \n3                                         0   \n4                                         0   \n...                                     ...   \n21943                                     0   \n21944                                     0   \n21945                                     0   \n21946                                     0   \n21947                                     0   \n\n       voltage-gated_potassium_channel_activator  \\\n0                                              0   \n1                                              0   \n2                                              0   \n3                                              0   \n4                                              0   \n...                                          ...   \n21943                                          0   \n21944                                          0   \n21945                                          0   \n21946                                          0   \n21947                                          0   \n\n       voltage-gated_sodium_channel_blocker  wdr5_mll_interaction_inhibitor  \\\n0                                         0                               0   \n1                                         0                               0   \n2                                         0                               0   \n3                                         0                               0   \n4                                         0                               0   \n...                                     ...                             ...   \n21943                                     0                               0   \n21944                                     0                               0   \n21945                                     0                               0   \n21946                                     0                               0   \n21947                                     0                               0   \n\n       wnt_agonist  xanthine_oxidase_inhibitor  xiap_inhibitor  kfold  \n0                0                           0               0      5  \n1                0                           0               0      0  \n2                0                           0               0      6  \n3                0                           0               0      0  \n4                0                           0               0      4  \n...            ...                         ...             ...    ...  \n21943            0                           0               0      5  \n21944            0                           0               0      1  \n21945            0                           0               0      5  \n21946            0                           0               0      1  \n21947            0                           0               0      6  \n\n[21948 rows x 1654 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>cp_time</th>\n      <th>cp_dose</th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>...</th>\n      <th>vesicular_monoamine_transporter_inhibitor</th>\n      <th>vitamin_k_antagonist</th>\n      <th>voltage-gated_calcium_channel_ligand</th>\n      <th>voltage-gated_potassium_channel_activator</th>\n      <th>voltage-gated_sodium_channel_blocker</th>\n      <th>wdr5_mll_interaction_inhibitor</th>\n      <th>wnt_agonist</th>\n      <th>xanthine_oxidase_inhibitor</th>\n      <th>xiap_inhibitor</th>\n      <th>kfold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_000644bb2</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>1.134849</td>\n      <td>0.907687</td>\n      <td>-0.416385</td>\n      <td>-0.966814</td>\n      <td>-0.254723</td>\n      <td>-1.017473</td>\n      <td>-1.364787</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_000779bfc</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.119282</td>\n      <td>0.681738</td>\n      <td>0.272399</td>\n      <td>0.080113</td>\n      <td>1.205169</td>\n      <td>0.686517</td>\n      <td>0.313396</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_000a6266a</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>0.779973</td>\n      <td>0.946463</td>\n      <td>1.425350</td>\n      <td>-0.132928</td>\n      <td>-0.006122</td>\n      <td>1.492493</td>\n      <td>0.235577</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_0015fd391</td>\n      <td>48</td>\n      <td>D1</td>\n      <td>-0.734910</td>\n      <td>-0.274641</td>\n      <td>-0.438509</td>\n      <td>0.759097</td>\n      <td>2.346330</td>\n      <td>-0.858153</td>\n      <td>-2.288417</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_001626bd3</td>\n      <td>72</td>\n      <td>D2</td>\n      <td>-0.452718</td>\n      <td>-0.477513</td>\n      <td>0.972316</td>\n      <td>0.970731</td>\n      <td>1.463427</td>\n      <td>-0.869555</td>\n      <td>-0.375501</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21943</th>\n      <td>id_fff8c2444</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>0.237856</td>\n      <td>-1.228203</td>\n      <td>0.218376</td>\n      <td>-0.365976</td>\n      <td>-0.330177</td>\n      <td>0.569243</td>\n      <td>-0.150978</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>21944</th>\n      <td>id_fffb1ceed</td>\n      <td>24</td>\n      <td>D2</td>\n      <td>0.209361</td>\n      <td>-0.022389</td>\n      <td>-0.235888</td>\n      <td>-0.796989</td>\n      <td>-0.674009</td>\n      <td>0.919312</td>\n      <td>0.735603</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21945</th>\n      <td>id_fffb70c0c</td>\n      <td>24</td>\n      <td>D2</td>\n      <td>-1.911021</td>\n      <td>0.587228</td>\n      <td>-0.588417</td>\n      <td>1.296405</td>\n      <td>-1.002640</td>\n      <td>0.850589</td>\n      <td>-0.304313</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>21946</th>\n      <td>id_fffcb9e7c</td>\n      <td>24</td>\n      <td>D1</td>\n      <td>0.816407</td>\n      <td>0.417618</td>\n      <td>0.431631</td>\n      <td>0.300617</td>\n      <td>1.070346</td>\n      <td>-0.024189</td>\n      <td>0.048942</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21947</th>\n      <td>id_ffffdd77b</td>\n      <td>72</td>\n      <td>D1</td>\n      <td>-1.243096</td>\n      <td>1.567730</td>\n      <td>-0.269573</td>\n      <td>1.083636</td>\n      <td>-0.511235</td>\n      <td>-2.099634</td>\n      <td>-1.622462</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>21948 rows × 1654 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "folds = train.copy()\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=NFOLDS)\n",
    "\n",
    "for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target_scored)):\n",
    "    folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "folds['kfold'] = folds['kfold'].astype(int)\n",
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1047"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "feature_cols = [c for c in process_data(folds).columns if c not in (target_scored_cols + target_nscored_cols)]\n",
    "feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, seed):\n",
    "\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    x_train, y_scored_train, y_nscored_train  = train_df[feature_cols].values, train_df[target_scored_cols].values, train_df[target_nscored_cols].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_scored_cols].values\n",
    "    \n",
    "    \n",
    "    # model = Model(\n",
    "    #     num_features=num_features,\n",
    "    #     num_targets_scored=num_targets_scored,\n",
    "    #     num_targets_nscored=num_targets_nscored,\n",
    "    #     hidden_size=hidden_size,\n",
    "    #     dropout_rate=dropout_rate,\n",
    "    # )\n",
    "\n",
    "    model = TabNetRegressor(n_d=24, \n",
    "                            n_a=24, \n",
    "                            n_steps=1, \n",
    "                            gamma=1.3, \n",
    "                            lambda_sparse=0, \n",
    "                            cat_dims=cat_dims, \n",
    "                            cat_emb_dim=cat_emb_dim, \n",
    "                            cat_idxs=cats_idx, \n",
    "                            optimizer_fn=torch.optim.Adam,\n",
    "                            optimizer_params=dict(lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY), \n",
    "                            mask_type='entmax', \n",
    "                            device_name=device, \n",
    "                            scheduler_params=dict(milestones=[ 100,150], gamma=0.9), \n",
    "                            scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n",
    "\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "\n",
    "    model.fit(X_train=x_train, \n",
    "              y_scored_train=y_scored_train,\n",
    "              y_nscored_train=y_nscored_train,  \n",
    "              X_valid=x_valid, \n",
    "              y_valid=y_valid,\n",
    "              max_epochs=EPOCHS,\n",
    "              patience=50, \n",
    "              batch_size=1024, \n",
    "              virtual_batch_size=128,\n",
    "              num_workers=0, \n",
    "              drop_last=False,\n",
    "              weights=0,\n",
    "              loss_fn=loss_fn,\n",
    "              loss_tr=loss_tr)\n",
    "\n",
    "\n",
    "    # model.init_bias(pos_scored_rate,pos_nscored_rate)\n",
    "\n",
    "    # model.to(DEVICE)\n",
    "    \n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    # scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,pct_start=0.1, div_factor=1e4, final_div_factor=1e5,\n",
    "    #                                           max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', factor=0.1, patience=5, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)\n",
    "\n",
    "    oof = np.zeros((len(train), target_scored.iloc[:, 1:].shape[1]))\n",
    "    \n",
    "    model.load_best_model()\n",
    "    preds = model.predict(x_valid)\n",
    "    oof[val_idx] = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n",
    "\n",
    "    x_test = test_[feature_cols].values\n",
    "    preds = model.predict(x_test)\n",
    "    predictions = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_scored_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_scored_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        print(f\"SEED {SEED} - FOLD {fold}\")\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SEED [0] - FOLD 0\nDevice used : cpu\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  1.50236 |   0.69417 |   5.4        True\n| 2     |  1.41007 |   0.69284 |   10.2       True\nTraining done in 10.162 seconds.\n---------------------------------------\nSEED [0] - FOLD 1\nDevice used : cpu\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  1.50285 |   0.69050 |   5.2        True\n| 2     |  1.40944 |   0.68337 |   10.0       True\nTraining done in 10.001 seconds.\n---------------------------------------\nSEED [0] - FOLD 2\nDevice used : cpu\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  1.50343 |   0.69049 |   4.7        True\n| 2     |  1.41026 |   0.68648 |   9.3        True\nTraining done in 9.350 seconds.\n---------------------------------------\nSEED [0] - FOLD 3\nDevice used : cpu\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  1.50338 |   0.69318 |   5.2        True\n| 2     |  1.41126 |   0.69195 |   9.8        True\nTraining done in 9.825 seconds.\n---------------------------------------\nSEED [0] - FOLD 4\nDevice used : cpu\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  1.50412 |   0.69176 |   4.8        True\n| 2     |  1.41077 |   0.68878 |   9.9        True\nTraining done in 9.931 seconds.\n---------------------------------------\nSEED [0] - FOLD 5\nDevice used : cpu\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  1.50386 |   0.69037 |   5.4        True\n| 2     |  1.41127 |   0.68488 |   11.1       True\nTraining done in 11.109 seconds.\n---------------------------------------\nSEED [0] - FOLD 6\nDevice used : cpu\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  1.50402 |   0.69049 |   6.9        True\n| 2     |  1.41151 |   0.68786 |   12.5       True\nTraining done in 12.486 seconds.\n---------------------------------------\n"
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "# SEED = [0,1,2,3,4,5,6] #<-- Update\n",
    "SEED = [0]\n",
    "oof = np.zeros((len(train), len(target_scored_cols)))\n",
    "predictions = np.zeros((len(test), len(target_scored_cols)))\n",
    "\n",
    "# mean_scored = np.mean(train[target_scored_cols].values,axis=0)\n",
    "# mean_nscored = np.mean(train[target_nscored_cols].values,axis=0)\n",
    "# pos_scored_rate = np.log(np.where(mean_scored==0, 1e-8, mean_scored))\n",
    "# pos_nscored_rate = np.log(np.where(mean_nscored==0, 1e-8, mean_nscored))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_scored_cols] = oof\n",
    "test[target_scored_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV log_loss:  0.6310457319309658\n"
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_scored_cols).merge(train[['sig_id']+target_scored_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "y_true = train_targets_scored[target_scored_cols].values\n",
    "y_pred = valid_results[target_scored_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_scored_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target_scored.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.5356952484298305\n"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}