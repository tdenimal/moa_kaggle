{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1605137350172",
   "display_name": "Python 3.7.9 64-bit ('moa_kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['train_targets_scored.csv',\n 'sample_submission.csv',\n '.gitkeep',\n 'train_drug.csv',\n 'train_features.csv',\n 'test_features.csv',\n 'train_targets_nonscored.csv']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "data_dir = '../data/01_raw'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "no_ctl = True\n",
    "scale = \"rankgauss\"\n",
    "decompo = \"PCA\"\n",
    "ncompo_genes = 600\n",
    "ncompo_cells = 50\n",
    "encoding = \"dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(data_dir+'/train_features.csv')\n",
    "train_targets_scored = pd.read_csv(data_dir+'/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(data_dir+'/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv(data_dir+'/test_features.csv')\n",
    "sample_submission = pd.read_csv(data_dir+'/sample_submission.csv')\n",
    "drug = pd.read_csv(data_dir+'/train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_scored = train_targets_scored.columns[1:]\n",
    "scored = train_targets_scored.merge(drug, on='sig_id', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "not_ctl\n"
    }
   ],
   "source": [
    "if no_ctl:\n",
    "    # cp_type == ctl_vehicle\n",
    "    print(\"not_ctl\")\n",
    "    train_features = train_features[train_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "    test_features = test_features[test_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "    train_targets_scored = train_targets_scored.iloc[train_features.index]\n",
    "    train_targets_nonscored = train_targets_nonscored.iloc[train_features.index]\n",
    "    train_features.reset_index(drop = True, inplace = True)\n",
    "    train_features.reset_index(drop = True, inplace = True)\n",
    "    train_targets_scored.reset_index(drop = True, inplace = True)\n",
    "    train_targets_nonscored.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Indiquer si valeur dans le range max, min\n",
    "\n",
    "# import seaborn as sns\n",
    "# data = pd.concat([train_features,test_features],axis=0)\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"ctl_vehicle\"][\"c-4\"],label=\"normal\")\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"trt_cp\"][\"c-4\"],label=\"treated\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets = train_targets_scored[[c for c in train_targets_scored.columns if (c != \"sig_id\")]].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_inhibitor\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_activator\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_agonist\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_antagonist\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([train_features, test_features], ignore_index = True)\n",
    "# cols_numeric = [feat for feat in list(data_all.columns) if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]]\n",
    "# mask = (data_all[cols_numeric].var() >= variance_threshould).values\n",
    "# tmp = data_all[cols_numeric].loc[:, mask]\n",
    "# data_all = pd.concat([data_all[[\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]], tmp], axis = 1)\n",
    "cols_numeric = [feat for feat in list(data_all.columns) if feat not in [\"sig_id\", \"cp_type\", \"cp_time\", \"cp_dose\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# train_features[GENES].apply(lambda x : stats.moment(x,moment=5),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True gauss rank\n",
    "import cupy as cp\n",
    "from cupyx.scipy.special import erfinv\n",
    "epsilon = 1e-6\n",
    "\n",
    "for k in (cols_numeric):\n",
    "    r_gpu = cp.array(data_all.loc[:,k])\n",
    "    r_gpu = r_gpu.argsort().argsort()\n",
    "    r_gpu = (r_gpu/r_gpu.max()-0.5)*2 \n",
    "    r_gpu = cp.clip(r_gpu,-1+epsilon,1-epsilon)\n",
    "    r_gpu = erfinv(r_gpu) \n",
    "    data_all.loc[:,k] = cp.asnumpy( r_gpu * np.sqrt(2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Quantiletransformer\n",
    "# for col in (cols_numeric):\n",
    "\n",
    "#     transformer = QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n",
    "#     vec_len = len(data_all[col].values)\n",
    "#     raw_vec = data_all[col].values.reshape(vec_len, 1)\n",
    "#     transformer.fit(raw_vec)\n",
    "\n",
    "#     data_all[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PCA\n"
    }
   ],
   "source": [
    "#PCA\n",
    "if decompo == \"PCA\":\n",
    "    print(\"PCA\")\n",
    "    GENES = [col for col in data_all.columns if col.startswith(\"g-\")]\n",
    "    CELLS = [col for col in data_all.columns if col.startswith(\"c-\")]\n",
    "    \n",
    "    pca_genes = PCA(n_components = ncompo_genes,\n",
    "                    random_state = seed).fit_transform(data_all[GENES])\n",
    "    pca_cells = PCA(n_components = ncompo_cells,\n",
    "                    random_state = seed).fit_transform(data_all[CELLS])\n",
    "    \n",
    "    pca_genes = pd.DataFrame(pca_genes, columns = [f\"pca_g-{i}\" for i in range(ncompo_genes)])\n",
    "    pca_cells = pd.DataFrame(pca_cells, columns = [f\"pca_c-{i}\" for i in range(ncompo_cells)])\n",
    "    data_all = pd.concat([data_all, pca_genes, pca_cells], axis = 1)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "One-Hot\n"
    }
   ],
   "source": [
    "# Encoding\n",
    "if encoding == \"lb\":\n",
    "    print(\"Label Encoding\")\n",
    "    for feat in [\"cp_time\", \"cp_dose\"]:\n",
    "        data_all[feat] = LabelEncoder().fit_transform(data_all[feat])\n",
    "elif encoding == \"dummy\":\n",
    "    print(\"One-Hot\")\n",
    "    data_all = pd.get_dummies(data_all, columns = [\"cp_time\", \"cp_dose\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in data_all.columns if col.startswith(\"g-\")]\n",
    "CELLS = [col for col in data_all.columns if col.startswith(\"c-\")]\n",
    "\n",
    "#for stats in [\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]:\n",
    "for stats in [\"sum\",  \"std\", \"mean\",\"kurt\", \"skew\"]:\n",
    "    data_all[\"g_\" + stats] = getattr(data_all[GENES], stats)(axis = 1)\n",
    "    data_all[\"c_\" + stats] = getattr(data_all[CELLS], stats)(axis = 1)\n",
    "\n",
    "for stats in [\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]:\n",
    "    data_all[\"gc_\" + stats] = getattr(data_all[GENES + CELLS], stats)(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "feat_cols = [ c for c in data_all.columns if c not in [\"sig_id\", \"cp_type\"]]\n",
    "var_thresh = VarianceThreshold(0.8)  #<-- Update\n",
    "data_feats = pd.DataFrame(var_thresh.fit_transform(data_all[feat_cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([data_all[\"sig_id\"],data_feats],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# distortion = []\n",
    "# for k in range(1,10):\n",
    "#     kmeans = KMeans(n_clusters = k, random_state = seed).fit(data_all)\n",
    "#     distortion += [kmeans.inertia_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(range(1,10),distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "def fe_cluster2(data, n_clusters = 3, seed = 42):\n",
    "    kmeans = KMeans(n_clusters = n_clusters, random_state = seed).fit(data.iloc[:,1:])\n",
    "    data['cluster'] = kmeans.labels_\n",
    "    data = pd.get_dummies(data, columns = ['cluster'])\n",
    "    return data\n",
    "    \n",
    "# data_all=fe_cluster2(data_all,n_clusters=3,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df and test_df\n",
    "try:\n",
    "    train_targets_scored.drop(\"sig_id\", axis = 1, inplace = True)\n",
    "    train_targets_nonscored.drop(\"sig_id\", axis = 1, inplace = True)\n",
    "except:\n",
    "    pass\n",
    "train_df = data_all[: train_features.shape[0]]\n",
    "train_df.reset_index(drop = True, inplace = True)\n",
    "# The following line it's a bad practice in my opinion, targets on train set\n",
    "#train_df = pd.concat([train_df, targets], axis = 1)\n",
    "test_df = data_all[train_df.shape[0]: ]\n",
    "test_df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train_df.shape: (21948, 1047)\ntest_df.shape: (3624, 1047)\n"
    }
   ],
   "source": [
    "print(f\"train_df.shape: {train_df.shape}\")\n",
    "print(f\"test_df.shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X_test.shape: (3624, 1047)\n"
    }
   ],
   "source": [
    "X_test = test_df.values\n",
    "print(f\"X_test.shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "#from pytorch_tabnet import tab_network\n",
    "from pytorch_tabnet.multiclass_utils import unique_labels\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from pytorch_tabnet.utils import (PredictDataset,\n",
    "                                  create_explain_matrix)\n",
    "from sklearn.base import BaseEstimator\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import io\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "def log_loss_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n",
    "    return results.mean()\n",
    "\n",
    "def log_loss_score(actual, predicted,  eps=1e-15):\n",
    "\n",
    "        \"\"\"\n",
    "        :param predicted:   The predicted probabilities as floats between 0-1\n",
    "        :param actual:      The binary labels. Either 0 or 1.\n",
    "        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n",
    "        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        p1 = actual * np.log(predicted+eps)\n",
    "        p0 = (1-actual) * np.log(1-predicted+eps)\n",
    "        loss = p0 + p1\n",
    "\n",
    "        return -loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Format for numpy array\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        The input matrix\n",
    "    y : 2D array\n",
    "        The one-hot encoded target\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y1,y2=None):\n",
    "        self.x = x\n",
    "        self.y1 = y1\n",
    "        self.y2 = y2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y1,y2 = self.x[index], self.y1[index], self.y2[index]\n",
    "        return x, y1,y2\n",
    "\n",
    "class ValidDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Format for numpy array\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D array\n",
    "        The input matrix\n",
    "    y : 2D array\n",
    "        The one-hot encoded target\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x, y1):\n",
    "        self.x = x\n",
    "        self.y1 = y1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y1 = self.x[index], self.y1[index]\n",
    "        return x, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    X_train, y_scored_train,y_nscored_train, X_valid,y_valid, weights, batch_size, num_workers, drop_last, pin_memory=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Create dataloaders with or wihtout subsampling depending on weights and balanced.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : np.ndarray\n",
    "        Training data\n",
    "    y_train : np.array\n",
    "        Mapped Training targets\n",
    "    eval_set : list of tuple\n",
    "        List of eval tuple set (X, y)\n",
    "    weights : either 0, 1, dict or iterable\n",
    "        if 0 (default) : no weights will be applied\n",
    "        if 1 : classification only, will balanced class with inverse frequency\n",
    "        if dict : keys are corresponding class values are sample weights\n",
    "        if iterable : list or np array must be of length equal to nb elements\n",
    "                      in the training set\n",
    "    batch_size : int\n",
    "        how many samples per batch to load\n",
    "    num_workers : int\n",
    "        how many subprocesses to use for data loading. 0 means that the data\n",
    "        will be loaded in the main process\n",
    "    drop_last : bool\n",
    "        set to True to drop the last incomplete batch, if the dataset size is not\n",
    "        divisible by the batch size. If False and the size of dataset is not\n",
    "        divisible by the batch size, then the last batch will be smaller\n",
    "    pin_memory : bool\n",
    "        Whether to pin GPU memory during training\n",
    "    Returns\n",
    "    -------\n",
    "    train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "        Training and validation dataloaders\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(weights, int):\n",
    "        if weights == 0:\n",
    "            need_shuffle = True\n",
    "            sampler = None\n",
    "        elif weights == 1:\n",
    "            need_shuffle = False\n",
    "            class_sample_count = np.array(\n",
    "                [len(np.where(y_train == t)[0]) for t in np.unique(y_scored_train)]\n",
    "            )\n",
    "\n",
    "            weights = 1.0 / class_sample_count\n",
    "\n",
    "            samples_weight = np.array([weights[t] for t in y_scored_train])\n",
    "\n",
    "            samples_weight = torch.from_numpy(samples_weight)\n",
    "            samples_weight = samples_weight.double()\n",
    "            sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "        else:\n",
    "            raise ValueError(\"Weights should be either 0, 1, dictionnary or list.\")\n",
    "    elif isinstance(weights, dict):\n",
    "        # custom weights per class\n",
    "        need_shuffle = False\n",
    "        samples_weight = np.array([weights[t] for t in y_scored_train])\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    else:\n",
    "        # custom weights\n",
    "        if len(weights) != len(y_scored_train):\n",
    "            raise ValueError(\"Custom weights should match number of train samples.\")\n",
    "        need_shuffle = False\n",
    "        samples_weight = np.array(weights)\n",
    "        sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        TrainDataset(X_train.astype(np.float32), y_scored_train, y_nscored_train),\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        shuffle=need_shuffle,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=drop_last,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    # valid_dataloaders = []\n",
    "    # for X, y in [(X_valid,y_valid)]:\n",
    "    #     valid_dataloaders.append(\n",
    "    #         DataLoader(\n",
    "    #             ValidDataset(X.astype(np.float32), y),\n",
    "    #             batch_size=batch_size,\n",
    "    #             shuffle=False,\n",
    "    #             num_workers=num_workers,\n",
    "    #             pin_memory=pin_memory\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    valid_dataloaders = DataLoader(\n",
    "        ValidDataset(X_valid.astype(np.float32), y_valid),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "\n",
    "    return train_dataloader, valid_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, BatchNorm1d, ReLU, PReLU,LeakyReLU\n",
    "import numpy as np\n",
    "from pytorch_tabnet import sparsemax\n",
    "\n",
    "\n",
    "def initialize_non_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(4*input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "def initialize_glu(module, input_dim, output_dim):\n",
    "    gain_value = np.sqrt((input_dim+output_dim)/np.sqrt(input_dim))\n",
    "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
    "    # torch.nn.init.zeros_(module.bias)\n",
    "    return\n",
    "\n",
    "\n",
    "class GBN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Ghost Batch Normalization\n",
    "        https://arxiv.org/abs/1705.08741\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, virtual_batch_size=128, momentum=0.01):\n",
    "        super(GBN, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.bn = BatchNorm1d(self.input_dim, momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
    "        res = [self.bn(x_) for x_ in chunks]\n",
    "\n",
    "        return torch.cat(res, dim=0)\n",
    "\n",
    "\n",
    "class TabNetNoEmbeddings(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim1,output_dim2,\n",
    "                 n_d=8, n_a=8,\n",
    "                 n_steps=3, gamma=1.3,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,\n",
    "                 virtual_batch_size=128, momentum=0.02,\n",
    "                 mask_type=\"sparsemax\"):\n",
    "        \"\"\"\n",
    "        Defines main part of the TabNet network without the embedding layers.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Number of features\n",
    "        output_dim : int or list of int for multi task classification\n",
    "            Dimension of network output\n",
    "            examples : one for regression, 2 for binary classification etc...\n",
    "        n_d : int\n",
    "            Dimension of the prediction  layer (usually between 4 and 64)\n",
    "        n_a : int\n",
    "            Dimension of the attention  layer (usually between 4 and 64)\n",
    "        n_steps : int\n",
    "            Number of sucessive steps in the newtork (usually betwenn 3 and 10)\n",
    "        gamma : float\n",
    "            Float above 1, scaling factor for attention updates (usually betwenn 1.0 to 2.0)\n",
    "        n_independent : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        n_shared : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        epsilon : float\n",
    "            Avoid log(0), this should be kept very low\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in all batch norm\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(TabNetNoEmbeddings, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "        self.is_multi_task = isinstance(output_dim1, list)\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.mask_type = mask_type\n",
    "        self.initial_bn = BatchNorm1d(self.input_dim, momentum=0.01)\n",
    "\n",
    "        if self.n_shared > 0:\n",
    "            shared_feat_transform = torch.nn.ModuleList()\n",
    "            for i in range(self.n_shared):\n",
    "                if i == 0:\n",
    "                    shared_feat_transform.append(Linear(self.input_dim,\n",
    "                                                        2*(n_d + n_a),\n",
    "                                                        bias=False))\n",
    "                else:\n",
    "                    shared_feat_transform.append(Linear(n_d + n_a, 2*(n_d + n_a), bias=False))\n",
    "\n",
    "        else:\n",
    "            shared_feat_transform = None\n",
    "\n",
    "        self.initial_splitter = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
    "                                                n_glu_independent=self.n_independent,\n",
    "                                                virtual_batch_size=self.virtual_batch_size,\n",
    "                                                momentum=momentum)\n",
    "\n",
    "        self.feat_transformers = torch.nn.ModuleList()\n",
    "        self.att_transformers = torch.nn.ModuleList()\n",
    "\n",
    "        for step in range(n_steps):\n",
    "            transformer = FeatTransformer(self.input_dim, n_d+n_a, shared_feat_transform,\n",
    "                                          n_glu_independent=self.n_independent,\n",
    "                                          virtual_batch_size=self.virtual_batch_size,\n",
    "                                          momentum=momentum)\n",
    "            attention = AttentiveTransformer(n_a, self.input_dim,\n",
    "                                             virtual_batch_size=self.virtual_batch_size,\n",
    "                                             momentum=momentum,\n",
    "                                             mask_type=self.mask_type)\n",
    "            self.feat_transformers.append(transformer)\n",
    "            self.att_transformers.append(attention)\n",
    "\n",
    "        if self.is_multi_task:\n",
    "            self.multi_task_mappings1 = torch.nn.ModuleList()\n",
    "            for task_dim in output_dim1:\n",
    "                task_mapping = Linear(n_d, task_dim, bias=False)\n",
    "                initialize_non_glu(task_mapping, n_d, task_dim)\n",
    "                self.multi_task_mappings1.append(task_mapping)\n",
    "\n",
    "            # self.multi_task_mappings2 = torch.nn.ModuleList()\n",
    "            # for task_dim in output_dim2:\n",
    "            #     task_mapping = Linear(n_d, task_dim, bias=False)\n",
    "            #     initialize_non_glu(task_mapping, n_d, task_dim)\n",
    "            #     self.multi_task_mappings2.append(task_mapping)\n",
    "        else:\n",
    "            self.final_mapping1 = Linear(n_d, output_dim1, bias=False)\n",
    "            initialize_non_glu(self.final_mapping1, n_d, output_dim1)\n",
    "            # self.final_mapping2 = Linear(n_d, output_dim2, bias=False)\n",
    "            # initialize_non_glu(self.final_mapping2, n_d, output_dim2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = 0\n",
    "        x = self.initial_bn(x)\n",
    "\n",
    "        prior = torch.ones(x.shape).to(x.device)\n",
    "        M_loss = 0\n",
    "        att = self.initial_splitter(x)[:, self.n_d:]\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            M = self.att_transformers[step](prior, att)\n",
    "            M_loss += torch.mean(torch.sum(torch.mul(M, torch.log(M+self.epsilon)),\n",
    "                                           dim=1))\n",
    "            # update prior\n",
    "            prior = torch.mul(self.gamma - M, prior)\n",
    "            # output\n",
    "            masked_x = torch.mul(M, x)\n",
    "            out = self.feat_transformers[step](masked_x)\n",
    "            d = PReLU().to(x.device)(out[:, :self.n_d])\n",
    "            res = torch.add(res, d)\n",
    "            # update attention\n",
    "            att = out[:, self.n_d:]\n",
    "\n",
    "        M_loss /= self.n_steps\n",
    "\n",
    "        if self.is_multi_task:\n",
    "            # Result will be in list format\n",
    "            out1 = []\n",
    "            for task_mapping in self.multi_task_mappings1:\n",
    "                out1.append(task_mapping(res))\n",
    "            # out2 = []\n",
    "            # for task_mapping in self.multi_task_mappings2:\n",
    "            #     out2.append(task_mapping(res))\n",
    "        else:\n",
    "            out1 = self.final_mapping1(res)\n",
    "            # out2 = self.final_mapping2(res)\n",
    "        # return out1,out2, M_loss\n",
    "        return out1, M_loss\n",
    "\n",
    "    def forward_masks(self, x):\n",
    "        x = self.initial_bn(x)\n",
    "\n",
    "        prior = torch.ones(x.shape).to(x.device)\n",
    "        M_explain = torch.zeros(x.shape).to(x.device)\n",
    "        att = self.initial_splitter(x)[:, self.n_d:]\n",
    "        masks = {}\n",
    "\n",
    "        for step in range(self.n_steps):\n",
    "            M = self.att_transformers[step](prior, att)\n",
    "            masks[step] = M\n",
    "            # update prior\n",
    "            prior = torch.mul(self.gamma - M, prior)\n",
    "            # output\n",
    "            masked_x = torch.mul(M, x)\n",
    "            out = self.feat_transformers[step](masked_x)\n",
    "            d = PReLU().to(x.device)(out[:, :self.n_d])\n",
    "            # explain\n",
    "            step_importance = torch.sum(d, dim=1)\n",
    "            M_explain += torch.mul(M, step_importance.unsqueeze(dim=1))\n",
    "            # update attention\n",
    "            att = out[:, self.n_d:]\n",
    "\n",
    "        return M_explain, masks\n",
    "\n",
    "\n",
    "class TabNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim1,output_dim2, n_d=8, n_a=8,\n",
    "                 n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,\n",
    "                 virtual_batch_size=128, momentum=0.02, device_name='auto',\n",
    "                 mask_type=\"sparsemax\"):\n",
    "        \"\"\"\n",
    "        Defines TabNet network\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Initial number of features\n",
    "        output_dim : int\n",
    "            Dimension of network output\n",
    "            examples : one for regression, 2 for binary classification etc...\n",
    "        n_d : int\n",
    "            Dimension of the prediction  layer (usually between 4 and 64)\n",
    "        n_a : int\n",
    "            Dimension of the attention  layer (usually between 4 and 64)\n",
    "        n_steps : int\n",
    "            Number of sucessive steps in the newtork (usually betwenn 3 and 10)\n",
    "        gamma : float\n",
    "            Float above 1, scaling factor for attention updates (usually betwenn 1.0 to 2.0)\n",
    "        cat_idxs : list of int\n",
    "            Index of each categorical column in the dataset\n",
    "        cat_dims : list of int\n",
    "            Number of categories in each categorical column\n",
    "        cat_emb_dim : int or list of int\n",
    "            Size of the embedding of categorical features\n",
    "            if int, all categorical features will have same embedding size\n",
    "            if list of int, every corresponding feature will have specific size\n",
    "        n_independent : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        n_shared : int\n",
    "            Number of independent GLU layer in each GLU block (default 2)\n",
    "        epsilon : float\n",
    "            Avoid log(0), this should be kept very low\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in all batch norm\n",
    "        device_name : {'auto', 'cuda', 'cpu'}\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(TabNet, self).__init__()\n",
    "        self.cat_idxs = cat_idxs or []\n",
    "        self.cat_dims = cat_dims or []\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.mask_type = mask_type\n",
    "\n",
    "        if self.n_steps <= 0:\n",
    "            raise ValueError(\"n_steps should be a positive integer.\")\n",
    "        if self.n_independent == 0 and self.n_shared == 0:\n",
    "            raise ValueError(\"n_shared and n_independant can't be both zero.\")\n",
    "\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.embedder = EmbeddingGenerator(input_dim, cat_dims, cat_idxs, cat_emb_dim)\n",
    "        self.post_embed_dim = self.embedder.post_embed_dim\n",
    "        self.tabnet = TabNetNoEmbeddings(self.post_embed_dim, output_dim1,output_dim2, n_d, n_a, n_steps,\n",
    "                                         gamma, n_independent, n_shared, epsilon,\n",
    "                                         virtual_batch_size, momentum, mask_type)\n",
    "\n",
    "        # Defining device\n",
    "        if device_name == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device_name = 'cuda'\n",
    "            else:\n",
    "                device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedder(x)\n",
    "        return self.tabnet(x)\n",
    "\n",
    "    def forward_masks(self, x):\n",
    "        x = self.embedder(x)\n",
    "        return self.tabnet.forward_masks(x)\n",
    "\n",
    "\n",
    "class AttentiveTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 virtual_batch_size=128,\n",
    "                 momentum=0.02,\n",
    "                 mask_type=\"sparsemax\"):\n",
    "        \"\"\"\n",
    "        Initialize an attention transformer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Input size\n",
    "        output_dim : int\n",
    "            Outpu_size\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        mask_type : str\n",
    "            Either \"sparsemax\" or \"entmax\" : this is the masking function to use\n",
    "        \"\"\"\n",
    "        super(AttentiveTransformer, self).__init__()\n",
    "        self.fc = Linear(input_dim, output_dim, bias=False)\n",
    "        initialize_non_glu(self.fc, input_dim, output_dim)\n",
    "        self.bn = GBN(output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "        if mask_type == \"sparsemax\":\n",
    "            # Sparsemax\n",
    "            self.selector = sparsemax.Sparsemax(dim=-1)\n",
    "        elif mask_type == \"entmax\":\n",
    "            # Entmax\n",
    "            self.selector = sparsemax.Entmax15(dim=-1)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Please choose either sparsemax\" +\n",
    "                                      \"or entmax as masktype\")\n",
    "\n",
    "    def forward(self, priors, processed_feat):\n",
    "        x = self.fc(processed_feat)\n",
    "        x = self.bn(x)\n",
    "        x = torch.mul(x, priors)\n",
    "        x = self.selector(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatTransformer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, shared_layers, n_glu_independent,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(FeatTransformer, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize a feature transformer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Input size\n",
    "        output_dim : int\n",
    "            Outpu_size\n",
    "        shared_layers : torch.nn.ModuleList\n",
    "            The shared block that should be common to every step\n",
    "        n_glu_independant : int\n",
    "            Number of independent GLU layers\n",
    "        virtual_batch_size : int\n",
    "            Batch size for Ghost Batch Normalization within GLU block(s)\n",
    "        momentum : float\n",
    "            Float value between 0 and 1 which will be used for momentum in batch norm\n",
    "        \"\"\"\n",
    "\n",
    "        params = {\n",
    "            'n_glu': n_glu_independent,\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        if shared_layers is None:\n",
    "            # no shared layers\n",
    "            self.shared = torch.nn.Identity()\n",
    "            is_first = True\n",
    "        else:\n",
    "            self.shared = GLU_Block(input_dim, output_dim,\n",
    "                                    first=True,\n",
    "                                    shared_layers=shared_layers,\n",
    "                                    n_glu=len(shared_layers),\n",
    "                                    virtual_batch_size=virtual_batch_size,\n",
    "                                    momentum=momentum)\n",
    "            is_first = False\n",
    "\n",
    "        if n_glu_independent == 0:\n",
    "            # no independent layers\n",
    "            self.specifics = torch.nn.Identity()\n",
    "        else:\n",
    "            spec_input_dim = input_dim if is_first else output_dim\n",
    "            self.specifics = GLU_Block(spec_input_dim, output_dim,\n",
    "                                       first=is_first,\n",
    "                                       **params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        x = self.specifics(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Block(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Independant GLU block, specific to each step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, n_glu=2, first=False, shared_layers=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Block, self).__init__()\n",
    "        self.first = first\n",
    "        self.shared_layers = shared_layers\n",
    "        self.n_glu = n_glu\n",
    "        self.glu_layers = torch.nn.ModuleList()\n",
    "\n",
    "        params = {\n",
    "            'virtual_batch_size': virtual_batch_size,\n",
    "            'momentum': momentum\n",
    "        }\n",
    "\n",
    "        fc = shared_layers[0] if shared_layers else None\n",
    "        self.glu_layers.append(GLU_Layer(input_dim, output_dim,\n",
    "                                         fc=fc,\n",
    "                                         **params))\n",
    "        for glu_id in range(1, self.n_glu):\n",
    "            fc = shared_layers[glu_id] if shared_layers else None\n",
    "            self.glu_layers.append(GLU_Layer(output_dim, output_dim,\n",
    "                                             fc=fc,\n",
    "                                             **params))\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = torch.sqrt(torch.FloatTensor([0.5]).to(x.device))\n",
    "        if self.first:  # the first layer of the block has no scale multiplication\n",
    "            x = self.glu_layers[0](x)\n",
    "            layers_left = range(1, self.n_glu)\n",
    "        else:\n",
    "            layers_left = range(self.n_glu)\n",
    "\n",
    "        for glu_id in layers_left:\n",
    "            x = torch.add(x, self.glu_layers[glu_id](x))\n",
    "            x = x*scale\n",
    "        return x\n",
    "\n",
    "\n",
    "class GLU_Layer(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, fc=None,\n",
    "                 virtual_batch_size=128, momentum=0.02):\n",
    "        super(GLU_Layer, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        if fc:\n",
    "            self.fc = fc\n",
    "        else:\n",
    "            self.fc = Linear(input_dim, 2*output_dim, bias=False)\n",
    "        initialize_glu(self.fc, input_dim, 2*output_dim)\n",
    "\n",
    "        self.bn = GBN(2*output_dim, virtual_batch_size=virtual_batch_size,\n",
    "                      momentum=momentum)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        out = torch.mul(x[:, :self.output_dim], torch.sigmoid(x[:, self.output_dim:]))\n",
    "        return out\n",
    "\n",
    "\n",
    "class EmbeddingGenerator(torch.nn.Module):\n",
    "    \"\"\"\n",
    "        Classical embeddings generator\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, cat_dims, cat_idxs, cat_emb_dim):\n",
    "        \"\"\" This is an embedding module for an entier set of features\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim : int\n",
    "            Number of features coming as input (number of columns)\n",
    "        cat_dims : list of int\n",
    "            Number of modalities for each categorial features\n",
    "            If the list is empty, no embeddings will be done\n",
    "        cat_idxs : list of int\n",
    "            Positional index for each categorical features in inputs\n",
    "        cat_emb_dim : int or list of int\n",
    "            Embedding dimension for each categorical features\n",
    "            If int, the same embdeding dimension will be used for all categorical features\n",
    "        \"\"\"\n",
    "        super(EmbeddingGenerator, self).__init__()\n",
    "        if cat_dims == [] or cat_idxs == []:\n",
    "            self.skip_embedding = True\n",
    "            self.post_embed_dim = input_dim\n",
    "            return\n",
    "\n",
    "        self.skip_embedding = False\n",
    "        if isinstance(cat_emb_dim, int):\n",
    "            self.cat_emb_dims = [cat_emb_dim]*len(cat_idxs)\n",
    "        else:\n",
    "            self.cat_emb_dims = cat_emb_dim\n",
    "\n",
    "        # check that all embeddings are provided\n",
    "        if len(self.cat_emb_dims) != len(cat_dims):\n",
    "            msg = \"\"\" cat_emb_dim and cat_dims must be lists of same length, got {len(self.cat_emb_dims)}\n",
    "                      and {len(cat_dims)}\"\"\"\n",
    "            raise ValueError(msg)\n",
    "        self.post_embed_dim = int(input_dim + np.sum(self.cat_emb_dims) - len(self.cat_emb_dims))\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleList()\n",
    "\n",
    "        # Sort dims by cat_idx\n",
    "        sorted_idxs = np.argsort(cat_idxs)\n",
    "        cat_dims = [cat_dims[i] for i in sorted_idxs]\n",
    "        self.cat_emb_dims = [self.cat_emb_dims[i] for i in sorted_idxs]\n",
    "\n",
    "        for cat_dim, emb_dim in zip(cat_dims, self.cat_emb_dims):\n",
    "            self.embeddings.append(torch.nn.Embedding(cat_dim, emb_dim))\n",
    "\n",
    "        # record continuous indices\n",
    "        self.continuous_idx = torch.ones(input_dim, dtype=torch.bool)\n",
    "        self.continuous_idx[cat_idxs] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply embdeddings to inputs\n",
    "        Inputs should be (batch_size, input_dim)\n",
    "        Outputs will be of size (batch_size, self.post_embed_dim)\n",
    "        \"\"\"\n",
    "        if self.skip_embedding:\n",
    "            # no embeddings required\n",
    "            return x\n",
    "\n",
    "        cols = []\n",
    "        cat_feat_counter = 0\n",
    "        for feat_init_idx, is_continuous in enumerate(self.continuous_idx):\n",
    "            # Enumerate through continuous idx boolean mask to apply embeddings\n",
    "            if is_continuous:\n",
    "                cols.append(x[:, feat_init_idx].float().view(-1, 1))\n",
    "            else:\n",
    "                cols.append(self.embeddings[cat_feat_counter](x[:, feat_init_idx].long()))\n",
    "                cat_feat_counter += 1\n",
    "        # concat\n",
    "        post_embeddings = torch.cat(cols, dim=1)\n",
    "        return post_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabModel(BaseEstimator):\n",
    "    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n",
    "                 lambda_sparse=1e-3, seed=0,\n",
    "                 clip_value=1, verbose=1,\n",
    "                 optimizer_fn=torch.optim.Adam,\n",
    "                 optimizer_params=dict(lr=2e-2),\n",
    "                 scheduler_params=None, scheduler_fn=None,\n",
    "                 mask_type=\"sparsemax\",\n",
    "                 input_dim=None, output_dim1=None,output_dim2=None,\n",
    "                 device_name='auto'):\n",
    "        \"\"\" Class for TabNet model\n",
    "        Parameters\n",
    "        ----------\n",
    "            device_name: str\n",
    "                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.cat_idxs = cat_idxs\n",
    "        self.cat_dims = cat_dims\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.clip_value = clip_value\n",
    "        self.verbose = verbose\n",
    "        self.optimizer_fn = optimizer_fn\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.device_name = device_name\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "        self.mask_type = mask_type\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim1 = output_dim1\n",
    "        self.output_dim2 = output_dim2\n",
    "\n",
    "        #self.batch_size = 1024\n",
    "        self.batch_size = 1024\n",
    "\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        # Defining device\n",
    "        if device_name == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device_name = 'cuda'\n",
    "            else:\n",
    "                device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        print(f\"Device used : {self.device}\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def construct_loaders(self, X_train, y_scored_train,y_nscored_train, X_valid, y_valid,\n",
    "                          weights, batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define construct_loaders to use this base class')\n",
    "\n",
    "    def init_network(\n",
    "                     self,\n",
    "                     input_dim,\n",
    "                     output_dim1,\n",
    "                     output_dim2,\n",
    "                     n_d,\n",
    "                     n_a,\n",
    "                     n_steps,\n",
    "                     gamma,\n",
    "                     cat_idxs,\n",
    "                     cat_dims,\n",
    "                     cat_emb_dim,\n",
    "                     n_independent,\n",
    "                     n_shared,\n",
    "                     epsilon,\n",
    "                     virtual_batch_size,\n",
    "                     momentum,\n",
    "                     device_name,\n",
    "                     mask_type,\n",
    "                     ):\n",
    "        self.network = TabNet(\n",
    "            input_dim,\n",
    "            output_dim1,\n",
    "            output_dim2,\n",
    "            n_d=n_d,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=cat_emb_dim,\n",
    "            n_independent=n_independent,\n",
    "            n_shared=n_shared,\n",
    "            epsilon=epsilon,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            momentum=momentum,\n",
    "            device_name=device_name,\n",
    "            mask_type=mask_type).to(self.device)\n",
    "\n",
    "        self.reducing_matrix = create_explain_matrix(\n",
    "            self.network.input_dim,\n",
    "            self.network.cat_emb_dim,\n",
    "            self.network.cat_idxs,\n",
    "            self.network.post_embed_dim)\n",
    "\n",
    "    def fit(self, X_train, y_scored_train, y_nscored_train, X_valid=None, y_valid=None, loss_fn=None,loss_tr=None,\n",
    "            weights=0, max_epochs=100, patience=10, batch_size=1024,\n",
    "            virtual_batch_size=128, num_workers=0, drop_last=False):\n",
    "        \"\"\"Train a neural network stored in self.network\n",
    "        Using train_dataloader for training data and\n",
    "        valid_dataloader for validation.\n",
    "        Parameters\n",
    "        ----------\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            weights : bool or dictionnary\n",
    "                0 for no balancing\n",
    "                1 for automated balancing\n",
    "                dict for custom weights per class\n",
    "            max_epochs : int\n",
    "                Maximum number of epochs during training\n",
    "            patience : int\n",
    "                Number of consecutive non improving epoch before early stopping\n",
    "            batch_size : int\n",
    "                Training batch size\n",
    "            virtual_batch_size : int\n",
    "                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n",
    "            num_workers : int\n",
    "                Number of workers used in torch.utils.data.DataLoader\n",
    "            drop_last : bool\n",
    "                Whether to drop last batch during training\n",
    "        \"\"\"\n",
    "        # update model name\n",
    "\n",
    "        self.update_fit_params(X_train, y_scored_train,y_nscored_train, X_valid, y_valid, loss_fn,loss_tr,\n",
    "                               weights, max_epochs, patience, batch_size,virtual_batch_size, num_workers, drop_last)\n",
    "\n",
    "\n",
    "        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n",
    "                                                                    y_scored_train,\n",
    "                                                                    y_nscored_train,\n",
    "                                                                    X_valid,\n",
    "                                                                    y_valid,\n",
    "                                                                    self.updated_weights,\n",
    "                                                                    self.batch_size,\n",
    "                                                                    self.num_workers,\n",
    "                                                                    self.drop_last)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim1=self.output_dim1,\n",
    "            output_dim2=self.output_dim2,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=self.virtual_batch_size,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "\n",
    "        self.optimizer = self.optimizer_fn(self.network.parameters(),\n",
    "                                           **self.optimizer_params)\n",
    "\n",
    "        if self.scheduler_fn:\n",
    "            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "        self.losses_train = []\n",
    "        self.losses_valid = []\n",
    "        self.learning_rates = []\n",
    "        self.metrics_train = []\n",
    "        self.metrics_valid = []\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(\"Will train until validation stopping metric\",\n",
    "                  f\"hasn't improved in {self.patience} rounds.\")\n",
    "            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n",
    "            print('---------------------------------------')\n",
    "            print(msg_epoch)\n",
    "\n",
    "        total_time = 0\n",
    "        while (self.epoch < self.max_epochs):\n",
    "            #  and self.patience_counter < self.patience):\n",
    "            starting_time = time.time()\n",
    "            # updates learning rate history\n",
    "            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n",
    "\n",
    "            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n",
    "\n",
    "            # leaving it here, may be used for callbacks later\n",
    "            self.losses_train.append(fit_metrics['train']['loss_avg'])\n",
    "            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n",
    "            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n",
    "            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n",
    "\n",
    "            stopping_loss = fit_metrics['valid']['stopping_loss']\n",
    "            if stopping_loss < self.best_cost:\n",
    "                self.best_cost = stopping_loss\n",
    "                self.patience_counter = 0\n",
    "                # Saving model\n",
    "                self.best_network = deepcopy(self.network)\n",
    "                has_improved = True\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                has_improved=False\n",
    "            self.epoch += 1\n",
    "            total_time += time.time() - starting_time\n",
    "\n",
    "\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                if self.epoch % self.verbose == 0:\n",
    "                    separator = \"|\"\n",
    "                    msg_epoch = f\"| {self.epoch:<5} | \"\n",
    "                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n",
    "                    msg_epoch += f\" {has_improved}\"\n",
    "                    print(msg_epoch)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            if self.patience_counter == self.patience:\n",
    "                print(f\"Early stopping occured at epoch {self.epoch}\")\n",
    "            print(f\"Training done in {total_time:.3f} seconds. Best loss : {self.best_cost:.5f}\")\n",
    "            print('---------------------------------------')\n",
    "\n",
    "        self.history = {\"train\": {\"loss\": self.losses_train,\n",
    "                                  \"metric\": self.metrics_train,\n",
    "                                  \"lr\": self.learning_rates},\n",
    "                        \"valid\": {\"loss\": self.losses_valid,\n",
    "                                  \"metric\": self.metrics_valid}}\n",
    "        # load best models post training\n",
    "        self.load_best_model()\n",
    "\n",
    "        # compute feature importance once the best model is defined\n",
    "        self._compute_feature_importances(train_dataloader)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saving model with two distinct files.\n",
    "        \"\"\"\n",
    "        saved_params = {}\n",
    "        for key, val in self.get_params().items():\n",
    "            if isinstance(val, type):\n",
    "                # Don't save torch specific params\n",
    "                continue\n",
    "            else:\n",
    "                saved_params[key] = val\n",
    "\n",
    "        # Create folder\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save models params\n",
    "        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(saved_params, f)\n",
    "\n",
    "        # Save state_dict\n",
    "        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n",
    "        shutil.make_archive(path, 'zip', path)\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Successfully saved model at {path}.zip\")\n",
    "        return f\"{path}.zip\"\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                with zipfile.ZipFile(filepath) as z:\n",
    "                    with z.open(\"model_params.json\") as f:\n",
    "                        loaded_params = json.load(f)\n",
    "                    with z.open(\"network.pt\") as f:\n",
    "                        try:\n",
    "                            saved_state_dict = torch.load(f)\n",
    "                        except io.UnsupportedOperation:\n",
    "                            # In Python <3.7, the returned file object is not seekable (which at least\n",
    "                            # some versions of PyTorch require) - so we'll try buffering it in to a\n",
    "                            # BytesIO instead:\n",
    "                            saved_state_dict = torch.load(io.BytesIO(f.read()))\n",
    "                            \n",
    "            except:\n",
    "                with open(os.path.join(filepath, \"model_params.json\")) as f:\n",
    "                        loaded_params = json.load(f)\n",
    "\n",
    "                saved_state_dict = torch.load(os.path.join(filepath, \"network.pt\"), map_location=\"cpu\")\n",
    " \n",
    "        except KeyError:\n",
    "            raise KeyError(\"Your zip file is missing at least one component\")\n",
    "\n",
    "        #print(loaded_params)\n",
    "        if torch.cuda.is_available():\n",
    "            device_name = 'cuda'\n",
    "        else:\n",
    "            device_name = 'cpu'\n",
    "        loaded_params[\"device_name\"] = device_name\n",
    "        self.__init__(**loaded_params)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim1=self.output_dim1,\n",
    "            output_dim2=self.output_dim2,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=1024,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "        self.network.load_state_dict(saved_state_dict)\n",
    "        self.network.eval()\n",
    "        return\n",
    "\n",
    "    def fit_epoch(self, train_dataloader, valid_dataloader):\n",
    "        \"\"\"\n",
    "        Evaluates and updates network for one epoch.\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with valid set\n",
    "        \"\"\"\n",
    "        train_metrics = self.train_epoch(train_dataloader)\n",
    "        valid_metrics = self.predict_epoch(valid_dataloader)\n",
    "\n",
    "        fit_metrics = {'train': train_metrics,\n",
    "                       'valid': valid_metrics}\n",
    "\n",
    "        return fit_metrics\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_batch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_batch to use this base class')\n",
    "\n",
    "    def load_best_model(self):\n",
    "        if self.best_network is not None:\n",
    "            self.network = self.best_network\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem or the last class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict to use this base class')\n",
    "\n",
    "    def explain(self, X):\n",
    "        \"\"\"\n",
    "        Return local explanation\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            M_explain: matrix\n",
    "                Importance per sample, per columns.\n",
    "            masks: matrix\n",
    "                Sparse matrix showing attention masks used by network.\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            for key, value in masks.items():\n",
    "                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n",
    "                                            self.reducing_matrix)\n",
    "\n",
    "            if batch_nb == 0:\n",
    "                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                             self.reducing_matrix)\n",
    "                res_masks = masks\n",
    "            else:\n",
    "                res_explain = np.vstack([res_explain,\n",
    "                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                                        self.reducing_matrix)])\n",
    "                for key, value in masks.items():\n",
    "                    res_masks[key] = np.vstack([res_masks[key], value])\n",
    "        return res_explain, res_masks\n",
    "\n",
    "    def _compute_feature_importances(self, loader):\n",
    "        self.network.eval()\n",
    "        feature_importances_ = np.zeros((self.network.post_embed_dim))\n",
    "        for data, targets,_ in loader:\n",
    "            data = data.to(self.device).float()\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n",
    "\n",
    "        feature_importances_ = csc_matrix.dot(feature_importances_,\n",
    "                                              self.reducing_matrix)\n",
    "        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n",
    "        \n",
    "\n",
    "\n",
    "class TabNetRegressor(TabModel):\n",
    "\n",
    "    def construct_loaders(self, X_train, y_scored_train,y_nscored_train, X_valid, y_valid, weights,\n",
    "                          batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if isinstance(weights, int):\n",
    "            if weights == 1:\n",
    "                raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "        if isinstance(weights, dict):\n",
    "            raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "\n",
    "\n",
    "\n",
    "        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n",
    "                                                                y_scored_train,\n",
    "                                                                y_nscored_train,\n",
    "                                                                X_valid,\n",
    "                                                                y_valid,\n",
    "                                                                weights,\n",
    "                                                                batch_size,\n",
    "                                                                num_workers,\n",
    "                                                                drop_last)\n",
    "        return train_dataloader, valid_dataloader\n",
    "\n",
    "    def update_fit_params(self, X_train, y_scored_train, y_nscored_train, X_valid, y_valid, loss_fn, loss_tr,\n",
    "                          weights, max_epochs, patience,batch_size, virtual_batch_size, num_workers, drop_last):\n",
    "\n",
    "\n",
    "        if loss_fn is None:\n",
    "            self.loss_fn = torch.nn.functional.mse_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "            self.loss_tr = loss_tr\n",
    "\n",
    "        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n",
    "        self.input_dim = X_train.shape[1]\n",
    "\n",
    "        if len(y_scored_train.shape) == 1:\n",
    "            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n",
    "                                if doing single regression.\"\"\")\n",
    "        assert y_scored_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n",
    "        self.output_dim1 = y_scored_train.shape[1]\n",
    "        self.output_dim2 = y_nscored_train.shape[1]\n",
    "\n",
    "        self.updated_weights = weights\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        # Initialize counters and histories.\n",
    "        self.patience_counter = 0\n",
    "        self.epoch = 0\n",
    "        self.best_cost = np.inf\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.train()\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets_scored, targets_nscored in train_loader:\n",
    "            batch_outs = self.train_batch(data, targets_scored, targets_nscored)\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "        # stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n",
    "        total_loss = total_loss / len(train_loader)\n",
    "\n",
    "        epoch_metrics = {'loss_avg': total_loss,\n",
    "                         'stopping_loss': total_loss,\n",
    "                         }\n",
    "\n",
    "        # if self.scheduler is not None:\n",
    "        #     self.scheduler.step()\n",
    "            \n",
    "        return epoch_metrics\n",
    "\n",
    "    def train_batch(self, data, targets_scored, targets_nscored):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        self.network.train()\n",
    "        data = data.to(self.device).float()\n",
    "\n",
    "        \n",
    "\n",
    "        targets_scored = targets_scored.to(self.device).float()\n",
    "        targets_nscored = targets_nscored.to(self.device).float()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # output1,output2, M_loss = self.network(data)\n",
    "        output1, M_loss = self.network(data)\n",
    "\n",
    "        loss1 = self.loss_fn(output1, targets_scored)\n",
    "        # loss2 = self.loss_fn(output2, targets_nscored)\n",
    "        loss = loss1 #+ loss2\n",
    "        \n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "\n",
    "        loss.backward()\n",
    "        if self.clip_value:\n",
    "            clip_grad_norm_(self.network.parameters(), self.clip_value)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output1,\n",
    "                      'y': targets_scored}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        self.network.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in loader:\n",
    "            batch_outs = self.predict_batch(data, targets)\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        stopping_loss = log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step(stopping_loss)\n",
    "\n",
    "        total_loss = total_loss / len(loader)\n",
    "        epoch_metrics = {'total_loss': total_loss,\n",
    "                         'stopping_loss': stopping_loss}\n",
    "\n",
    "        return epoch_metrics\n",
    "\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        data = data.to(self.device).float()\n",
    "        targets = targets.to(self.device).float()\n",
    "\n",
    "        # output,_, M_loss = self.network(data)\n",
    "        output,M_loss = self.network(data)\n",
    "       \n",
    "        loss = self.loss_fn(output, targets)\n",
    "        #print(self.loss_fn, loss)\n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "        #print(loss)\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        results = []\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            #output,_, M_loss = self.network(data)\n",
    "            output, M_loss = self.network(data)\n",
    "            predictions = output.cpu().detach().numpy()\n",
    "            results.append(predictions)\n",
    "        res = np.vstack(results)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "NFOLDS = 5\n",
    "\n",
    "EPOCHS = 300 #200\n",
    "PATIENCE_SCH=20 \n",
    "PATIENCE=35\n",
    "LEARNING_RATE = 1e-3 #1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "\n",
    "save_name = \"../data/tabnet-weights-public/tabnet-raw-public-step1/tabnet_raw_step1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCATE DRUGS\n",
    "vc = scored.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc<=18].index.sort_values()\n",
    "vc2 = vc.loc[vc>18].index.sort_values()\n",
    "\n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}; dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = scored.groupby('drug_id')[targets_scored].mean().loc[vc1]\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_scored])):\n",
    "    dd = {k:fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "\n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_scored])):\n",
    "    dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "# ASSIGN FOLDS\n",
    "train_df = train_df.merge(drug,on=\"sig_id\")\n",
    "train_df['fold'] = train_df.drug_id.map(dct1)\n",
    "train_df.loc[train_df.fold.isna(),'fold'] =\\\n",
    "    train_df.loc[train_df.fold.isna(),'sig_id'].map(dct2)\n",
    "train_df.fold = train_df.fold.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feat_cols = [c for c in train_df.columns if c not in [\"sig_id\",\"drug_id\",\"fold\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # ASSIGN FOLDS\n",
    "# scored['fold'] = scored.drug_id.map(dct1)\n",
    "# scored.loc[scored.fold.isna(),'fold'] =\\\n",
    "#     scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n",
    "# scored.fold = scored.fold.astype('int8')\n",
    "\n",
    "\n",
    "# folds = train_df.copy()\n",
    "\n",
    "# mskf = MultilabelStratifiedKFold(n_splits=NFOLDS)\n",
    "\n",
    "# for f, (t_idx, v_idx) in enumerate(mskf.split(X=train_df, y=train_targets_scored)):\n",
    "#     folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "# folds['kfold'] = folds['kfold'].astype(int)\n",
    "# folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def run_training(fold, seed):\n",
    "\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train = train_df[train_df['fold'] != fold][feat_cols]\n",
    "    valid = train_df[train_df['fold'] == fold][feat_cols]\n",
    "\n",
    "\n",
    "    X_train, y_scored_train, y_nscored_train   = train.values, train_targets_scored.values[train.index, :], train_targets_nonscored.values[train.index, :]\n",
    "    X_val, y_val = valid.values, train_targets_scored.values[valid.index, :]\n",
    "    \n",
    "\n",
    "    # print(math.ceil(X_train.shape[0]/BATCH_SIZE))\n",
    "# \n",
    "    # model = TabNetRegressor(n_d=24, \n",
    "    #                         n_a=24, \n",
    "    #                         n_steps=1, \n",
    "    #                         gamma=1.3,\n",
    "    #                         # cat_dims=cat_dims, \n",
    "    #                         # cat_emb_dim=cat_emb_dim, \n",
    "    #                         # cat_idxs=cats_idx,\n",
    "    #                         lambda_sparse=0, \n",
    "    #                         optimizer_fn=torch.optim.Adam,\n",
    "    #                         optimizer_params=dict(lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY), \n",
    "    #                         mask_type='entmax', \n",
    "    #                         device_name=device, \n",
    "    #                         scheduler_params=dict(pct_start=0.1, div_factor=1e4, final_div_factor=1e5,\n",
    "    #                                           max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=math.ceil(x_train.shape[0]/BATCH_SIZE)), \n",
    "    #                         scheduler_fn=torch.optim.lr_scheduler.OneCycleLR)\n",
    "    model = TabNetRegressor(\n",
    "                            n_d = 28,\n",
    "                            n_a = 28,\n",
    "                            n_steps = 1,\n",
    "                            gamma = 1.4,\n",
    "                            n_independent=2,\n",
    "                            n_shared=1,\n",
    "                            momentum=0.02,\n",
    "                            epsilon=1e-15,\n",
    "                            lambda_sparse = 0,\n",
    "                            optimizer_fn = optim.Adam,\n",
    "                            optimizer_params = dict(lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY),\n",
    "                            mask_type = \"entmax\",\n",
    "                            scheduler_params = dict(\n",
    "                                mode = \"min\", patience = PATIENCE_SCH, min_lr = 1e-6, factor = 0.9, verbose = True),\n",
    "                            scheduler_fn = ReduceLROnPlateau,\n",
    "                            seed = seed,\n",
    "                            verbose = 1)\n",
    "\n",
    "                             \n",
    "\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "\n",
    "    # model.fit(X_train=x_train, \n",
    "    #           y_scored_train=y_scored_train,\n",
    "    #           y_nscored_train=y_nscored_train,  \n",
    "    #           X_valid=x_valid, \n",
    "    #           y_valid=y_valid,\n",
    "    #           max_epochs=EPOCHS,\n",
    "    #           patience=EPOCHS, \n",
    "    #           batch_size=BATCH_SIZE, \n",
    "    #           virtual_batch_size=128,\n",
    "    #           num_workers=0, \n",
    "    #           drop_last=False,\n",
    "    #           weights=0,\n",
    "    #           loss_fn=loss_fn,\n",
    "    #           loss_tr=loss_tr)\n",
    "\n",
    "    model.fit(\n",
    "        X_train = X_train,\n",
    "        y_scored_train=y_scored_train,\n",
    "        y_nscored_train=y_nscored_train, \n",
    "        # eval_set = [(X_val, y_val)],\n",
    "        X_valid=X_val, \n",
    "        y_valid=y_val,\n",
    "        # eval_name = [\"val\"],\n",
    "        # eval_metric = [\"logits_ll\"],\n",
    "        max_epochs = EPOCHS,\n",
    "        patience = PATIENCE,\n",
    "        batch_size = BATCH_SIZE, \n",
    "        virtual_batch_size = 32,\n",
    "        num_workers = 1,\n",
    "        drop_last = False,\n",
    "        # To use binary cross entropy because this is not a regression problem\n",
    "        loss_fn = F.binary_cross_entropy_with_logits\n",
    "    )\n",
    "\n",
    "\n",
    "    oof = np.zeros((train_df.shape[0], train_targets_scored.shape[1]))\n",
    "    \n",
    "    model.load_best_model()\n",
    "    preds = model.predict(X_val)\n",
    "    oof[valid.index] = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n",
    "\n",
    "    X_test = test_df[feat_cols].values\n",
    "    preds = model.predict(X_test)\n",
    "    predictions = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 200 #200\n",
    "PATIENCE_SCH=10 \n",
    "PATIENCE=35 #20\n",
    "LEARNING_RATE =2e-2 #1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 5\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "\n",
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((train_df.shape[0], train_targets_scored.shape[1]))\n",
    "    predictions = np.zeros((test_df.shape[0], train_targets_scored.shape[1]))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        print(f\"SEED {SEED} - FOLD {fold}\")\n",
    "        oof_, pred_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SEED [0] - FOLD 0\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 35 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  0.15390 |   0.04328 |   2.1        True\n| 2     |  0.02646 |   0.02326 |   4.0        True\n| 3     |  0.02194 |   0.02287 |   5.9        True\n| 4     |  0.02119 |   0.02205 |   7.8        True\n| 5     |  0.02049 |   0.02171 |   9.8        True\n| 6     |  0.02023 |   0.02122 |   11.7       True\n| 7     |  0.01988 |   0.02293 |   13.7       False\n| 8     |  0.02046 |   0.02098 |   15.6       True\n| 9     |  0.01947 |   0.02055 |   17.6       True\n| 10    |  0.01945 |   0.02165 |   19.5       False\n| 11    |  0.01909 |   0.01950 |   21.5       True\n| 12    |  0.01898 |   0.01974 |   23.4       False\n| 13    |  0.01911 |   0.02119 |   25.4       False\n| 14    |  0.01894 |   0.01958 |   27.4       False\n| 15    |  0.01926 |   0.02192 |   29.4       False\n| 16    |  0.01935 |   0.01982 |   31.3       False\nEpoch    17: reducing learning rate of group 0 to 8.9000e-02.\n| 17    |  0.01907 |   0.01968 |   33.3       False\n| 18    |  0.01906 |   0.01936 |   35.3       True\n| 19    |  0.01910 |   0.02063 |   37.2       False\n| 20    |  0.01896 |   0.01925 |   39.2       True\n| 21    |  0.01858 |   0.02014 |   41.1       False\n| 22    |  0.01900 |   0.01909 |   43.1       True\n| 23    |  0.01872 |   0.02020 |   45.1       False\n| 24    |  0.01892 |   0.01978 |   47.1       False\n| 25    |  0.01885 |   0.01960 |   49.0       False\n| 26    |  0.01869 |   0.01916 |   51.0       False\n| 27    |  0.01857 |   0.02004 |   52.9       False\nEpoch    28: reducing learning rate of group 0 to 7.9210e-02.\n| 28    |  0.01879 |   0.01968 |   54.8       False\n| 29    |  0.01836 |   0.01888 |   56.8       True\n| 30    |  0.01807 |   0.01913 |   58.7       False\n| 31    |  0.01843 |   0.01898 |   60.7       False\n| 32    |  0.01842 |   0.01914 |   62.6       False\n| 33    |  0.01822 |   0.01907 |   64.7       False\n| 34    |  0.01827 |   0.02084 |   66.5       False\nEpoch    35: reducing learning rate of group 0 to 7.0497e-02.\n| 35    |  0.01859 |   0.01892 |   68.5       False\n| 36    |  0.01835 |   0.01975 |   70.5       False\n| 37    |  0.01826 |   0.01919 |   72.5       False\n| 38    |  0.01838 |   0.01884 |   74.5       True\n| 39    |  0.01857 |   0.01908 |   76.5       False\n| 40    |  0.01824 |   0.01917 |   78.4       False\n| 41    |  0.01837 |   0.01910 |   80.5       False\n| 42    |  0.01832 |   0.01938 |   82.5       False\n| 43    |  0.01830 |   0.01891 |   84.5       False\n| 44    |  0.01830 |   0.01878 |   86.5       True\n| 45    |  0.01805 |   0.01891 |   88.6       False\n| 46    |  0.01819 |   0.01900 |   90.6       False\n| 47    |  0.01805 |   0.01919 |   92.7       False\n| 48    |  0.01843 |   0.01925 |   94.5       False\n| 49    |  0.01822 |   0.01903 |   96.5       False\nEpoch    50: reducing learning rate of group 0 to 6.2742e-02.\n| 50    |  0.01821 |   0.01918 |   98.6       False\n| 51    |  0.01818 |   0.01893 |   100.6      False\n| 52    |  0.01808 |   0.01879 |   102.6      False\n| 53    |  0.01812 |   0.01943 |   104.6      False\n| 54    |  0.01785 |   0.01890 |   106.6      False\n| 55    |  0.01803 |   0.01897 |   108.6      False\nEpoch    56: reducing learning rate of group 0 to 5.5841e-02.\n| 56    |  0.01825 |   0.01965 |   110.7      False\n| 57    |  0.01807 |   0.01934 |   112.8      False\n| 58    |  0.01785 |   0.01881 |   114.8      False\n| 59    |  0.01787 |   0.01866 |   116.9      True\n| 60    |  0.01762 |   0.01876 |   118.8      False\n| 61    |  0.01782 |   0.01855 |   120.8      True\n| 62    |  0.01803 |   0.01925 |   122.8      False\n| 63    |  0.01794 |   0.01920 |   124.8      False\n| 64    |  0.01804 |   0.01868 |   126.8      False\n| 65    |  0.01775 |   0.01894 |   128.8      False\n| 66    |  0.01797 |   0.01997 |   130.9      False\nEpoch    67: reducing learning rate of group 0 to 4.9698e-02.\n| 67    |  0.01805 |   0.01868 |   132.8      False\n| 68    |  0.01782 |   0.01875 |   134.9      False\n| 69    |  0.01778 |   0.01872 |   136.9      False\n| 70    |  0.01770 |   0.01853 |   139.0      True\n| 71    |  0.01768 |   0.01869 |   141.0      False\n| 72    |  0.01766 |   0.01879 |   143.1      False\n| 73    |  0.01760 |   0.01860 |   145.1      False\n| 74    |  0.01743 |   0.01853 |   147.1      True\n| 75    |  0.01762 |   0.01861 |   149.1      False\n| 76    |  0.01754 |   0.01853 |   151.2      False\n| 77    |  0.01770 |   0.01929 |   153.2      False\n| 78    |  0.01783 |   0.01845 |   155.3      True\n| 79    |  0.01778 |   0.01932 |   157.3      False\n| 80    |  0.01792 |   0.01849 |   159.4      False\n| 81    |  0.01789 |   0.01860 |   161.4      False\n| 82    |  0.01784 |   0.01857 |   163.4      False\n| 83    |  0.01788 |   0.01871 |   165.5      False\nEpoch    84: reducing learning rate of group 0 to 4.4231e-02.\n| 84    |  0.01802 |   0.01868 |   167.5      False\n| 85    |  0.01787 |   0.01849 |   169.5      False\n| 86    |  0.01756 |   0.01847 |   171.6      False\n| 87    |  0.01769 |   0.01865 |   173.6      False\n| 88    |  0.01773 |   0.01847 |   175.7      False\n| 89    |  0.01765 |   0.01867 |   177.6      False\nEpoch    90: reducing learning rate of group 0 to 3.9366e-02.\n| 90    |  0.01760 |   0.01868 |   179.7      False\n| 91    |  0.01754 |   0.01817 |   181.7      True\n| 92    |  0.01748 |   0.01833 |   183.7      False\n| 93    |  0.01744 |   0.01857 |   185.7      False\n| 94    |  0.01764 |   0.01835 |   187.7      False\n| 95    |  0.01762 |   0.01883 |   189.7      False\n| 96    |  0.01756 |   0.01842 |   191.8      False\nEpoch    97: reducing learning rate of group 0 to 3.5036e-02.\n| 97    |  0.01748 |   0.01833 |   193.8      False\n| 98    |  0.01746 |   0.01838 |   195.9      False\n| 99    |  0.01738 |   0.01831 |   197.9      False\n| 100   |  0.01736 |   0.01853 |   199.9      False\n| 101   |  0.01745 |   0.01830 |   201.9      False\n| 102   |  0.01746 |   0.01877 |   203.9      False\nEpoch   103: reducing learning rate of group 0 to 3.1182e-02.\n| 103   |  0.01760 |   0.01828 |   206.0      False\n| 104   |  0.01745 |   0.01830 |   208.1      False\n| 105   |  0.01735 |   0.01884 |   210.2      False\n| 106   |  0.01745 |   0.01832 |   212.2      False\n| 107   |  0.01734 |   0.01812 |   214.2      True\n| 108   |  0.01736 |   0.01832 |   216.2      False\n| 109   |  0.01744 |   0.01845 |   218.2      False\n| 110   |  0.01744 |   0.01844 |   220.2      False\n| 111   |  0.01733 |   0.01824 |   222.2      False\n| 112   |  0.01718 |   0.01814 |   224.3      False\nEpoch   113: reducing learning rate of group 0 to 2.7752e-02.\n| 113   |  0.01728 |   0.01867 |   226.4      False\n| 114   |  0.01736 |   0.01817 |   228.4      False\n| 115   |  0.01730 |   0.01842 |   230.4      False\n| 116   |  0.01727 |   0.01834 |   232.4      False\n| 117   |  0.01729 |   0.01809 |   234.5      True\n| 118   |  0.01732 |   0.01825 |   236.5      False\n| 119   |  0.01716 |   0.01819 |   238.5      False\n| 120   |  0.01722 |   0.01875 |   240.5      False\n| 121   |  0.01715 |   0.01820 |   242.5      False\n| 122   |  0.01709 |   0.01824 |   244.6      False\nEpoch   123: reducing learning rate of group 0 to 2.4699e-02.\n| 123   |  0.01710 |   0.01834 |   246.7      False\n| 124   |  0.01720 |   0.01819 |   248.7      False\n| 125   |  0.01715 |   0.01815 |   250.8      False\n| 126   |  0.01710 |   0.01827 |   252.8      False\n| 127   |  0.01710 |   0.01884 |   254.8      False\n| 128   |  0.01737 |   0.01821 |   256.9      False\nEpoch   129: reducing learning rate of group 0 to 2.1982e-02.\n| 129   |  0.01710 |   0.01809 |   258.9      False\n| 130   |  0.01712 |   0.01840 |   261.1      False\n| 131   |  0.01705 |   0.01805 |   263.1      True\n| 132   |  0.01701 |   0.01813 |   265.1      False\n| 133   |  0.01713 |   0.01835 |   267.1      False\n| 134   |  0.01706 |   0.01805 |   269.2      True\n| 135   |  0.01732 |   0.01816 |   271.1      False\n| 136   |  0.01702 |   0.01812 |   273.1      False\n| 137   |  0.01708 |   0.01826 |   275.1      False\n| 138   |  0.01700 |   0.01812 |   277.2      False\n| 139   |  0.01699 |   0.01808 |   279.2      False\nEpoch   140: reducing learning rate of group 0 to 1.9564e-02.\n| 140   |  0.01693 |   0.01814 |   281.3      False\n| 141   |  0.01703 |   0.01819 |   283.3      False\n| 142   |  0.01702 |   0.01803 |   285.4      True\n| 143   |  0.01694 |   0.01801 |   287.4      True\n| 144   |  0.01690 |   0.01815 |   289.5      False\n| 145   |  0.01713 |   0.01805 |   291.6      False\n| 146   |  0.01703 |   0.01813 |   293.6      False\n| 147   |  0.01701 |   0.01807 |   295.6      False\n| 148   |  0.01693 |   0.01808 |   297.5      False\nEpoch   149: reducing learning rate of group 0 to 1.7412e-02.\n| 149   |  0.01692 |   0.01802 |   299.5      False\n| 150   |  0.01683 |   0.01817 |   301.5      False\n| 151   |  0.01689 |   0.01813 |   303.5      False\n| 152   |  0.01698 |   0.01813 |   305.6      False\n| 153   |  0.01682 |   0.01810 |   307.6      False\n| 154   |  0.01686 |   0.01804 |   309.7      False\nEpoch   155: reducing learning rate of group 0 to 1.5497e-02.\n| 155   |  0.01689 |   0.01892 |   311.8      False\n| 156   |  0.01713 |   0.01819 |   313.8      False\n| 157   |  0.01687 |   0.01806 |   315.8      False\n| 158   |  0.01681 |   0.01797 |   317.9      True\n| 159   |  0.01693 |   0.01810 |   319.9      False\n| 160   |  0.01685 |   0.01810 |   322.0      False\n| 161   |  0.01672 |   0.01802 |   324.1      False\n| 162   |  0.01684 |   0.01803 |   326.2      False\n| 163   |  0.01671 |   0.01800 |   328.2      False\nEpoch   164: reducing learning rate of group 0 to 1.3792e-02.\n| 164   |  0.01670 |   0.01802 |   330.3      False\n| 165   |  0.01670 |   0.01797 |   332.3      True\n| 166   |  0.01672 |   0.01791 |   334.3      True\n| 167   |  0.01669 |   0.01796 |   336.3      False\n| 168   |  0.01670 |   0.01793 |   338.5      False\n| 169   |  0.01651 |   0.01802 |   340.6      False\n| 170   |  0.01657 |   0.01814 |   342.7      False\n| 171   |  0.01672 |   0.01802 |   344.8      False\nEpoch   172: reducing learning rate of group 0 to 1.2275e-02.\n| 172   |  0.01664 |   0.01803 |   346.9      False\n| 173   |  0.01653 |   0.01790 |   348.9      True\n| 174   |  0.01651 |   0.01808 |   350.9      False\n| 175   |  0.01656 |   0.01805 |   353.0      False\n| 176   |  0.01667 |   0.01794 |   354.9      False\n| 177   |  0.01647 |   0.01805 |   357.1      False\n| 178   |  0.01654 |   0.01798 |   359.1      False\nEpoch   179: reducing learning rate of group 0 to 1.0925e-02.\n| 179   |  0.01635 |   0.01808 |   361.1      False\n| 180   |  0.01650 |   0.01803 |   363.1      False\n| 181   |  0.01646 |   0.01791 |   365.2      False\n| 182   |  0.01655 |   0.01801 |   367.3      False\n| 183   |  0.01664 |   0.01795 |   369.3      False\n| 184   |  0.01625 |   0.01789 |   371.3      True\n| 185   |  0.01639 |   0.01792 |   373.5      False\n| 186   |  0.01644 |   0.01799 |   375.5      False\n| 187   |  0.01651 |   0.01796 |   377.5      False\n| 188   |  0.01641 |   0.01799 |   379.6      False\n| 189   |  0.01652 |   0.01794 |   381.6      False\nEpoch   190: reducing learning rate of group 0 to 9.7230e-03.\n| 190   |  0.01646 |   0.01792 |   383.7      False\n| 191   |  0.01653 |   0.01796 |   385.7      False\n| 192   |  0.01647 |   0.01788 |   387.7      True\n| 193   |  0.01628 |   0.01796 |   389.7      False\n| 194   |  0.01630 |   0.01791 |   391.8      False\n| 195   |  0.01630 |   0.01797 |   393.8      False\n| 196   |  0.01634 |   0.01798 |   395.9      False\n| 197   |  0.01631 |   0.01803 |   397.9      False\nEpoch   198: reducing learning rate of group 0 to 8.6535e-03.\n| 198   |  0.01642 |   0.01794 |   399.9      False\n| 199   |  0.01619 |   0.01801 |   402.1      False\n| 200   |  0.01614 |   0.01797 |   404.2      False\n| 201   |  0.01620 |   0.01799 |   406.2      False\n| 202   |  0.01607 |   0.01798 |   408.3      False\n| 203   |  0.01608 |   0.01792 |   410.4      False\nEpoch   204: reducing learning rate of group 0 to 7.7016e-03.\n| 204   |  0.01607 |   0.01792 |   412.4      False\n| 205   |  0.01608 |   0.01794 |   414.4      False\n| 206   |  0.01611 |   0.01797 |   416.5      False\n| 207   |  0.01623 |   0.01789 |   418.5      False\n| 208   |  0.01617 |   0.01799 |   420.6      False\n| 209   |  0.01609 |   0.01797 |   422.8      False\nEpoch   210: reducing learning rate of group 0 to 6.8544e-03.\n| 210   |  0.01617 |   0.01797 |   424.8      False\n| 211   |  0.01605 |   0.01798 |   426.9      False\n| 212   |  0.01607 |   0.01794 |   428.9      False\n| 213   |  0.01608 |   0.01806 |   430.9      False\n| 214   |  0.01597 |   0.01792 |   432.9      False\n| 215   |  0.01600 |   0.01786 |   435.0      True\n| 216   |  0.01597 |   0.01798 |   437.0      False\n| 217   |  0.01605 |   0.01798 |   439.0      False\n| 218   |  0.01596 |   0.01797 |   441.1      False\n| 219   |  0.01607 |   0.01800 |   443.2      False\n| 220   |  0.01602 |   0.01806 |   445.3      False\nEpoch   221: reducing learning rate of group 0 to 6.1004e-03.\n| 221   |  0.01601 |   0.01792 |   447.4      False\n| 222   |  0.01587 |   0.01793 |   449.4      False\n| 223   |  0.01589 |   0.01791 |   451.5      False\n| 224   |  0.01592 |   0.01793 |   453.6      False\n| 225   |  0.01591 |   0.01799 |   455.6      False\n| 226   |  0.01590 |   0.01804 |   457.5      False\nEpoch   227: reducing learning rate of group 0 to 5.4294e-03.\n| 227   |  0.01599 |   0.01807 |   459.5      False\n| 228   |  0.01580 |   0.01799 |   461.6      False\n| 229   |  0.01575 |   0.01799 |   463.6      False\n| 230   |  0.01581 |   0.01800 |   465.7      False\n| 231   |  0.01570 |   0.01792 |   467.7      False\n| 232   |  0.01579 |   0.01801 |   469.7      False\nEpoch   233: reducing learning rate of group 0 to 4.8321e-03.\n| 233   |  0.01577 |   0.01807 |   471.8      False\n| 234   |  0.01568 |   0.01817 |   474.0      False\n| 235   |  0.01588 |   0.01798 |   476.1      False\n| 236   |  0.01572 |   0.01800 |   478.2      False\n| 237   |  0.01571 |   0.01807 |   480.3      False\n| 238   |  0.01559 |   0.01810 |   482.4      False\nEpoch   239: reducing learning rate of group 0 to 4.3006e-03.\n| 239   |  0.01554 |   0.01806 |   484.4      False\n| 240   |  0.01551 |   0.01809 |   486.5      False\n| 241   |  0.01554 |   0.01808 |   488.6      False\n| 242   |  0.01558 |   0.01812 |   490.8      False\n| 243   |  0.01552 |   0.01807 |   492.9      False\n| 244   |  0.01564 |   0.01816 |   495.0      False\nEpoch   245: reducing learning rate of group 0 to 3.8275e-03.\n| 245   |  0.01555 |   0.01813 |   497.1      False\n| 246   |  0.01541 |   0.01809 |   499.1      False\n| 247   |  0.01556 |   0.01822 |   501.2      False\n| 248   |  0.01561 |   0.01817 |   503.3      False\n| 249   |  0.01549 |   0.01814 |   505.2      False\n| 250   |  0.01540 |   0.01810 |   507.3      False\nEpoch   251: reducing learning rate of group 0 to 3.4065e-03.\n| 251   |  0.01540 |   0.01812 |   509.4      False\n| 252   |  0.01555 |   0.01814 |   511.4      False\n| 253   |  0.01540 |   0.01818 |   513.5      False\n| 254   |  0.01545 |   0.01819 |   515.6      False\n| 255   |  0.01526 |   0.01815 |   517.5      False\n| 256   |  0.01521 |   0.01815 |   519.7      False\nEpoch   257: reducing learning rate of group 0 to 3.0318e-03.\n| 257   |  0.01524 |   0.01813 |   521.7      False\n| 258   |  0.01522 |   0.01808 |   523.8      False\n| 259   |  0.01530 |   0.01821 |   525.9      False\n| 260   |  0.01536 |   0.01825 |   527.9      False\n| 261   |  0.01525 |   0.01806 |   530.0      False\n| 262   |  0.01534 |   0.01815 |   532.1      False\nEpoch   263: reducing learning rate of group 0 to 2.6983e-03.\n| 263   |  0.01518 |   0.01812 |   534.2      False\n| 264   |  0.01508 |   0.01821 |   536.2      False\n| 265   |  0.01522 |   0.01820 |   538.3      False\n| 266   |  0.01519 |   0.01819 |   540.4      False\n| 267   |  0.01506 |   0.01821 |   542.4      False\n| 268   |  0.01502 |   0.01823 |   544.3      False\nEpoch   269: reducing learning rate of group 0 to 2.4015e-03.\n| 269   |  0.01498 |   0.01829 |   546.4      False\n| 270   |  0.01515 |   0.01833 |   548.3      False\n| 271   |  0.01496 |   0.01830 |   550.4      False\n| 272   |  0.01499 |   0.01833 |   552.5      False\n| 273   |  0.01493 |   0.01833 |   554.6      False\n| 274   |  0.01498 |   0.01835 |   556.6      False\nEpoch   275: reducing learning rate of group 0 to 2.1373e-03.\n| 275   |  0.01493 |   0.01838 |   558.7      False\n| 276   |  0.01498 |   0.01833 |   560.8      False\n| 277   |  0.01487 |   0.01839 |   562.9      False\n| 278   |  0.01489 |   0.01838 |   564.9      False\n| 279   |  0.01491 |   0.01842 |   567.0      False\n| 280   |  0.01469 |   0.01842 |   569.1      False\nEpoch   281: reducing learning rate of group 0 to 1.9022e-03.\n| 281   |  0.01485 |   0.01847 |   571.3      False\n| 282   |  0.01476 |   0.01854 |   573.3      False\n| 283   |  0.01470 |   0.01850 |   575.4      False\n| 284   |  0.01477 |   0.01853 |   577.4      False\n| 285   |  0.01463 |   0.01854 |   579.5      False\n| 286   |  0.01460 |   0.01853 |   581.6      False\nEpoch   287: reducing learning rate of group 0 to 1.6930e-03.\n| 287   |  0.01457 |   0.01849 |   583.7      False\n| 288   |  0.01454 |   0.01855 |   585.7      False\n| 289   |  0.01468 |   0.01862 |   587.8      False\n| 290   |  0.01465 |   0.01854 |   589.9      False\n| 291   |  0.01468 |   0.01863 |   591.9      False\n| 292   |  0.01450 |   0.01866 |   594.0      False\nEpoch   293: reducing learning rate of group 0 to 1.5067e-03.\n| 293   |  0.01446 |   0.01859 |   596.0      False\n| 294   |  0.01447 |   0.01869 |   598.1      False\n| 295   |  0.01442 |   0.01865 |   600.2      False\n| 296   |  0.01452 |   0.01868 |   602.3      False\n| 297   |  0.01444 |   0.01872 |   604.3      False\n| 298   |  0.01438 |   0.01876 |   606.4      False\nEpoch   299: reducing learning rate of group 0 to 1.3410e-03.\n| 299   |  0.01442 |   0.01875 |   608.4      False\n| 300   |  0.01425 |   0.01879 |   610.5      False\nTraining done in 610.504 seconds. Best loss : 0.01786\n---------------------------------------\nSEED [0] - FOLD 1\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 35 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  0.15335 |   0.03965 |   2.1        True\n| 2     |  0.02492 |   0.02170 |   4.2        True\n| 3     |  0.02140 |   0.02081 |   6.3        True\n| 4     |  0.02086 |   0.02196 |   8.4        False\n| 5     |  0.02038 |   0.02275 |   10.6       False\n| 6     |  0.02003 |   0.02082 |   12.6       False\n| 7     |  0.01996 |   0.02374 |   14.7       False\n| 8     |  0.01966 |   0.02000 |   16.8       True\n| 9     |  0.01972 |   0.02271 |   18.9       False\n| 10    |  0.01948 |   0.02346 |   20.9       False\n| 11    |  0.01927 |   0.01941 |   22.9       True\n| 12    |  0.01943 |   0.01990 |   25.0       False\n| 13    |  0.01924 |   0.01946 |   27.2       False\n| 14    |  0.01927 |   0.01906 |   29.3       True\n| 15    |  0.01929 |   0.01957 |   31.4       False\n| 16    |  0.01921 |   0.01890 |   33.5       True\n| 17    |  0.01889 |   0.01866 |   35.6       True\n| 18    |  0.01906 |   0.01875 |   37.6       False\n| 19    |  0.01880 |   0.01855 |   39.9       True\n| 20    |  0.01893 |   0.01880 |   42.0       False\n| 21    |  0.01883 |   0.01882 |   44.0       False\n| 22    |  0.01901 |   0.01888 |   46.0       False\n| 23    |  0.01875 |   0.01864 |   48.0       False\n| 24    |  0.01879 |   0.01914 |   50.1       False\nEpoch    25: reducing learning rate of group 0 to 8.9000e-02.\n| 25    |  0.01891 |   0.01867 |   52.2       False\n| 26    |  0.01868 |   0.01877 |   54.3       False\n| 27    |  0.01861 |   0.01844 |   56.3       True\n| 28    |  0.01848 |   0.01859 |   58.5       False\n| 29    |  0.01866 |   0.01927 |   60.5       False\n| 30    |  0.01875 |   0.01896 |   62.5       False\n| 31    |  0.01865 |   0.01880 |   64.7       False\n| 32    |  0.01854 |   0.01873 |   66.8       False\n| 33    |  0.01866 |   0.01843 |   68.9       True\n| 34    |  0.01855 |   0.01851 |   70.8       False\n| 35    |  0.01833 |   0.01829 |   72.9       True\n| 36    |  0.01831 |   0.01886 |   74.9       False\n| 37    |  0.01842 |   0.01825 |   77.0       True\n| 38    |  0.01838 |   0.01836 |   79.1       False\n| 39    |  0.01839 |   0.01860 |   81.1       False\n| 40    |  0.01856 |   0.01884 |   83.3       False\n| 41    |  0.01859 |   0.01855 |   85.5       False\n| 42    |  0.01849 |   0.01888 |   87.7       False\nEpoch    43: reducing learning rate of group 0 to 7.9210e-02.\n| 43    |  0.01837 |   0.01912 |   89.7       False\n| 44    |  0.01833 |   0.01826 |   91.7       False\n| 45    |  0.01821 |   0.01933 |   93.9       False\n| 46    |  0.01841 |   0.01840 |   95.9       False\n| 47    |  0.01827 |   0.01891 |   98.1       False\n| 48    |  0.01822 |   0.01824 |   100.3      True\n| 49    |  0.01833 |   0.01861 |   102.4      False\n| 50    |  0.01835 |   0.01820 |   104.4      True\n| 51    |  0.01830 |   0.01821 |   106.5      False\n| 52    |  0.01818 |   0.01829 |   108.7      False\n| 53    |  0.01822 |   0.01848 |   110.8      False\n| 54    |  0.01823 |   0.01867 |   112.9      False\n| 55    |  0.01853 |   0.01822 |   115.0      False\nEpoch    56: reducing learning rate of group 0 to 7.0497e-02.\n| 56    |  0.01820 |   0.01873 |   117.2      False\n| 57    |  0.01827 |   0.01820 |   119.3      True\n| 58    |  0.01812 |   0.01819 |   121.4      True\n| 59    |  0.01815 |   0.01828 |   123.5      False\n| 60    |  0.01830 |   0.01828 |   125.6      False\n| 61    |  0.01830 |   0.01873 |   127.7      False\n| 62    |  0.01818 |   0.01809 |   129.9      True\n| 63    |  0.01807 |   0.01817 |   131.9      False\n| 64    |  0.01809 |   0.01833 |   134.0      False\n| 65    |  0.01795 |   0.01810 |   136.1      False\n| 66    |  0.01805 |   0.01851 |   138.2      False\n| 67    |  0.01818 |   0.01906 |   140.2      False\nEpoch    68: reducing learning rate of group 0 to 6.2742e-02.\n| 68    |  0.01812 |   0.01828 |   142.4      False\n| 69    |  0.01809 |   0.01843 |   144.4      False\n| 70    |  0.01809 |   0.01809 |   146.5      True\n| 71    |  0.01787 |   0.01819 |   148.5      False\n| 72    |  0.01796 |   0.02018 |   150.7      False\n| 73    |  0.01809 |   0.01813 |   152.8      False\n| 74    |  0.01795 |   0.01788 |   154.8      True\n| 75    |  0.01801 |   0.01814 |   156.9      False\n| 76    |  0.01806 |   0.01842 |   158.9      False\n| 77    |  0.01804 |   0.01807 |   161.0      False\n| 78    |  0.01795 |   0.01817 |   163.3      False\n| 79    |  0.01803 |   0.01829 |   165.3      False\nEpoch    80: reducing learning rate of group 0 to 5.5841e-02.\n| 80    |  0.01820 |   0.01833 |   167.4      False\n| 81    |  0.01799 |   0.01814 |   169.5      False\n| 82    |  0.01793 |   0.01823 |   171.6      False\n| 83    |  0.01805 |   0.01795 |   173.7      False\n| 84    |  0.01792 |   0.01805 |   175.7      False\n| 85    |  0.01800 |   0.01792 |   177.7      False\nEpoch    86: reducing learning rate of group 0 to 4.9698e-02.\n| 86    |  0.01776 |   0.01820 |   179.8      False\n| 87    |  0.01784 |   0.01799 |   181.9      False\n| 88    |  0.01783 |   0.01782 |   184.0      True\n| 89    |  0.01779 |   0.01796 |   186.1      False\n| 90    |  0.01779 |   0.01805 |   188.2      False\n| 91    |  0.01784 |   0.01806 |   190.2      False\n| 92    |  0.01786 |   0.01807 |   192.3      False\n| 93    |  0.01788 |   0.01795 |   194.4      False\nEpoch    94: reducing learning rate of group 0 to 4.4231e-02.\n| 94    |  0.01770 |   0.01791 |   196.4      False\n| 95    |  0.01762 |   0.01788 |   198.6      False\n| 96    |  0.01776 |   0.01817 |   200.8      False\n| 97    |  0.01780 |   0.01786 |   203.0      False\n| 98    |  0.01773 |   0.01770 |   205.1      True\n| 99    |  0.01764 |   0.01785 |   207.1      False\n| 100   |  0.01766 |   0.01789 |   209.1      False\n| 101   |  0.01772 |   0.01787 |   211.3      False\n| 102   |  0.01770 |   0.01806 |   213.3      False\n| 103   |  0.01768 |   0.01794 |   215.5      False\nEpoch   104: reducing learning rate of group 0 to 3.9366e-02.\n| 104   |  0.01763 |   0.01778 |   217.7      False\n| 105   |  0.01772 |   0.01782 |   219.8      False\n| 106   |  0.01748 |   0.01768 |   221.8      True\n| 107   |  0.01747 |   0.01767 |   223.9      True\n| 108   |  0.01756 |   0.01824 |   226.1      False\n| 109   |  0.01755 |   0.01797 |   228.0      False\n| 110   |  0.01769 |   0.01776 |   230.2      False\n| 111   |  0.01755 |   0.01773 |   232.3      False\n| 112   |  0.01748 |   0.01791 |   234.3      False\nEpoch   113: reducing learning rate of group 0 to 3.5036e-02.\n| 113   |  0.01752 |   0.01786 |   236.4      False\n| 114   |  0.01744 |   0.01762 |   238.7      True\n| 115   |  0.01743 |   0.01805 |   240.7      False\n| 116   |  0.01760 |   0.01798 |   242.8      False\n| 117   |  0.01746 |   0.01762 |   244.9      True\n| 118   |  0.01747 |   0.01789 |   247.0      False\n| 119   |  0.01753 |   0.01776 |   249.2      False\n| 120   |  0.01740 |   0.01758 |   251.2      True\n| 121   |  0.01747 |   0.01768 |   253.2      False\n| 122   |  0.01758 |   0.01778 |   255.3      False\n| 123   |  0.01740 |   0.01787 |   257.3      False\n| 124   |  0.01737 |   0.01770 |   259.4      False\n| 125   |  0.01746 |   0.01802 |   261.5      False\nEpoch   126: reducing learning rate of group 0 to 3.1182e-02.\n| 126   |  0.01739 |   0.01761 |   263.6      False\n| 127   |  0.01723 |   0.01772 |   265.6      False\n| 128   |  0.01720 |   0.01751 |   267.6      True\n| 129   |  0.01731 |   0.01770 |   269.7      False\n| 130   |  0.01745 |   0.01756 |   271.7      False\n| 131   |  0.01730 |   0.01766 |   273.9      False\n| 132   |  0.01740 |   0.01749 |   276.0      True\n| 133   |  0.01738 |   0.01767 |   278.2      False\n| 134   |  0.01726 |   0.01782 |   280.3      False\n| 135   |  0.01724 |   0.01756 |   282.5      False\n| 136   |  0.01731 |   0.01793 |   284.6      False\n| 137   |  0.01731 |   0.01902 |   286.6      False\nEpoch   138: reducing learning rate of group 0 to 2.7752e-02.\n| 138   |  0.01785 |   0.01790 |   288.6      False\n| 139   |  0.01741 |   0.01760 |   290.7      False\n| 140   |  0.01722 |   0.01761 |   292.7      False\n| 141   |  0.01729 |   0.01791 |   294.8      False\n| 142   |  0.01736 |   0.01794 |   296.9      False\n| 143   |  0.01717 |   0.01760 |   299.0      False\nEpoch   144: reducing learning rate of group 0 to 2.4699e-02.\n| 144   |  0.01723 |   0.01758 |   301.1      False\n| 145   |  0.01706 |   0.01754 |   303.2      False\n| 146   |  0.01707 |   0.01756 |   305.2      False\n| 147   |  0.01709 |   0.01749 |   307.3      False\n| 148   |  0.01713 |   0.01748 |   309.4      True\n| 149   |  0.01711 |   0.01752 |   311.5      False\n| 150   |  0.01709 |   0.01755 |   313.5      False\n| 151   |  0.01721 |   0.01763 |   315.7      False\n| 152   |  0.01716 |   0.01775 |   317.8      False\n| 153   |  0.01701 |   0.01776 |   319.9      False\nEpoch   154: reducing learning rate of group 0 to 2.1982e-02.\n| 154   |  0.01712 |   0.01760 |   321.9      False\n| 155   |  0.01723 |   0.01748 |   324.0      False\n| 156   |  0.01708 |   0.01748 |   326.1      False\n| 157   |  0.01711 |   0.01747 |   328.1      True\n| 158   |  0.01690 |   0.01747 |   330.2      True\n| 159   |  0.01698 |   0.01748 |   332.3      False\n| 160   |  0.01689 |   0.01761 |   334.3      False\n| 161   |  0.01707 |   0.01753 |   336.4      False\n| 162   |  0.01713 |   0.01748 |   338.5      False\n| 163   |  0.01683 |   0.01743 |   340.5      True\n| 164   |  0.01695 |   0.01739 |   342.6      True\n| 165   |  0.01688 |   0.01746 |   344.7      False\n| 166   |  0.01705 |   0.01773 |   346.8      False\n| 167   |  0.01698 |   0.01755 |   348.9      False\n| 168   |  0.01688 |   0.01757 |   351.0      False\n| 169   |  0.01693 |   0.01754 |   353.1      False\nEpoch   170: reducing learning rate of group 0 to 1.9564e-02.\n| 170   |  0.01700 |   0.01768 |   355.3      False\n| 171   |  0.01693 |   0.01754 |   357.4      False\n| 172   |  0.01685 |   0.01753 |   359.5      False\n| 173   |  0.01684 |   0.01756 |   361.5      False\n| 174   |  0.01684 |   0.01751 |   363.6      False\n| 175   |  0.01701 |   0.01748 |   365.7      False\nEpoch   176: reducing learning rate of group 0 to 1.7412e-02.\n| 176   |  0.01691 |   0.01760 |   367.8      False\n| 177   |  0.01675 |   0.01770 |   369.8      False\n| 178   |  0.01675 |   0.01741 |   371.9      False\n| 179   |  0.01669 |   0.01741 |   374.0      False\n| 180   |  0.01686 |   0.01739 |   376.0      False\n| 181   |  0.01681 |   0.01850 |   378.1      False\nEpoch   182: reducing learning rate of group 0 to 1.5497e-02.\n| 182   |  0.01716 |   0.01747 |   380.2      False\n| 183   |  0.01680 |   0.01733 |   382.2      True\n| 184   |  0.01673 |   0.01742 |   384.3      False\n| 185   |  0.01674 |   0.01750 |   386.4      False\n| 186   |  0.01666 |   0.01730 |   388.5      True\n| 187   |  0.01664 |   0.01777 |   390.7      False\n| 188   |  0.01672 |   0.01752 |   392.7      False\n| 189   |  0.01686 |   0.01762 |   394.9      False\n| 190   |  0.01705 |   0.01756 |   396.9      False\n| 191   |  0.01688 |   0.01747 |   399.1      False\nEpoch   192: reducing learning rate of group 0 to 1.3792e-02.\n| 192   |  0.01670 |   0.01751 |   401.2      False\n| 193   |  0.01665 |   0.01741 |   403.3      False\n| 194   |  0.01665 |   0.01743 |   405.4      False\n| 195   |  0.01666 |   0.01740 |   407.5      False\n| 196   |  0.01661 |   0.01740 |   409.6      False\n| 197   |  0.01667 |   0.01730 |   411.6      False\nEpoch   198: reducing learning rate of group 0 to 1.2275e-02.\n| 198   |  0.01657 |   0.01735 |   413.8      False\n| 199   |  0.01649 |   0.01740 |   415.8      False\n| 200   |  0.01656 |   0.01744 |   417.9      False\n| 201   |  0.01668 |   0.01749 |   420.0      False\n| 202   |  0.01651 |   0.01729 |   422.1      True\n| 203   |  0.01647 |   0.01745 |   424.3      False\n| 204   |  0.01648 |   0.01736 |   426.3      False\n| 205   |  0.01643 |   0.01730 |   428.5      False\n| 206   |  0.01645 |   0.01739 |   430.5      False\n| 207   |  0.01659 |   0.01739 |   432.6      False\nEpoch   208: reducing learning rate of group 0 to 1.0925e-02.\n| 208   |  0.01641 |   0.01745 |   434.7      False\n| 209   |  0.01636 |   0.01743 |   436.9      False\n| 210   |  0.01647 |   0.01741 |   439.0      False\n| 211   |  0.01648 |   0.01742 |   441.1      False\n| 212   |  0.01642 |   0.01757 |   443.1      False\n| 213   |  0.01644 |   0.01746 |   445.2      False\nEpoch   214: reducing learning rate of group 0 to 9.7230e-03.\n| 214   |  0.01633 |   0.01737 |   447.3      False\n| 215   |  0.01634 |   0.01730 |   449.3      False\n| 216   |  0.01634 |   0.01744 |   451.4      False\n| 217   |  0.01626 |   0.01742 |   453.6      False\n| 218   |  0.01630 |   0.01738 |   455.7      False\n| 219   |  0.01630 |   0.01800 |   457.8      False\nEpoch   220: reducing learning rate of group 0 to 8.6535e-03.\n| 220   |  0.01633 |   0.01740 |   459.9      False\n| 221   |  0.01627 |   0.01733 |   462.0      False\n| 222   |  0.01602 |   0.01741 |   464.1      False\n| 223   |  0.01609 |   0.01738 |   466.2      False\n| 224   |  0.01604 |   0.01748 |   468.3      False\n| 225   |  0.01614 |   0.01742 |   470.5      False\nEpoch   226: reducing learning rate of group 0 to 7.7016e-03.\n| 226   |  0.01604 |   0.01746 |   472.6      False\n| 227   |  0.01610 |   0.01738 |   474.7      False\n| 228   |  0.01609 |   0.01751 |   476.7      False\n| 229   |  0.01609 |   0.01753 |   478.8      False\n| 230   |  0.01607 |   0.01744 |   480.9      False\n| 231   |  0.01602 |   0.01741 |   483.0      False\nEpoch   232: reducing learning rate of group 0 to 6.8544e-03.\n| 232   |  0.01595 |   0.01775 |   485.0      False\n| 233   |  0.01603 |   0.01738 |   487.1      False\n| 234   |  0.01595 |   0.01742 |   489.3      False\n| 235   |  0.01590 |   0.01745 |   491.4      False\n| 236   |  0.01589 |   0.01739 |   493.5      False\n| 237   |  0.01593 |   0.01744 |   495.5      False\nEpoch   238: reducing learning rate of group 0 to 6.1004e-03.\n| 238   |  0.01595 |   0.01747 |   497.6      False\n| 239   |  0.01583 |   0.01746 |   499.7      False\n| 240   |  0.01580 |   0.01745 |   501.8      False\n| 241   |  0.01574 |   0.01840 |   503.9      False\n| 242   |  0.01612 |   0.01757 |   506.0      False\n| 243   |  0.01588 |   0.01737 |   508.1      False\nEpoch   244: reducing learning rate of group 0 to 5.4294e-03.\n| 244   |  0.01582 |   0.01747 |   510.2      False\n| 245   |  0.01568 |   0.01752 |   512.4      False\n| 246   |  0.01574 |   0.01748 |   514.5      False\n| 247   |  0.01574 |   0.01754 |   516.6      False\n| 248   |  0.01579 |   0.01750 |   518.7      False\n| 249   |  0.01575 |   0.01744 |   520.7      False\nEpoch   250: reducing learning rate of group 0 to 4.8321e-03.\n| 250   |  0.01555 |   0.01746 |   522.8      False\n| 251   |  0.01561 |   0.01755 |   524.9      False\n| 252   |  0.01560 |   0.01742 |   527.1      False\n| 253   |  0.01560 |   0.01751 |   529.2      False\n| 254   |  0.01557 |   0.01763 |   531.3      False\n| 255   |  0.01557 |   0.01754 |   533.3      False\nEpoch   256: reducing learning rate of group 0 to 4.3006e-03.\n| 256   |  0.01547 |   0.01755 |   535.4      False\n| 257   |  0.01559 |   0.01755 |   537.5      False\n| 258   |  0.01553 |   0.01755 |   539.5      False\n| 259   |  0.01545 |   0.01748 |   541.6      False\n| 260   |  0.01542 |   0.01753 |   543.7      False\n| 261   |  0.01539 |   0.01759 |   545.9      False\nEpoch   262: reducing learning rate of group 0 to 3.8275e-03.\n| 262   |  0.01543 |   0.01759 |   548.0      False\n| 263   |  0.01538 |   0.01770 |   550.0      False\n| 264   |  0.01539 |   0.01781 |   552.1      False\n| 265   |  0.01544 |   0.01765 |   554.2      False\n| 266   |  0.01539 |   0.01760 |   556.2      False\n| 267   |  0.01535 |   0.01760 |   558.4      False\nEpoch   268: reducing learning rate of group 0 to 3.4065e-03.\n| 268   |  0.01540 |   0.01770 |   560.5      False\n| 269   |  0.01531 |   0.01772 |   562.6      False\n| 270   |  0.01524 |   0.01769 |   564.6      False\n| 271   |  0.01515 |   0.01765 |   566.8      False\n| 272   |  0.01515 |   0.01766 |   569.0      False\n| 273   |  0.01509 |   0.01771 |   571.1      False\nEpoch   274: reducing learning rate of group 0 to 3.0318e-03.\n| 274   |  0.01521 |   0.01776 |   573.1      False\n| 275   |  0.01506 |   0.01777 |   575.2      False\n| 276   |  0.01508 |   0.01769 |   577.3      False\n| 277   |  0.01500 |   0.01773 |   579.5      False\n| 278   |  0.01516 |   0.01781 |   581.6      False\n| 279   |  0.01516 |   0.01780 |   583.6      False\nEpoch   280: reducing learning rate of group 0 to 2.6983e-03.\n| 280   |  0.01522 |   0.01776 |   585.7      False\n| 281   |  0.01502 |   0.01778 |   587.8      False\n| 282   |  0.01487 |   0.01779 |   589.8      False\n| 283   |  0.01496 |   0.01788 |   591.9      False\n| 284   |  0.01490 |   0.01804 |   594.1      False\n| 285   |  0.01491 |   0.01785 |   596.1      False\nEpoch   286: reducing learning rate of group 0 to 2.4015e-03.\n| 286   |  0.01486 |   0.01796 |   598.2      False\n| 287   |  0.01493 |   0.01787 |   600.3      False\n| 288   |  0.01477 |   0.01792 |   602.3      False\n| 289   |  0.01494 |   0.01790 |   604.4      False\n| 290   |  0.01476 |   0.01791 |   606.5      False\n| 291   |  0.01479 |   0.01794 |   608.6      False\nEpoch   292: reducing learning rate of group 0 to 2.1373e-03.\n| 292   |  0.01480 |   0.01797 |   610.7      False\n| 293   |  0.01492 |   0.01796 |   612.8      False\n| 294   |  0.01472 |   0.01802 |   615.0      False\n| 295   |  0.01465 |   0.01803 |   617.1      False\n| 296   |  0.01471 |   0.01802 |   619.2      False\n| 297   |  0.01465 |   0.01812 |   621.3      False\nEpoch   298: reducing learning rate of group 0 to 1.9022e-03.\n| 298   |  0.01462 |   0.01804 |   623.4      False\n| 299   |  0.01459 |   0.01813 |   625.5      False\n| 300   |  0.01460 |   0.01812 |   627.5      False\nTraining done in 627.500 seconds. Best loss : 0.01729\n---------------------------------------\nSEED [0] - FOLD 2\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 35 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  0.15246 |   0.04025 |   2.1        True\n| 2     |  0.02409 |   0.02210 |   4.2        True\n| 3     |  0.02151 |   0.02123 |   6.3        True\n| 4     |  0.02108 |   0.02079 |   8.4        True\n| 5     |  0.02054 |   0.02174 |   10.5       False\n| 6     |  0.02009 |   0.02089 |   12.5       False\n| 7     |  0.01968 |   0.02217 |   14.6       False\n| 8     |  0.01962 |   0.01986 |   16.7       True\n| 9     |  0.01934 |   0.02009 |   18.9       False\n| 10    |  0.01937 |   0.01942 |   20.9       True\n| 11    |  0.01924 |   0.02016 |   23.0       False\n| 12    |  0.01915 |   0.02220 |   25.0       False\n| 13    |  0.01924 |   0.02055 |   27.1       False\n| 14    |  0.01947 |   0.01928 |   29.2       True\n| 15    |  0.01896 |   0.01943 |   31.2       False\n| 16    |  0.01887 |   0.01928 |   33.3       False\n| 17    |  0.01886 |   0.02024 |   35.5       False\n| 18    |  0.01901 |   0.01889 |   37.6       True\n| 19    |  0.01877 |   0.01929 |   39.6       False\n| 20    |  0.01913 |   0.01947 |   41.8       False\n| 21    |  0.01871 |   0.01967 |   43.8       False\n| 22    |  0.01876 |   0.01935 |   45.8       False\n| 23    |  0.01865 |   0.01876 |   47.9       True\n| 24    |  0.01863 |   0.01928 |   49.9       False\n| 25    |  0.01884 |   0.01901 |   52.0       False\n| 26    |  0.01861 |   0.01875 |   54.1       True\n| 27    |  0.01860 |   0.01877 |   56.2       False\n| 28    |  0.01866 |   0.01924 |   58.3       False\n| 29    |  0.01862 |   0.01893 |   60.3       False\n| 30    |  0.01850 |   0.01995 |   62.4       False\n| 31    |  0.01874 |   0.01909 |   64.5       False\nEpoch    32: reducing learning rate of group 0 to 8.9000e-02.\n| 32    |  0.01855 |   0.01896 |   66.6       False\n| 33    |  0.01847 |   0.01897 |   68.7       False\n| 34    |  0.01847 |   0.01869 |   71.0       True\n| 35    |  0.01828 |   0.01873 |   73.2       False\n| 36    |  0.01852 |   0.01891 |   75.3       False\n| 37    |  0.01848 |   0.01876 |   77.5       False\n| 38    |  0.01870 |   0.01871 |   79.6       False\n| 39    |  0.01848 |   0.02019 |   81.8       False\nEpoch    40: reducing learning rate of group 0 to 7.9210e-02.\n| 40    |  0.01841 |   0.01933 |   83.9       False\n| 41    |  0.01838 |   0.01859 |   86.0       True\n| 42    |  0.01841 |   0.01875 |   88.1       False\n| 43    |  0.01831 |   0.01866 |   90.2       False\n| 44    |  0.01826 |   0.01891 |   92.4       False\n| 45    |  0.01822 |   0.01859 |   94.5       True\n| 46    |  0.01819 |   0.01924 |   96.7       False\n| 47    |  0.01883 |   0.01982 |   98.7       False\n| 48    |  0.01837 |   0.01928 |   100.8      False\n| 49    |  0.01861 |   0.01910 |   102.9      False\n| 50    |  0.01834 |   0.01874 |   105.1      False\nEpoch    51: reducing learning rate of group 0 to 7.0497e-02.\n| 51    |  0.01821 |   0.01887 |   107.1      False\n| 52    |  0.01818 |   0.01866 |   109.2      False\n| 53    |  0.01800 |   0.01857 |   111.2      True\n| 54    |  0.01810 |   0.01860 |   113.3      False\n| 55    |  0.01794 |   0.01844 |   115.5      True\n| 56    |  0.01811 |   0.01861 |   117.6      False\n| 57    |  0.01844 |   0.01859 |   119.7      False\n| 58    |  0.01813 |   0.01851 |   121.7      False\n| 59    |  0.01804 |   0.01862 |   123.9      False\n| 60    |  0.01830 |   0.01850 |   126.0      False\n| 61    |  0.01801 |   0.01831 |   128.2      True\n| 62    |  0.01805 |   0.01860 |   130.4      False\n| 63    |  0.01822 |   0.01917 |   132.4      False\n| 64    |  0.01807 |   0.01844 |   134.5      False\n| 65    |  0.01796 |   0.01853 |   136.6      False\n| 66    |  0.01803 |   0.01881 |   138.6      False\nEpoch    67: reducing learning rate of group 0 to 6.2742e-02.\n| 67    |  0.01815 |   0.01861 |   140.8      False\n| 68    |  0.01793 |   0.01850 |   142.9      False\n| 69    |  0.01791 |   0.01884 |   144.9      False\n| 70    |  0.01797 |   0.01851 |   147.1      False\n| 71    |  0.01792 |   0.01860 |   149.2      False\n| 72    |  0.01795 |   0.01831 |   151.2      True\n| 73    |  0.01795 |   0.01856 |   153.3      False\n| 74    |  0.01784 |   0.01829 |   155.3      True\n| 75    |  0.01796 |   0.02004 |   157.5      False\n| 76    |  0.01800 |   0.01846 |   159.6      False\n| 77    |  0.01806 |   0.01875 |   161.8      False\n| 78    |  0.01807 |   0.01854 |   163.8      False\n| 79    |  0.01781 |   0.01843 |   165.9      False\nEpoch    80: reducing learning rate of group 0 to 5.5841e-02.\n| 80    |  0.01784 |   0.01869 |   168.0      False\n| 81    |  0.01780 |   0.01839 |   170.1      False\n| 82    |  0.01788 |   0.01828 |   172.2      True\n| 83    |  0.01779 |   0.01833 |   174.4      False\n| 84    |  0.01777 |   0.01958 |   176.6      False\n| 85    |  0.01794 |   0.01862 |   178.7      False\n| 86    |  0.01785 |   0.01846 |   180.8      False\n| 87    |  0.01788 |   0.01837 |   182.9      False\nEpoch    88: reducing learning rate of group 0 to 4.9698e-02.\n| 88    |  0.01787 |   0.01848 |   184.9      False\n| 89    |  0.01793 |   0.01826 |   187.0      True\n| 90    |  0.01808 |   0.01857 |   189.1      False\n| 91    |  0.01768 |   0.01830 |   191.2      False\n| 92    |  0.01764 |   0.01858 |   193.3      False\n| 93    |  0.01781 |   0.01812 |   195.5      True\n| 94    |  0.01767 |   0.01900 |   197.5      False\n| 95    |  0.01772 |   0.01849 |   199.7      False\n| 96    |  0.01767 |   0.01821 |   201.8      False\n| 97    |  0.01776 |   0.01838 |   203.8      False\n| 98    |  0.01777 |   0.01839 |   205.9      False\nEpoch    99: reducing learning rate of group 0 to 4.4231e-02.\n| 99    |  0.01772 |   0.01823 |   208.1      False\n| 100   |  0.01753 |   0.01828 |   210.2      False\n| 101   |  0.01758 |   0.01815 |   212.2      False\n| 102   |  0.01770 |   0.01817 |   214.3      False\n| 103   |  0.01785 |   0.01824 |   216.4      False\n| 104   |  0.01770 |   0.01829 |   218.5      False\nEpoch   105: reducing learning rate of group 0 to 3.9366e-02.\n| 105   |  0.01749 |   0.01823 |   220.5      False\n| 106   |  0.01754 |   0.01821 |   222.7      False\n| 107   |  0.01756 |   0.01808 |   224.8      True\n| 108   |  0.01746 |   0.01809 |   227.0      False\n| 109   |  0.01745 |   0.01809 |   229.1      False\n| 110   |  0.01766 |   0.01820 |   231.2      False\n| 111   |  0.01754 |   0.01808 |   233.3      True\n| 112   |  0.01751 |   0.01808 |   235.4      False\n| 113   |  0.01754 |   0.01799 |   237.6      True\n| 114   |  0.01756 |   0.01808 |   239.7      False\n| 115   |  0.01740 |   0.01813 |   241.7      False\n| 116   |  0.01750 |   0.01803 |   243.8      False\n| 117   |  0.01746 |   0.01840 |   245.8      False\n| 118   |  0.01756 |   0.01813 |   247.8      False\nEpoch   119: reducing learning rate of group 0 to 3.5036e-02.\n| 119   |  0.01745 |   0.01803 |   250.0      False\n| 120   |  0.01746 |   0.01812 |   252.2      False\n| 121   |  0.01735 |   0.01797 |   254.2      True\n| 122   |  0.01736 |   0.01830 |   256.4      False\n| 123   |  0.01740 |   0.01811 |   258.6      False\n| 124   |  0.01736 |   0.01815 |   260.7      False\n| 125   |  0.01737 |   0.01802 |   262.9      False\n| 126   |  0.01745 |   0.01808 |   265.0      False\nEpoch   127: reducing learning rate of group 0 to 3.1182e-02.\n| 127   |  0.01732 |   0.01806 |   267.1      False\n| 128   |  0.01716 |   0.01802 |   269.2      False\n| 129   |  0.01721 |   0.01803 |   271.3      False\n| 130   |  0.01728 |   0.01794 |   273.4      True\n| 131   |  0.01724 |   0.01799 |   275.5      False\n| 132   |  0.01713 |   0.01804 |   277.7      False\n| 133   |  0.01718 |   0.01788 |   279.7      True\n| 134   |  0.01726 |   0.01816 |   281.8      False\n| 135   |  0.01723 |   0.01796 |   284.0      False\n| 136   |  0.01731 |   0.01804 |   286.1      False\n| 137   |  0.01725 |   0.01796 |   288.2      False\n| 138   |  0.01733 |   0.01828 |   290.3      False\nEpoch   139: reducing learning rate of group 0 to 2.7752e-02.\n| 139   |  0.01732 |   0.01800 |   292.4      False\n| 140   |  0.01714 |   0.01782 |   294.6      True\n| 141   |  0.01701 |   0.01803 |   296.6      False\n| 142   |  0.01712 |   0.01793 |   298.8      False\n| 143   |  0.01718 |   0.01804 |   301.0      False\n| 144   |  0.01720 |   0.01805 |   303.2      False\n| 145   |  0.01713 |   0.01791 |   305.3      False\nEpoch   146: reducing learning rate of group 0 to 2.4699e-02.\n| 146   |  0.01714 |   0.01805 |   307.4      False\n| 147   |  0.01708 |   0.01787 |   309.5      False\n| 148   |  0.01707 |   0.01793 |   311.7      False\n| 149   |  0.01697 |   0.01782 |   313.8      True\n| 150   |  0.01704 |   0.01784 |   315.9      False\n| 151   |  0.01694 |   0.01788 |   318.0      False\n| 152   |  0.01706 |   0.01795 |   320.1      False\n| 153   |  0.01705 |   0.01802 |   322.2      False\n| 154   |  0.01707 |   0.01795 |   324.4      False\nEpoch   155: reducing learning rate of group 0 to 2.1982e-02.\n| 155   |  0.01701 |   0.01787 |   326.4      False\n| 156   |  0.01692 |   0.01802 |   328.5      False\n| 157   |  0.01702 |   0.01784 |   330.5      False\n| 158   |  0.01693 |   0.01777 |   332.7      True\n| 159   |  0.01694 |   0.01795 |   334.8      False\n| 160   |  0.01686 |   0.01792 |   337.0      False\n| 161   |  0.01698 |   0.01857 |   339.1      False\n| 162   |  0.01692 |   0.01784 |   341.2      False\n| 163   |  0.01695 |   0.01781 |   343.3      False\nEpoch   164: reducing learning rate of group 0 to 1.9564e-02.\n| 164   |  0.01689 |   0.01840 |   345.4      False\n| 165   |  0.01690 |   0.01784 |   347.6      False\n| 166   |  0.01677 |   0.01785 |   349.7      False\n| 167   |  0.01679 |   0.01774 |   351.7      True\n| 168   |  0.01684 |   0.01786 |   353.8      False\n| 169   |  0.01690 |   0.01778 |   355.9      False\n| 170   |  0.01674 |   0.01790 |   357.9      False\n| 171   |  0.01685 |   0.01832 |   360.1      False\n| 172   |  0.01697 |   0.01776 |   362.2      False\nEpoch   173: reducing learning rate of group 0 to 1.7412e-02.\n| 173   |  0.01682 |   0.01782 |   364.3      False\n| 174   |  0.01663 |   0.01768 |   366.4      True\n| 175   |  0.01670 |   0.01788 |   368.6      False\n| 176   |  0.01677 |   0.01782 |   370.8      False\n| 177   |  0.01665 |   0.01770 |   372.9      False\n| 178   |  0.01676 |   0.01779 |   375.0      False\n| 179   |  0.01693 |   0.01776 |   377.1      False\nEpoch   180: reducing learning rate of group 0 to 1.5497e-02.\n| 180   |  0.01688 |   0.01789 |   379.3      False\n| 181   |  0.01668 |   0.01777 |   381.4      False\n| 182   |  0.01667 |   0.01780 |   383.5      False\n| 183   |  0.01658 |   0.01775 |   385.7      False\n| 184   |  0.01665 |   0.01775 |   387.8      False\n| 185   |  0.01663 |   0.01777 |   389.9      False\nEpoch   186: reducing learning rate of group 0 to 1.3792e-02.\n| 186   |  0.01672 |   0.01776 |   392.1      False\n| 187   |  0.01662 |   0.01761 |   394.2      True\n| 188   |  0.01644 |   0.01768 |   396.3      False\n| 189   |  0.01655 |   0.01780 |   398.4      False\n| 190   |  0.01649 |   0.01771 |   400.5      False\n| 191   |  0.01651 |   0.01773 |   402.6      False\n| 192   |  0.01652 |   0.01771 |   404.8      False\nEpoch   193: reducing learning rate of group 0 to 1.2275e-02.\n| 193   |  0.01657 |   0.01775 |   406.9      False\n| 194   |  0.01646 |   0.01770 |   409.1      False\n| 195   |  0.01643 |   0.01776 |   411.3      False\n| 196   |  0.01642 |   0.01768 |   413.4      False\n| 197   |  0.01643 |   0.01775 |   415.5      False\n| 198   |  0.01638 |   0.01769 |   417.6      False\nEpoch   199: reducing learning rate of group 0 to 1.0925e-02.\n| 199   |  0.01638 |   0.01763 |   419.7      False\n| 200   |  0.01638 |   0.01766 |   421.8      False\n| 201   |  0.01636 |   0.01771 |   423.8      False\n| 202   |  0.01631 |   0.01769 |   426.0      False\n| 203   |  0.01633 |   0.01782 |   428.1      False\n| 204   |  0.01639 |   0.01769 |   430.2      False\nEpoch   205: reducing learning rate of group 0 to 9.7230e-03.\n| 205   |  0.01632 |   0.01774 |   432.4      False\n| 206   |  0.01625 |   0.01769 |   434.5      False\n| 207   |  0.01625 |   0.01762 |   436.7      False\n| 208   |  0.01632 |   0.01768 |   438.8      False\n| 209   |  0.01616 |   0.01768 |   440.9      False\n| 210   |  0.01623 |   0.01783 |   443.1      False\nEpoch   211: reducing learning rate of group 0 to 8.6535e-03.\n| 211   |  0.01626 |   0.01765 |   445.1      False\n| 212   |  0.01621 |   0.01771 |   447.2      False\n| 213   |  0.01620 |   0.01775 |   449.4      False\n| 214   |  0.01618 |   0.01772 |   451.5      False\n| 215   |  0.01616 |   0.01771 |   453.5      False\n| 216   |  0.01613 |   0.01771 |   455.6      False\nEpoch   217: reducing learning rate of group 0 to 7.7016e-03.\n| 217   |  0.01617 |   0.01773 |   457.7      False\n| 218   |  0.01605 |   0.01776 |   459.9      False\n| 219   |  0.01611 |   0.01779 |   462.0      False\n| 220   |  0.01616 |   0.01776 |   464.0      False\n| 221   |  0.01605 |   0.01775 |   466.2      False\n| 222   |  0.01601 |   0.01777 |   468.2      False\nEpoch   223: reducing learning rate of group 0 to 6.8544e-03.\n| 223   |  0.01601 |   0.01775 |   470.4      False\n| 224   |  0.01597 |   0.01771 |   472.6      False\n| 225   |  0.01592 |   0.01776 |   474.7      False\n| 226   |  0.01593 |   0.01782 |   476.8      False\n| 227   |  0.01589 |   0.01779 |   478.9      False\n| 228   |  0.01587 |   0.01782 |   481.1      False\nEpoch   229: reducing learning rate of group 0 to 6.1004e-03.\n| 229   |  0.01615 |   0.01784 |   483.2      False\n| 230   |  0.01602 |   0.01785 |   485.3      False\n| 231   |  0.01590 |   0.01787 |   487.3      False\n| 232   |  0.01608 |   0.01778 |   489.4      False\n| 233   |  0.01591 |   0.01775 |   491.4      False\n| 234   |  0.01583 |   0.01776 |   493.5      False\nEpoch   235: reducing learning rate of group 0 to 5.4294e-03.\n| 235   |  0.01587 |   0.01783 |   495.6      False\n| 236   |  0.01584 |   0.01784 |   497.8      False\n| 237   |  0.01575 |   0.01781 |   499.9      False\n| 238   |  0.01568 |   0.01787 |   502.0      False\n| 239   |  0.01575 |   0.01786 |   504.2      False\n| 240   |  0.01563 |   0.01784 |   506.3      False\nEpoch   241: reducing learning rate of group 0 to 4.8321e-03.\n| 241   |  0.01568 |   0.01792 |   508.4      False\n| 242   |  0.01561 |   0.01787 |   510.5      False\n| 243   |  0.01565 |   0.01796 |   512.5      False\n| 244   |  0.01561 |   0.01787 |   514.7      False\n| 245   |  0.01567 |   0.01790 |   516.8      False\n| 246   |  0.01550 |   0.01788 |   518.9      False\nEpoch   247: reducing learning rate of group 0 to 4.3006e-03.\n| 247   |  0.01559 |   0.01798 |   521.1      False\n| 248   |  0.01558 |   0.01794 |   523.3      False\n| 249   |  0.01544 |   0.01797 |   525.4      False\n| 250   |  0.01548 |   0.01792 |   527.5      False\n| 251   |  0.01557 |   0.01800 |   529.6      False\n| 252   |  0.01555 |   0.01798 |   531.7      False\nEpoch   253: reducing learning rate of group 0 to 3.8275e-03.\n| 253   |  0.01553 |   0.01799 |   533.9      False\n| 254   |  0.01548 |   0.01809 |   536.0      False\n| 255   |  0.01542 |   0.01795 |   538.2      False\n| 256   |  0.01546 |   0.01792 |   540.3      False\n| 257   |  0.01543 |   0.01793 |   542.4      False\n| 258   |  0.01544 |   0.01796 |   544.6      False\nEpoch   259: reducing learning rate of group 0 to 3.4065e-03.\n| 259   |  0.01538 |   0.01803 |   546.7      False\n| 260   |  0.01541 |   0.01801 |   548.8      False\n| 261   |  0.01533 |   0.01803 |   551.0      False\n| 262   |  0.01530 |   0.01804 |   553.1      False\n| 263   |  0.01520 |   0.01805 |   555.2      False\n| 264   |  0.01524 |   0.01810 |   557.4      False\nEpoch   265: reducing learning rate of group 0 to 3.0318e-03.\n| 265   |  0.01525 |   0.01814 |   559.5      False\n| 266   |  0.01531 |   0.01812 |   561.6      False\n| 267   |  0.01516 |   0.01810 |   563.6      False\n| 268   |  0.01529 |   0.01813 |   565.7      False\n| 269   |  0.01526 |   0.01814 |   567.8      False\n| 270   |  0.01530 |   0.01811 |   569.9      False\nEpoch   271: reducing learning rate of group 0 to 2.6983e-03.\n| 271   |  0.01531 |   0.01816 |   572.0      False\n| 272   |  0.01516 |   0.01813 |   574.0      False\n| 273   |  0.01502 |   0.01812 |   576.2      False\n| 274   |  0.01508 |   0.01821 |   578.3      False\n| 275   |  0.01501 |   0.01817 |   580.5      False\n| 276   |  0.01505 |   0.01812 |   582.7      False\nEpoch   277: reducing learning rate of group 0 to 2.4015e-03.\n| 277   |  0.01506 |   0.01820 |   584.8      False\n| 278   |  0.01506 |   0.01823 |   586.9      False\n| 279   |  0.01500 |   0.01828 |   588.9      False\n| 280   |  0.01496 |   0.01826 |   591.0      False\n| 281   |  0.01484 |   0.01823 |   593.2      False\n| 282   |  0.01494 |   0.01826 |   595.3      False\nEpoch   283: reducing learning rate of group 0 to 2.1373e-03.\n| 283   |  0.01496 |   0.01838 |   597.4      False\n| 284   |  0.01491 |   0.01824 |   599.5      False\n| 285   |  0.01496 |   0.01834 |   601.6      False\n| 286   |  0.01487 |   0.01839 |   603.8      False\n| 287   |  0.01484 |   0.01828 |   605.9      False\n| 288   |  0.01481 |   0.01841 |   607.9      False\nEpoch   289: reducing learning rate of group 0 to 1.9022e-03.\n| 289   |  0.01487 |   0.01831 |   610.2      False\n| 290   |  0.01482 |   0.01837 |   612.3      False\n| 291   |  0.01478 |   0.01839 |   614.4      False\n| 292   |  0.01479 |   0.01834 |   616.5      False\n| 293   |  0.01486 |   0.01831 |   618.6      False\n| 294   |  0.01467 |   0.01850 |   620.7      False\nEpoch   295: reducing learning rate of group 0 to 1.6930e-03.\n| 295   |  0.01475 |   0.01846 |   622.8      False\n| 296   |  0.01460 |   0.01839 |   624.9      False\n| 297   |  0.01468 |   0.01843 |   627.1      False\n| 298   |  0.01475 |   0.01851 |   629.2      False\n| 299   |  0.01472 |   0.01847 |   631.3      False\n| 300   |  0.01466 |   0.01843 |   633.5      False\nTraining done in 633.477 seconds. Best loss : 0.01761\n---------------------------------------\nSEED [0] - FOLD 3\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 35 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  0.15360 |   0.04174 |   2.1        True\n| 2     |  0.02627 |   0.02222 |   4.2        True\n| 3     |  0.02179 |   0.02285 |   6.4        False\n| 4     |  0.02125 |   0.02165 |   8.4        True\n| 5     |  0.02089 |   0.02225 |   10.6       False\n| 6     |  0.02119 |   0.02294 |   12.7       False\n| 7     |  0.02075 |   0.02137 |   14.8       True\n| 8     |  0.02039 |   0.02173 |   16.9       False\n| 9     |  0.02000 |   0.02137 |   19.0       False\n| 10    |  0.01991 |   0.02128 |   21.0       True\n| 11    |  0.01959 |   0.01951 |   23.1       True\n| 12    |  0.01971 |   0.02067 |   25.2       False\n| 13    |  0.01955 |   0.01945 |   27.3       True\n| 14    |  0.01954 |   0.02006 |   29.4       False\n| 15    |  0.01928 |   0.01920 |   31.5       True\n| 16    |  0.01917 |   0.01972 |   33.6       False\n| 17    |  0.01921 |   0.01964 |   35.9       False\n| 18    |  0.01904 |   0.02213 |   38.1       False\n| 19    |  0.01905 |   0.01900 |   40.2       True\n| 20    |  0.01901 |   0.01904 |   42.2       False\n| 21    |  0.01889 |   0.01938 |   44.4       False\n| 22    |  0.01891 |   0.01897 |   46.5       True\n| 23    |  0.01887 |   0.01906 |   48.6       False\n| 24    |  0.01872 |   0.01872 |   50.8       True\n| 25    |  0.01856 |   0.01877 |   53.0       False\n| 26    |  0.01864 |   0.01857 |   55.0       True\n| 27    |  0.01853 |   0.01881 |   57.1       False\n| 28    |  0.01868 |   0.02040 |   59.2       False\n| 29    |  0.01872 |   0.01972 |   61.2       False\n| 30    |  0.01863 |   0.01902 |   63.3       False\n| 31    |  0.01860 |   0.01883 |   65.4       False\nEpoch    32: reducing learning rate of group 0 to 8.9000e-02.\n| 32    |  0.01881 |   0.01897 |   67.5       False\n| 33    |  0.01835 |   0.01872 |   69.6       False\n| 34    |  0.01840 |   0.01849 |   71.7       True\n| 35    |  0.01851 |   0.02014 |   74.0       False\n| 36    |  0.01859 |   0.01881 |   76.1       False\n| 37    |  0.01836 |   0.01935 |   78.2       False\n| 38    |  0.01837 |   0.01861 |   80.4       False\n| 39    |  0.01825 |   0.01892 |   82.5       False\nEpoch    40: reducing learning rate of group 0 to 7.9210e-02.\n| 40    |  0.01828 |   0.01857 |   84.6       False\n| 41    |  0.01818 |   0.02067 |   86.7       False\n| 42    |  0.01832 |   0.01845 |   88.8       True\n| 43    |  0.01824 |   0.01829 |   91.0       True\n| 44    |  0.01818 |   0.01867 |   93.1       False\n| 45    |  0.01854 |   0.01886 |   95.1       False\n| 46    |  0.01829 |   0.01882 |   97.3       False\n| 47    |  0.01835 |   0.01859 |   99.4       False\n| 48    |  0.01818 |   0.01835 |   101.5      False\nEpoch    49: reducing learning rate of group 0 to 7.0497e-02.\n| 49    |  0.01874 |   0.01860 |   103.6      False\n| 50    |  0.01824 |   0.01854 |   105.7      False\n| 51    |  0.01815 |   0.01826 |   108.0      True\n| 52    |  0.01820 |   0.01870 |   110.0      False\n| 53    |  0.01813 |   0.01832 |   112.2      False\n| 54    |  0.01808 |   0.01902 |   114.3      False\n| 55    |  0.01804 |   0.01823 |   116.5      True\n| 56    |  0.01804 |   0.01860 |   118.6      False\n| 57    |  0.01827 |   0.01830 |   120.7      False\n| 58    |  0.01802 |   0.01823 |   122.8      False\n| 59    |  0.01799 |   0.01855 |   124.9      False\n| 60    |  0.01810 |   0.01832 |   127.1      False\nEpoch    61: reducing learning rate of group 0 to 6.2742e-02.\n| 61    |  0.01815 |   0.01844 |   129.1      False\n| 62    |  0.01799 |   0.01812 |   131.2      True\n| 63    |  0.01794 |   0.01833 |   133.3      False\n| 64    |  0.01801 |   0.01833 |   135.3      False\n| 65    |  0.01795 |   0.01864 |   137.4      False\n| 66    |  0.01817 |   0.01834 |   139.5      False\n| 67    |  0.01792 |   0.01825 |   141.6      False\nEpoch    68: reducing learning rate of group 0 to 5.5841e-02.\n| 68    |  0.01794 |   0.01825 |   143.7      False\n| 69    |  0.01781 |   0.01827 |   145.8      False\n| 70    |  0.01797 |   0.01818 |   148.0      False\n| 71    |  0.01784 |   0.01842 |   150.1      False\n| 72    |  0.01793 |   0.01808 |   152.1      True\n| 73    |  0.01787 |   0.01829 |   154.3      False\n| 74    |  0.01796 |   0.01803 |   156.4      True\n| 75    |  0.01789 |   0.01811 |   158.5      False\n| 76    |  0.01796 |   0.01829 |   160.7      False\n| 77    |  0.01794 |   0.01806 |   162.8      False\n| 78    |  0.01789 |   0.01836 |   165.1      False\n| 79    |  0.01801 |   0.01821 |   167.1      False\nEpoch    80: reducing learning rate of group 0 to 4.9698e-02.\n| 80    |  0.01774 |   0.01818 |   169.3      False\n| 81    |  0.01778 |   0.01807 |   171.3      False\n| 82    |  0.01764 |   0.01830 |   173.5      False\n| 83    |  0.01777 |   0.01810 |   175.6      False\n| 84    |  0.01775 |   0.01824 |   177.8      False\n| 85    |  0.01780 |   0.01828 |   179.9      False\nEpoch    86: reducing learning rate of group 0 to 4.4231e-02.\n| 86    |  0.01781 |   0.01808 |   182.0      False\n| 87    |  0.01769 |   0.01835 |   184.1      False\n| 88    |  0.01763 |   0.01799 |   186.3      True\n| 89    |  0.01755 |   0.01812 |   188.4      False\n| 90    |  0.01769 |   0.01804 |   190.4      False\n| 91    |  0.01760 |   0.01826 |   192.6      False\n| 92    |  0.01753 |   0.01835 |   194.8      False\n| 93    |  0.01773 |   0.01870 |   196.8      False\n| 94    |  0.01782 |   0.01799 |   198.9      True\n| 95    |  0.01768 |   0.01799 |   201.1      False\n| 96    |  0.01762 |   0.01823 |   203.3      False\n| 97    |  0.01773 |   0.01845 |   205.4      False\n| 98    |  0.01761 |   0.01821 |   207.6      False\n| 99    |  0.01750 |   0.02044 |   209.7      False\nEpoch   100: reducing learning rate of group 0 to 3.9366e-02.\n| 100   |  0.01825 |   0.01833 |   211.8      False\n| 101   |  0.01777 |   0.01791 |   214.0      True\n| 102   |  0.01755 |   0.01853 |   216.1      False\n| 103   |  0.01759 |   0.01804 |   218.3      False\n| 104   |  0.01753 |   0.01822 |   220.5      False\n| 105   |  0.01778 |   0.01802 |   222.6      False\n| 106   |  0.01780 |   0.01797 |   224.7      False\nEpoch   107: reducing learning rate of group 0 to 3.5036e-02.\n| 107   |  0.01762 |   0.01794 |   226.9      False\n| 108   |  0.01739 |   0.01784 |   229.0      True\n| 109   |  0.01754 |   0.01803 |   231.1      False\n| 110   |  0.01757 |   0.01793 |   233.2      False\n| 111   |  0.01753 |   0.01786 |   235.3      False\n| 112   |  0.01754 |   0.01820 |   237.5      False\n| 113   |  0.01751 |   0.01814 |   239.5      False\nEpoch   114: reducing learning rate of group 0 to 3.1182e-02.\n| 114   |  0.01744 |   0.01800 |   241.7      False\n| 115   |  0.01744 |   0.01820 |   243.7      False\n| 116   |  0.01747 |   0.01797 |   245.8      False\n| 117   |  0.01747 |   0.01800 |   247.9      False\n| 118   |  0.01749 |   0.01784 |   250.0      False\n| 119   |  0.01746 |   0.01769 |   252.1      True\n| 120   |  0.01746 |   0.01806 |   254.2      False\n| 121   |  0.01739 |   0.01771 |   256.3      False\n| 122   |  0.01742 |   0.01790 |   258.4      False\n| 123   |  0.01742 |   0.01819 |   260.6      False\n| 124   |  0.01746 |   0.01790 |   262.6      False\nEpoch   125: reducing learning rate of group 0 to 2.7752e-02.\n| 125   |  0.01745 |   0.01784 |   264.8      False\n| 126   |  0.01746 |   0.01781 |   266.8      False\n| 127   |  0.01749 |   0.01776 |   268.9      False\n| 128   |  0.01725 |   0.01783 |   271.1      False\n| 129   |  0.01730 |   0.01776 |   273.3      False\n| 130   |  0.01731 |   0.01791 |   275.4      False\n| 131   |  0.01735 |   0.01767 |   277.5      True\n| 132   |  0.01723 |   0.01783 |   279.6      False\n| 133   |  0.01712 |   0.01775 |   281.8      False\n| 134   |  0.01730 |   0.01790 |   283.9      False\n| 135   |  0.01741 |   0.01799 |   286.0      False\n| 136   |  0.01718 |   0.01783 |   288.1      False\nEpoch   137: reducing learning rate of group 0 to 2.4699e-02.\n| 137   |  0.01718 |   0.01778 |   290.2      False\n| 138   |  0.01711 |   0.01798 |   292.4      False\n| 139   |  0.01721 |   0.01802 |   294.5      False\n| 140   |  0.01720 |   0.01798 |   296.7      False\n| 141   |  0.01722 |   0.01776 |   298.8      False\n| 142   |  0.01712 |   0.01787 |   300.9      False\nEpoch   143: reducing learning rate of group 0 to 2.1982e-02.\n| 143   |  0.01721 |   0.01788 |   303.1      False\n| 144   |  0.01708 |   0.01810 |   305.3      False\n| 145   |  0.01709 |   0.01775 |   307.4      False\n| 146   |  0.01706 |   0.01784 |   309.5      False\n| 147   |  0.01718 |   0.01794 |   311.6      False\n| 148   |  0.01714 |   0.01771 |   313.8      False\nEpoch   149: reducing learning rate of group 0 to 1.9564e-02.\n| 149   |  0.01716 |   0.01795 |   315.9      False\n| 150   |  0.01708 |   0.01763 |   318.0      True\n| 151   |  0.01701 |   0.01763 |   320.2      False\n| 152   |  0.01711 |   0.01772 |   322.3      False\n| 153   |  0.01698 |   0.01770 |   324.4      False\n| 154   |  0.01708 |   0.01775 |   326.6      False\n| 155   |  0.01709 |   0.01765 |   328.7      False\nEpoch   156: reducing learning rate of group 0 to 1.7412e-02.\n| 156   |  0.01707 |   0.01772 |   330.8      False\n| 157   |  0.01689 |   0.01766 |   332.9      False\n| 158   |  0.01690 |   0.01801 |   335.2      False\n| 159   |  0.01728 |   0.01765 |   337.2      False\n| 160   |  0.01697 |   0.01776 |   339.3      False\n| 161   |  0.01689 |   0.01771 |   341.5      False\nEpoch   162: reducing learning rate of group 0 to 1.5497e-02.\n| 162   |  0.01691 |   0.01785 |   344.1      False\n| 163   |  0.01690 |   0.01756 |   346.9      True\n| 164   |  0.01681 |   0.01760 |   349.5      False\n| 165   |  0.01674 |   0.01760 |   352.2      False\n| 166   |  0.01682 |   0.01772 |   354.7      False\n| 167   |  0.01686 |   0.01758 |   356.9      False\n| 168   |  0.01681 |   0.01769 |   359.3      False\nEpoch   169: reducing learning rate of group 0 to 1.3792e-02.\n| 169   |  0.01686 |   0.01761 |   361.7      False\n| 170   |  0.01676 |   0.01759 |   364.3      False\n| 171   |  0.01676 |   0.01782 |   366.7      False\n| 172   |  0.01679 |   0.01758 |   369.2      False\n| 173   |  0.01678 |   0.01763 |   371.7      False\n| 174   |  0.01681 |   0.01759 |   374.0      False\nEpoch   175: reducing learning rate of group 0 to 1.2275e-02.\n| 175   |  0.01675 |   0.01770 |   376.3      False\n| 176   |  0.01664 |   0.01777 |   378.6      False\n| 177   |  0.01670 |   0.01766 |   381.0      False\n| 178   |  0.01667 |   0.01768 |   383.6      False\n| 179   |  0.01674 |   0.01757 |   386.5      False\n| 180   |  0.01674 |   0.01774 |   389.1      False\nEpoch   181: reducing learning rate of group 0 to 1.0925e-02.\n| 181   |  0.01669 |   0.01768 |   391.6      False\n| 182   |  0.01669 |   0.01771 |   393.9      False\n| 183   |  0.01654 |   0.01756 |   396.3      False\n| 184   |  0.01649 |   0.01763 |   398.7      False\n| 185   |  0.01662 |   0.01770 |   401.1      False\n| 186   |  0.01655 |   0.01776 |   403.6      False\nEpoch   187: reducing learning rate of group 0 to 9.7230e-03.\n| 187   |  0.01659 |   0.01768 |   406.2      False\n| 188   |  0.01650 |   0.01766 |   408.8      False\n| 189   |  0.01646 |   0.01769 |   411.3      False\n| 190   |  0.01653 |   0.01767 |   413.5      False\n| 191   |  0.01646 |   0.01765 |   415.6      False\n| 192   |  0.01649 |   0.01766 |   417.8      False\nEpoch   193: reducing learning rate of group 0 to 8.6535e-03.\n| 193   |  0.01644 |   0.01766 |   420.0      False\n| 194   |  0.01643 |   0.01766 |   422.2      False\n| 195   |  0.01646 |   0.01767 |   424.4      False\n| 196   |  0.01640 |   0.01778 |   426.5      False\n| 197   |  0.01644 |   0.01773 |   428.7      False\n| 198   |  0.01648 |   0.01775 |   430.9      False\nEpoch   199: reducing learning rate of group 0 to 7.7016e-03.\n| 199   |  0.01644 |   0.01764 |   433.2      False\n| 200   |  0.01646 |   0.01767 |   435.4      False\n| 201   |  0.01636 |   0.01763 |   437.6      False\n| 202   |  0.01629 |   0.01771 |   439.8      False\n| 203   |  0.01634 |   0.01780 |   441.9      False\n| 204   |  0.01637 |   0.01775 |   444.1      False\nEpoch   205: reducing learning rate of group 0 to 6.8544e-03.\n| 205   |  0.01632 |   0.01770 |   446.3      False\n| 206   |  0.01627 |   0.01771 |   448.5      False\n| 207   |  0.01626 |   0.01774 |   450.6      False\n| 208   |  0.01616 |   0.01770 |   452.8      False\n| 209   |  0.01640 |   0.01769 |   455.0      False\n| 210   |  0.01628 |   0.01770 |   457.1      False\nEpoch   211: reducing learning rate of group 0 to 6.1004e-03.\n| 211   |  0.01619 |   0.01764 |   459.2      False\n| 212   |  0.01631 |   0.01779 |   461.4      False\n| 213   |  0.01624 |   0.01777 |   463.6      False\n| 214   |  0.01617 |   0.01772 |   465.8      False\n| 215   |  0.01614 |   0.01775 |   468.0      False\n| 216   |  0.01619 |   0.01780 |   470.2      False\nEpoch   217: reducing learning rate of group 0 to 5.4294e-03.\n| 217   |  0.01613 |   0.01769 |   472.4      False\n| 218   |  0.01616 |   0.01773 |   474.5      False\n| 219   |  0.01611 |   0.01765 |   476.8      False\n| 220   |  0.01609 |   0.01767 |   479.0      False\n| 221   |  0.01603 |   0.01769 |   481.1      False\n| 222   |  0.01608 |   0.01777 |   483.3      False\nEpoch   223: reducing learning rate of group 0 to 4.8321e-03.\n| 223   |  0.01606 |   0.01768 |   485.5      False\n| 224   |  0.01599 |   0.01771 |   487.7      False\n| 225   |  0.01593 |   0.01786 |   489.8      False\n| 226   |  0.01602 |   0.01776 |   492.0      False\n| 227   |  0.01603 |   0.01777 |   494.2      False\n| 228   |  0.01599 |   0.01769 |   496.4      False\nEpoch   229: reducing learning rate of group 0 to 4.3006e-03.\n| 229   |  0.01600 |   0.01780 |   498.6      False\n| 230   |  0.01605 |   0.01767 |   500.8      False\n| 231   |  0.01595 |   0.01786 |   503.1      False\n| 232   |  0.01586 |   0.01779 |   505.3      False\n| 233   |  0.01589 |   0.01799 |   507.5      False\n| 234   |  0.01583 |   0.01781 |   509.6      False\nEpoch   235: reducing learning rate of group 0 to 3.8275e-03.\n| 235   |  0.01590 |   0.01777 |   511.9      False\n| 236   |  0.01586 |   0.01779 |   514.0      False\n| 237   |  0.01576 |   0.01789 |   516.2      False\n| 238   |  0.01570 |   0.01782 |   518.4      False\n| 239   |  0.01581 |   0.01792 |   520.6      False\n| 240   |  0.01571 |   0.01783 |   522.8      False\nEpoch   241: reducing learning rate of group 0 to 3.4065e-03.\n| 241   |  0.01573 |   0.01783 |   525.0      False\n| 242   |  0.01571 |   0.01788 |   527.1      False\n| 243   |  0.01563 |   0.01790 |   529.4      False\n| 244   |  0.01569 |   0.01789 |   531.6      False\n| 245   |  0.01570 |   0.01796 |   533.8      False\n| 246   |  0.01572 |   0.01796 |   536.0      False\nEpoch   247: reducing learning rate of group 0 to 3.0318e-03.\n| 247   |  0.01563 |   0.01791 |   538.2      False\n| 248   |  0.01554 |   0.01803 |   540.4      False\n| 249   |  0.01555 |   0.01791 |   542.5      False\n| 250   |  0.01557 |   0.01788 |   544.8      False\n| 251   |  0.01564 |   0.01805 |   547.0      False\n| 252   |  0.01553 |   0.01793 |   549.2      False\nEpoch   253: reducing learning rate of group 0 to 2.6983e-03.\n| 253   |  0.01558 |   0.01809 |   551.4      False\n| 254   |  0.01549 |   0.01794 |   553.6      False\n| 255   |  0.01537 |   0.01796 |   555.8      False\n| 256   |  0.01552 |   0.01799 |   558.0      False\n| 257   |  0.01542 |   0.01794 |   560.1      False\n| 258   |  0.01541 |   0.01804 |   562.2      False\nEpoch   259: reducing learning rate of group 0 to 2.4015e-03.\n| 259   |  0.01541 |   0.01810 |   564.5      False\n| 260   |  0.01534 |   0.01801 |   566.6      False\n| 261   |  0.01534 |   0.01802 |   568.9      False\n| 262   |  0.01542 |   0.01806 |   571.1      False\n| 263   |  0.01540 |   0.01808 |   573.3      False\n| 264   |  0.01532 |   0.01805 |   575.5      False\nEpoch   265: reducing learning rate of group 0 to 2.1373e-03.\n| 265   |  0.01529 |   0.01806 |   577.7      False\n| 266   |  0.01524 |   0.01804 |   579.9      False\n| 267   |  0.01520 |   0.01809 |   582.1      False\n| 268   |  0.01521 |   0.01815 |   584.3      False\n| 269   |  0.01522 |   0.01811 |   586.4      False\n| 270   |  0.01522 |   0.01814 |   588.6      False\nEpoch   271: reducing learning rate of group 0 to 1.9022e-03.\n| 271   |  0.01516 |   0.01816 |   590.9      False\n| 272   |  0.01514 |   0.01811 |   593.1      False\n| 273   |  0.01510 |   0.01819 |   595.3      False\n| 274   |  0.01512 |   0.01818 |   597.5      False\n| 275   |  0.01509 |   0.01830 |   599.7      False\n| 276   |  0.01506 |   0.01816 |   601.9      False\nEpoch   277: reducing learning rate of group 0 to 1.6930e-03.\n| 277   |  0.01507 |   0.01823 |   604.2      False\n| 278   |  0.01508 |   0.01832 |   606.4      False\n| 279   |  0.01504 |   0.01829 |   608.5      False\n| 280   |  0.01506 |   0.01831 |   610.7      False\n| 281   |  0.01509 |   0.01823 |   613.0      False\n| 282   |  0.01491 |   0.01828 |   615.1      False\nEpoch   283: reducing learning rate of group 0 to 1.5067e-03.\n| 283   |  0.01492 |   0.01829 |   617.3      False\n| 284   |  0.01492 |   0.01826 |   619.5      False\n| 285   |  0.01490 |   0.01840 |   621.8      False\n| 286   |  0.01488 |   0.01832 |   623.9      False\n| 287   |  0.01488 |   0.01837 |   626.2      False\n| 288   |  0.01492 |   0.01842 |   628.4      False\nEpoch   289: reducing learning rate of group 0 to 1.3410e-03.\n| 289   |  0.01486 |   0.01848 |   630.6      False\n| 290   |  0.01485 |   0.01838 |   632.8      False\n| 291   |  0.01480 |   0.01839 |   635.3      False\n| 292   |  0.01480 |   0.01842 |   637.5      False\n| 293   |  0.01483 |   0.01838 |   639.7      False\n| 294   |  0.01478 |   0.01837 |   641.9      False\nEpoch   295: reducing learning rate of group 0 to 1.1935e-03.\n| 295   |  0.01472 |   0.01843 |   644.1      False\n| 296   |  0.01475 |   0.01842 |   646.3      False\n| 297   |  0.01471 |   0.01845 |   648.6      False\n| 298   |  0.01479 |   0.01853 |   650.7      False\n| 299   |  0.01478 |   0.01855 |   652.9      False\n| 300   |  0.01479 |   0.01850 |   655.1      False\nTraining done in 655.096 seconds. Best loss : 0.01756\n---------------------------------------\nSEED [0] - FOLD 4\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 35 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  0.15226 |   0.04436 |   2.2        True\n| 2     |  0.02572 |   0.02241 |   4.4        True\n| 3     |  0.02166 |   0.02204 |   6.5        True\n| 4     |  0.02109 |   0.02318 |   8.7        False\n| 5     |  0.02088 |   0.02174 |   10.9       True\n| 6     |  0.02011 |   0.02228 |   13.2       False\n| 7     |  0.01996 |   0.02207 |   15.3       False\n| 8     |  0.01960 |   0.02398 |   17.6       False\n| 9     |  0.01985 |   0.02088 |   19.8       True\n| 10    |  0.01953 |   0.02000 |   22.1       True\n| 11    |  0.01943 |   0.02274 |   24.2       False\n| 12    |  0.01923 |   0.01954 |   26.5       True\n| 13    |  0.01898 |   0.01945 |   28.7       True\n| 14    |  0.01894 |   0.01976 |   30.9       False\n| 15    |  0.01896 |   0.01930 |   33.1       True\n| 16    |  0.01884 |   0.01952 |   35.3       False\n| 17    |  0.01890 |   0.01941 |   37.4       False\n| 18    |  0.01903 |   0.01929 |   39.6       True\n| 19    |  0.01877 |   0.01916 |   41.8       True\n| 20    |  0.01887 |   0.01931 |   44.0       False\n| 21    |  0.01890 |   0.01888 |   46.1       True\n| 22    |  0.01869 |   0.01940 |   48.2       False\n| 23    |  0.01879 |   0.01909 |   50.4       False\n| 24    |  0.01890 |   0.01987 |   52.6       False\n| 25    |  0.01879 |   0.01956 |   54.9       False\n| 26    |  0.01874 |   0.01903 |   57.1       False\nEpoch    27: reducing learning rate of group 0 to 8.9000e-02.\n| 27    |  0.01867 |   0.01914 |   59.3       False\n| 28    |  0.01877 |   0.01918 |   61.5       False\n| 29    |  0.01850 |   0.01930 |   63.6       False\n| 30    |  0.01870 |   0.01899 |   65.8       False\n| 31    |  0.01857 |   0.01937 |   68.0       False\n| 32    |  0.01870 |   0.01889 |   70.2       False\n| 33    |  0.01862 |   0.01887 |   72.4       True\n| 34    |  0.01873 |   0.01883 |   74.6       True\n| 35    |  0.01861 |   0.01898 |   76.9       False\n| 36    |  0.01876 |   0.01908 |   79.1       False\n| 37    |  0.01846 |   0.01892 |   81.3       False\n| 38    |  0.01848 |   0.01884 |   83.5       False\n| 39    |  0.01861 |   0.01918 |   85.6       False\nEpoch    40: reducing learning rate of group 0 to 7.9210e-02.\n| 40    |  0.01837 |   0.01911 |   87.8       False\n| 41    |  0.01848 |   0.01934 |   90.1       False\n| 42    |  0.01858 |   0.01908 |   92.3       False\n| 43    |  0.01844 |   0.02155 |   94.4       False\n| 44    |  0.01835 |   0.01896 |   96.6       False\n| 45    |  0.01830 |   0.01958 |   98.9       False\nEpoch    46: reducing learning rate of group 0 to 7.0497e-02.\n| 46    |  0.01833 |   0.01964 |   101.1      False\n| 47    |  0.01818 |   0.01880 |   103.3      True\n| 48    |  0.01824 |   0.01991 |   105.6      False\n| 49    |  0.01843 |   0.01852 |   107.8      True\n| 50    |  0.01819 |   0.01899 |   110.0      False\n| 51    |  0.01825 |   0.01871 |   112.2      False\n| 52    |  0.01830 |   0.01891 |   114.4      False\n| 53    |  0.01818 |   0.01911 |   116.6      False\n| 54    |  0.01837 |   0.01990 |   118.8      False\nEpoch    55: reducing learning rate of group 0 to 6.2742e-02.\n| 55    |  0.01834 |   0.01904 |   121.1      False\n| 56    |  0.01821 |   0.01944 |   123.2      False\n| 57    |  0.01822 |   0.01881 |   125.3      False\n| 58    |  0.01818 |   0.01927 |   127.5      False\n| 59    |  0.01827 |   0.01886 |   129.8      False\n| 60    |  0.01830 |   0.01862 |   131.9      False\nEpoch    61: reducing learning rate of group 0 to 5.5841e-02.\n| 61    |  0.01835 |   0.01904 |   134.1      False\n| 62    |  0.01818 |   0.01858 |   136.3      False\n| 63    |  0.01804 |   0.01867 |   138.6      False\n| 64    |  0.01811 |   0.02010 |   140.7      False\n| 65    |  0.01821 |   0.01878 |   142.9      False\n| 66    |  0.01814 |   0.01853 |   145.1      False\nEpoch    67: reducing learning rate of group 0 to 4.9698e-02.\n| 67    |  0.01796 |   0.01911 |   147.3      False\n| 68    |  0.01790 |   0.01934 |   149.5      False\n| 69    |  0.01788 |   0.01840 |   151.8      True\n| 70    |  0.01800 |   0.01906 |   154.0      False\n| 71    |  0.01827 |   0.01867 |   156.2      False\n| 72    |  0.01805 |   0.01849 |   158.4      False\n| 73    |  0.01786 |   0.01865 |   160.7      False\n| 74    |  0.01786 |   0.01856 |   162.9      False\n| 75    |  0.01783 |   0.01832 |   165.2      True\n| 76    |  0.01778 |   0.01852 |   167.4      False\n| 77    |  0.01791 |   0.01838 |   169.6      False\n| 78    |  0.01792 |   0.01890 |   171.8      False\n| 79    |  0.01795 |   0.01851 |   174.1      False\n| 80    |  0.01792 |   0.01839 |   176.3      False\nEpoch    81: reducing learning rate of group 0 to 4.4231e-02.\n| 81    |  0.01803 |   0.01964 |   179.4      False\n| 82    |  0.01784 |   0.01832 |   182.2      True\n| 83    |  0.01799 |   0.01856 |   185.5      False\n| 84    |  0.01815 |   0.01840 |   188.2      False\n| 85    |  0.01788 |   0.01829 |   190.3      True\n| 86    |  0.01776 |   0.01869 |   192.5      False\n| 87    |  0.01777 |   0.01841 |   194.7      False\n| 88    |  0.01802 |   0.01830 |   196.9      False\n| 89    |  0.01803 |   0.01847 |   199.2      False\n| 90    |  0.01780 |   0.01849 |   201.4      False\nEpoch    91: reducing learning rate of group 0 to 3.9366e-02.\n| 91    |  0.01787 |   0.01860 |   204.7      False\n| 92    |  0.01754 |   0.01836 |   207.1      False\n| 93    |  0.01769 |   0.01884 |   209.8      False\n| 94    |  0.01766 |   0.01822 |   212.6      True\n| 95    |  0.01761 |   0.01833 |   215.1      False\n| 96    |  0.01781 |   0.01825 |   217.5      False\n| 97    |  0.01779 |   0.01822 |   219.8      True\n| 98    |  0.01771 |   0.01842 |   222.4      False\n| 99    |  0.01763 |   0.01839 |   224.4      False\nEpoch   100: reducing learning rate of group 0 to 3.5036e-02.\n| 100   |  0.01792 |   0.01833 |   226.6      False\n| 101   |  0.01779 |   0.01824 |   228.7      False\n| 102   |  0.01777 |   0.01832 |   230.7      False\n| 103   |  0.01775 |   0.01818 |   232.7      True\n| 104   |  0.01771 |   0.01835 |   234.7      False\n| 105   |  0.01755 |   0.01838 |   236.8      False\n| 106   |  0.01769 |   0.01832 |   238.8      False\n| 107   |  0.01759 |   0.01851 |   240.8      False\n| 108   |  0.01773 |   0.01837 |   242.9      False\nEpoch   109: reducing learning rate of group 0 to 3.1182e-02.\n| 109   |  0.01764 |   0.01822 |   244.9      False\n| 110   |  0.01740 |   0.01815 |   246.9      True\n| 111   |  0.01762 |   0.01813 |   248.9      True\n| 112   |  0.01751 |   0.01841 |   251.0      False\n| 113   |  0.01765 |   0.01818 |   253.1      False\n| 114   |  0.01756 |   0.01827 |   256.2      False\n| 115   |  0.01762 |   0.01827 |   258.5      False\n| 116   |  0.01766 |   0.01861 |   260.5      False\nEpoch   117: reducing learning rate of group 0 to 2.7752e-02.\n| 117   |  0.01774 |   0.01836 |   263.0      False\n| 118   |  0.01752 |   0.01891 |   265.2      False\n| 119   |  0.01756 |   0.01817 |   267.3      False\n| 120   |  0.01747 |   0.01827 |   269.4      False\n| 121   |  0.01747 |   0.01801 |   271.4      True\n| 122   |  0.01754 |   0.01809 |   273.5      False\n| 123   |  0.01736 |   0.01805 |   275.5      False\n| 124   |  0.01741 |   0.01861 |   278.0      False\n| 125   |  0.01746 |   0.01832 |   280.0      False\n| 126   |  0.01763 |   0.01830 |   281.9      False\nEpoch   127: reducing learning rate of group 0 to 2.4699e-02.\n| 127   |  0.01747 |   0.01811 |   283.8      False\n| 128   |  0.01734 |   0.01799 |   285.8      True\n| 129   |  0.01735 |   0.01809 |   287.7      False\n| 130   |  0.01740 |   0.01801 |   289.8      False\n| 131   |  0.01738 |   0.01805 |   291.7      False\n| 132   |  0.01740 |   0.01797 |   293.6      True\n| 133   |  0.01719 |   0.01796 |   295.7      True\n| 134   |  0.01732 |   0.01898 |   297.7      False\n| 135   |  0.01756 |   0.01822 |   299.8      False\n| 136   |  0.01742 |   0.01816 |   301.7      False\n| 137   |  0.01727 |   0.01800 |   303.6      False\n| 138   |  0.01730 |   0.01804 |   305.7      False\nEpoch   139: reducing learning rate of group 0 to 2.1982e-02.\n| 139   |  0.01737 |   0.01820 |   307.8      False\n| 140   |  0.01732 |   0.01800 |   309.9      False\n| 141   |  0.01728 |   0.01796 |   311.8      True\n| 142   |  0.01733 |   0.01801 |   313.8      False\n| 143   |  0.01727 |   0.01795 |   315.9      True\n| 144   |  0.01732 |   0.01805 |   318.0      False\n| 145   |  0.01720 |   0.01805 |   320.0      False\n| 146   |  0.01714 |   0.01798 |   321.9      False\n| 147   |  0.01717 |   0.01823 |   323.8      False\n| 148   |  0.01723 |   0.01791 |   326.0      True\n| 149   |  0.01714 |   0.01801 |   328.2      False\n| 150   |  0.01711 |   0.01805 |   330.6      False\n| 151   |  0.01721 |   0.01811 |   332.7      False\n| 152   |  0.01718 |   0.01821 |   334.7      False\n| 153   |  0.01736 |   0.01793 |   336.7      False\nEpoch   154: reducing learning rate of group 0 to 1.9564e-02.\n| 154   |  0.01724 |   0.01801 |   338.7      False\n| 155   |  0.01719 |   0.01799 |   340.8      False\n| 156   |  0.01722 |   0.01797 |   342.9      False\n| 157   |  0.01730 |   0.01821 |   344.9      False\n| 158   |  0.01724 |   0.01787 |   346.9      True\n| 159   |  0.01711 |   0.01805 |   349.0      False\n| 160   |  0.01695 |   0.01791 |   350.8      False\n| 161   |  0.01704 |   0.01789 |   352.9      False\n| 162   |  0.01709 |   0.01790 |   355.0      False\n| 163   |  0.01726 |   0.01829 |   356.9      False\nEpoch   164: reducing learning rate of group 0 to 1.7412e-02.\n| 164   |  0.01723 |   0.01814 |   359.0      False\n| 165   |  0.01698 |   0.01794 |   361.1      False\n| 166   |  0.01698 |   0.01794 |   363.1      False\n| 167   |  0.01701 |   0.01776 |   365.2      True\n| 168   |  0.01698 |   0.01787 |   367.2      False\n| 169   |  0.01685 |   0.01792 |   369.1      False\n| 170   |  0.01693 |   0.01784 |   371.0      False\n| 171   |  0.01696 |   0.01801 |   373.0      False\n| 172   |  0.01695 |   0.01789 |   375.1      False\nEpoch   173: reducing learning rate of group 0 to 1.5497e-02.\n| 173   |  0.01693 |   0.01804 |   377.2      False\n| 174   |  0.01694 |   0.01786 |   379.3      False\n| 175   |  0.01682 |   0.01787 |   381.3      False\n| 176   |  0.01687 |   0.01801 |   383.2      False\n| 177   |  0.01693 |   0.01787 |   385.1      False\n| 178   |  0.01692 |   0.01793 |   387.0      False\nEpoch   179: reducing learning rate of group 0 to 1.3792e-02.\n| 179   |  0.01705 |   0.01786 |   389.1      False\n| 180   |  0.01687 |   0.01796 |   391.0      False\n| 181   |  0.01692 |   0.01783 |   392.9      False\n| 182   |  0.01682 |   0.01792 |   394.9      False\n| 183   |  0.01673 |   0.01777 |   396.9      False\n| 184   |  0.01671 |   0.01780 |   398.9      False\nEpoch   185: reducing learning rate of group 0 to 1.2275e-02.\n| 185   |  0.01685 |   0.01778 |   400.9      False\n| 186   |  0.01685 |   0.01777 |   402.9      False\n| 187   |  0.01683 |   0.01771 |   404.9      True\n| 188   |  0.01663 |   0.01781 |   406.9      False\n| 189   |  0.01671 |   0.01775 |   408.9      False\n| 190   |  0.01680 |   0.01779 |   410.9      False\n| 191   |  0.01677 |   0.01780 |   412.9      False\n| 192   |  0.01676 |   0.01785 |   414.9      False\nEpoch   193: reducing learning rate of group 0 to 1.0925e-02.\n| 193   |  0.01675 |   0.01774 |   416.8      False\n| 194   |  0.01678 |   0.01789 |   418.8      False\n| 195   |  0.01675 |   0.01776 |   420.8      False\n| 196   |  0.01659 |   0.01766 |   422.8      True\n| 197   |  0.01653 |   0.01780 |   424.8      False\n| 198   |  0.01663 |   0.01787 |   426.8      False\n| 199   |  0.01664 |   0.01805 |   428.8      False\n| 200   |  0.01671 |   0.01800 |   430.9      False\n| 201   |  0.01659 |   0.01777 |   433.0      False\nEpoch   202: reducing learning rate of group 0 to 9.7230e-03.\n| 202   |  0.01657 |   0.01777 |   435.0      False\n| 203   |  0.01659 |   0.01771 |   437.0      False\n| 204   |  0.01647 |   0.01784 |   439.0      False\n| 205   |  0.01647 |   0.01787 |   441.0      False\n| 206   |  0.01655 |   0.01807 |   443.0      False\n| 207   |  0.01652 |   0.01812 |   445.0      False\nEpoch   208: reducing learning rate of group 0 to 8.6535e-03.\n| 208   |  0.01661 |   0.01790 |   446.9      False\n| 209   |  0.01650 |   0.01777 |   449.1      False\n| 210   |  0.01650 |   0.01780 |   451.1      False\n| 211   |  0.01646 |   0.01805 |   453.1      False\n| 212   |  0.01647 |   0.01778 |   455.1      False\n| 213   |  0.01654 |   0.01782 |   457.1      False\nEpoch   214: reducing learning rate of group 0 to 7.7016e-03.\n| 214   |  0.01640 |   0.01788 |   459.2      False\n| 215   |  0.01648 |   0.01769 |   461.3      False\n| 216   |  0.01633 |   0.01777 |   463.2      False\n| 217   |  0.01626 |   0.01781 |   465.2      False\n| 218   |  0.01638 |   0.01775 |   467.3      False\n| 219   |  0.01631 |   0.01772 |   469.4      False\nEpoch   220: reducing learning rate of group 0 to 6.8544e-03.\n| 220   |  0.01631 |   0.01779 |   471.3      False\n| 221   |  0.01631 |   0.01770 |   473.7      False\n| 222   |  0.01615 |   0.01777 |   476.0      False\n| 223   |  0.01622 |   0.01783 |   478.4      False\n| 224   |  0.01629 |   0.01786 |   480.6      False\n| 225   |  0.01625 |   0.01785 |   482.8      False\nEpoch   226: reducing learning rate of group 0 to 6.1004e-03.\n| 226   |  0.01632 |   0.01782 |   485.2      False\n| 227   |  0.01604 |   0.01782 |   487.6      False\n| 228   |  0.01613 |   0.01781 |   489.7      False\n| 229   |  0.01618 |   0.01780 |   491.8      False\n| 230   |  0.01616 |   0.01781 |   493.8      False\n| 231   |  0.01601 |   0.01784 |   496.6      False\nEpoch   232: reducing learning rate of group 0 to 5.4294e-03.\n| 232   |  0.01598 |   0.01771 |   499.6      False\n| 233   |  0.01605 |   0.01787 |   502.3      False\n| 234   |  0.01606 |   0.01790 |   504.7      False\n| 235   |  0.01608 |   0.01780 |   507.1      False\n| 236   |  0.01595 |   0.01777 |   509.2      False\n| 237   |  0.01601 |   0.01774 |   512.4      False\nEpoch   238: reducing learning rate of group 0 to 4.8321e-03.\n| 238   |  0.01608 |   0.01783 |   516.5      False\n| 239   |  0.01584 |   0.01785 |   519.0      False\n| 240   |  0.01602 |   0.01781 |   522.1      False\n| 241   |  0.01594 |   0.01786 |   524.5      False\n| 242   |  0.01587 |   0.01787 |   526.9      False\n| 243   |  0.01589 |   0.01796 |   529.5      False\nEpoch   244: reducing learning rate of group 0 to 4.3006e-03.\n| 244   |  0.01579 |   0.01797 |   532.7      False\n| 245   |  0.01571 |   0.01787 |   535.2      False\n| 246   |  0.01577 |   0.01794 |   538.0      False\n| 247   |  0.01572 |   0.01783 |   541.1      False\n| 248   |  0.01579 |   0.01793 |   544.3      False\n| 249   |  0.01584 |   0.01782 |   546.6      False\nEpoch   250: reducing learning rate of group 0 to 3.8275e-03.\n| 250   |  0.01588 |   0.01788 |   548.7      False\n| 251   |  0.01571 |   0.01780 |   550.9      False\n| 252   |  0.01582 |   0.01787 |   553.1      False\n| 253   |  0.01574 |   0.01790 |   555.3      False\n| 254   |  0.01563 |   0.01796 |   557.4      False\n| 255   |  0.01554 |   0.01796 |   559.6      False\nEpoch   256: reducing learning rate of group 0 to 3.4065e-03.\n| 256   |  0.01577 |   0.01798 |   561.7      False\n| 257   |  0.01569 |   0.01795 |   563.9      False\n| 258   |  0.01551 |   0.01799 |   566.1      False\n| 259   |  0.01559 |   0.01802 |   568.3      False\n| 260   |  0.01572 |   0.01799 |   570.5      False\n| 261   |  0.01559 |   0.01798 |   572.7      False\nEpoch   262: reducing learning rate of group 0 to 3.0318e-03.\n| 262   |  0.01550 |   0.01795 |   574.9      False\n| 263   |  0.01550 |   0.01793 |   577.1      False\n| 264   |  0.01563 |   0.01814 |   579.4      False\n| 265   |  0.01563 |   0.01799 |   581.6      False\n| 266   |  0.01553 |   0.01815 |   584.2      False\n| 267   |  0.01548 |   0.01803 |   587.1      False\nEpoch   268: reducing learning rate of group 0 to 2.6983e-03.\n| 268   |  0.01550 |   0.01797 |   590.6      False\n| 269   |  0.01551 |   0.01800 |   593.3      False\n| 270   |  0.01545 |   0.01804 |   595.5      False\n| 271   |  0.01545 |   0.01804 |   597.7      False\n| 272   |  0.01527 |   0.01802 |   599.9      False\n| 273   |  0.01534 |   0.01803 |   602.1      False\nEpoch   274: reducing learning rate of group 0 to 2.4015e-03.\n| 274   |  0.01526 |   0.01808 |   604.3      False\n| 275   |  0.01524 |   0.01804 |   606.4      False\n| 276   |  0.01518 |   0.01803 |   608.5      False\n| 277   |  0.01524 |   0.01811 |   610.8      False\n| 278   |  0.01525 |   0.01813 |   612.9      False\n| 279   |  0.01543 |   0.01810 |   615.2      False\nEpoch   280: reducing learning rate of group 0 to 2.1373e-03.\n| 280   |  0.01524 |   0.01814 |   617.4      False\n| 281   |  0.01523 |   0.01804 |   619.6      False\n| 282   |  0.01504 |   0.01814 |   621.8      False\n| 283   |  0.01512 |   0.01824 |   624.0      False\n| 284   |  0.01507 |   0.01817 |   626.3      False\n| 285   |  0.01511 |   0.01825 |   628.4      False\nEpoch   286: reducing learning rate of group 0 to 1.9022e-03.\n| 286   |  0.01513 |   0.01817 |   630.6      False\n| 287   |  0.01517 |   0.01824 |   632.8      False\n| 288   |  0.01500 |   0.01825 |   635.0      False\n| 289   |  0.01510 |   0.01827 |   637.1      False\n| 290   |  0.01510 |   0.01830 |   639.3      False\n| 291   |  0.01502 |   0.01835 |   641.5      False\nEpoch   292: reducing learning rate of group 0 to 1.6930e-03.\n| 292   |  0.01486 |   0.01837 |   643.6      False\n| 293   |  0.01488 |   0.01834 |   645.8      False\n| 294   |  0.01486 |   0.01837 |   647.9      False\n| 295   |  0.01479 |   0.01834 |   650.1      False\n| 296   |  0.01487 |   0.01850 |   652.3      False\n| 297   |  0.01485 |   0.01848 |   654.5      False\nEpoch   298: reducing learning rate of group 0 to 1.5067e-03.\n| 298   |  0.01483 |   0.01844 |   656.7      False\n| 299   |  0.01480 |   0.01842 |   658.9      False\n| 300   |  0.01477 |   0.01847 |   661.1      False\nTraining done in 661.139 seconds. Best loss : 0.01766\n---------------------------------------\n"
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "# SEED = [0,1,2,3,4,5,6] #<-- Update\n",
    "SEED = [0]\n",
    "oof = np.zeros((train_df.shape[0], train_targets_scored.shape[1]))\n",
    "predictions = np.zeros((test_df.shape[0], train_targets_scored.shape[1]))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train_df[train_targets_scored.columns] = oof\n",
    "test_df[train_targets_scored.columns] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV log_loss:  0.01759779843636604\n"
    }
   ],
   "source": [
    "# valid_results = train_targets_scored.drop(columns=train_targets_scored.columns).merge(train[['sig_id']+target_scored_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "y_true = train_targets_scored.values\n",
    "y_pred = train_df[train_targets_scored.columns].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(train_targets_scored.columns)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / train_targets_scored.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6237775398126352\n"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV log_loss:  0.017530493076413634 - 0.6326524137654977\n",
    "#repoch 35 - CV log_loss:  0.01753016342198732 - 0.6341601932337556\n",
    "#avec vrai gauss rank - CV log_loss:  0.01752392583464945 - 0.630131576466939\n",
    "#gamma 1.4 CV log_loss:  0.01750923874224864  - 0.6317490046360553\n",
    "#shared 1 - CV log_loss:  0.017507106913879845 - 0.6393618612870094\n",
    "\n",
    "\n",
    "\n",
    "#patience 35, pat_sch 10 - CV log_loss:  0.017479210852207033 - 0.6407340883204129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline CV log_loss:  0.01754342404326328 0.6248144785829379\n",
    "#without kmeans : CV log_loss:  0.017544906477526573 0.6324338816269723\n",
    "#g600 c50 : CV log_loss:  0.01758957586194886 - 0.629949728832278\n",
    "#64/64 CV log_loss:  0.017583096074781448 - 0.6209159297054776\n",
    "\n",
    "#with loss2 - CV log_loss:  0.017794634325947235 0.6275013092647985\n",
    "#with loss2 PCA g600 c50 - CV log_loss:  0.01781988120869796 0.6298911856651602\n",
    "#with loss2 patience 25 - CV log_loss:  0.01779312218380405 0.6281097324835089\n",
    "#with loss2 patience 30 - CV log_loss:  0.017790775082051624 0.6293814639679891\n",
    "#with loss2 patience 35 - CV log_loss:  0.01779312218380405 0.6281097324835089\n",
    "#with loss2 patience 30 + virtual_batch_size 64 - CV log_loss:  0.01787414236838633 0.6346938458990472\n",
    "#with loss2 patience 30 + virtual_batch_size 16 - CV log_loss:  0.017741189570392005 0.6226257296329173\n",
    "#with loss2 patience 35 + virtual_batch_size 16 - CV log_loss:  0.017730623769838815 0.6249452053517877\n",
    "#with loss2 patience 40/ EPOCH 300 + virtual_batch_size 16 - CV log_loss:  0.017715413609244578 0.6266322089415273\n",
    "\n",
    "#pca g 100 CV log_loss:  0.017574289640172935 0.6241280400069048\n",
    "#pca g 140 CV log_loss:  CV log_loss:  0.017551766904420348 0.6303389793487976\n",
    "#relu -> prelu : CV log_loss:  0.017567823483257198 0.6293113851846648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#baseline model              CV log_loss:  0.016687406017433053 0.7492731581216698\n",
    "# patience 30                CV log_loss:  0.01690850658900589  0.735966961676482\n",
    "# stats g,c (sum,std)        CV log_loss:  0.01682518316741461  0.7404482804848934\n",
    "# stats g,c,gc (sum,std)     CV log_loss:  0.016810316073916132 0.7439597605459504\n",
    "# stats g,c,gc (sum,std,mean)CV log_loss:  0.016802178301951694 0.7393447505525768\n",
    "# stats g,c (sum,std,mean) , gc (sum,std) CV log_loss:  0.01680768146509414 0.7398723816282889\n",
    "# stats g,c (sum,std,mean,kurt, gc (sum,std) CV log_loss:  0.016789419505802778 0.7452451122810315\n",
    "# stats gc,c (+skew)                         CV log_loss:  0.01676525710038393 0.738088542786878\n",
    "\n",
    "#modif PCA\n",
    "#CV log_loss :  0.016842789818741998 - 0.7398728478185493\n",
    "#g 80 -> 100 : CV log_loss:  0.0168098197740287 0.7426392383610864\n",
    "#g80 -> 100 + all stats : CV log_loss:  0.01680981977402873 0.7426392383610864\n",
    "#g100 c10 -> c20 CV log_loss:  0.016816606944252275 0.746173280900466\n",
    "#g100 -> 150 CV log_loss:  0.01675937045748948 - 0.747606042991269\n",
    "#g150 -> 200 CV log_loss:  0.016843896450527557- 0.7365906276733302\n",
    "#c20 -> 30 CV log_loss:  0.016823623179551828 0.7407075622326613\n",
    "#relu -> prelu  CV log_loss:  0.016814056825770442 0.7449464736120271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"model_3_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}