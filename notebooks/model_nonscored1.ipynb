{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1605786381256",
   "display_name": "Python 3.7.9 64-bit ('moa_kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['train_targets_scored.csv',\n 'sample_submission.csv',\n '.gitkeep',\n 'train_drug.csv',\n 'train_features.csv',\n 'test_features.csv',\n 'train_targets_nonscored.csv']"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "data_dir = '../data/01_raw'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "no_ctl = True\n",
    "ncompo_genes = 600\n",
    "ncompo_cells = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(data_dir+'/train_features.csv')\n",
    "train_targets_scored = pd.read_csv(data_dir+'/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv(data_dir+'/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv(data_dir+'/test_features.csv')\n",
    "sample_submission = pd.read_csv(data_dir+'/sample_submission.csv')\n",
    "drug = pd.read_csv(data_dir+'/train_drug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n23812  id_fffcb9e7c                            0                       0   \n\n       acat_inhibitor  acetylcholine_receptor_agonist  \\\n23812               0                               0   \n\n       acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n23812                                  0                               0   \n\n       adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n23812                           0                              0   \n\n       adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n23812                           0  ...                                      0   \n\n       trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n23812             0                0                  0   \n\n       tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n23812                          0                                      0   \n\n       vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n23812                0          0                           0              0  \n\n[1 rows x 207 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sig_id</th>\n      <th>5-alpha_reductase_inhibitor</th>\n      <th>11-beta-hsd1_inhibitor</th>\n      <th>acat_inhibitor</th>\n      <th>acetylcholine_receptor_agonist</th>\n      <th>acetylcholine_receptor_antagonist</th>\n      <th>acetylcholinesterase_inhibitor</th>\n      <th>adenosine_receptor_agonist</th>\n      <th>adenosine_receptor_antagonist</th>\n      <th>adenylyl_cyclase_activator</th>\n      <th>...</th>\n      <th>tropomyosin_receptor_kinase_inhibitor</th>\n      <th>trpv_agonist</th>\n      <th>trpv_antagonist</th>\n      <th>tubulin_inhibitor</th>\n      <th>tyrosine_kinase_inhibitor</th>\n      <th>ubiquitin_specific_protease_inhibitor</th>\n      <th>vegfr_inhibitor</th>\n      <th>vitamin_b</th>\n      <th>vitamin_d_receptor_agonist</th>\n      <th>wnt_inhibitor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>23812</th>\n      <td>id_fffcb9e7c</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows Ã— 207 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train_targets_scored[train_targets_scored[\"sig_id\"] == \"id_fffcb9e7c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_nscored = train_targets_nonscored.columns[1:]\n",
    "nscored = train_targets_nonscored.merge(drug, on='sig_id', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "not_ctl\n"
    }
   ],
   "source": [
    "if no_ctl:\n",
    "    # cp_type == ctl_vehicle\n",
    "    print(\"not_ctl\")\n",
    "    train_features = train_features[train_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "    test_features = test_features[test_features[\"cp_type\"] != \"ctl_vehicle\"]\n",
    "    train_targets_nonscored = train_targets_nonscored.iloc[train_features.index]\n",
    "    train_features.reset_index(drop = True, inplace = True)\n",
    "    test_features.reset_index(drop = True, inplace = True)\n",
    "    train_targets_nonscored.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Indiquer si valeur dans le range max, min\n",
    "\n",
    "# import seaborn as sns\n",
    "# data = pd.concat([train_features,test_features],axis=0)\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"ctl_vehicle\"][\"c-4\"],label=\"normal\")\n",
    "\n",
    "# sns.distplot(data[data[\"cp_type\"] == \"trt_cp\"][\"c-4\"],label=\"treated\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets = train_targets_scored[[c for c in train_targets_scored.columns if (c != \"sig_id\")]].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_inhibitor\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_activator\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_agonist\",\"\"))\n",
    "# sum_targets[\"index\"] = sum_targets[\"index\"].apply(lambda x : x.replace(\"_antagonist\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# train_features[GENES].apply(lambda x : stats.moment(x,moment=5),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RankGauss\n",
    "for col in (GENES + CELLS):\n",
    "    transformer = QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n",
    "    vec_len = len(train_features[col].values)\n",
    "    vec_len_test = len(test_features[col].values)\n",
    "    raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n",
    "\n",
    "#True gauss rank\n",
    "# import cupy as cp\n",
    "# from cupyx.scipy.special import erfinv\n",
    "# epsilon = 1e-6\n",
    "\n",
    "# data = pd.concat([pd.DataFrame(train_features[GENES + CELLS]), pd.DataFrame(test_features[GENES + CELLS])])\n",
    "\n",
    "# for k in (GENES + CELLS):\n",
    "#     r_gpu = cp.array(data.loc[:,k])\n",
    "#     r_gpu = r_gpu.argsort().argsort()\n",
    "#     r_gpu = (r_gpu/r_gpu.max()-0.5)*2 \n",
    "#     r_gpu = cp.clip(r_gpu,-1+epsilon,1-epsilon)\n",
    "#     r_gpu = erfinv(r_gpu) \n",
    "#     data.loc[:,k] = cp.asnumpy( r_gpu * np.sqrt(2) )\n",
    "\n",
    "# train_features[GENES + CELLS] = data[:train_features.shape[0]]; test_features[GENES + CELLS] = data[-test_features.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENES\n",
    "n_comp = ncompo_genes \n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[train_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELLS\n",
    "n_comp = ncompo_cells\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def fe_stats(train, test):\n",
    "    \n",
    "    features_g = GENES\n",
    "    features_c = CELLS\n",
    "    \n",
    "    for df in train, test:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1) ## <==\n",
    "        # df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1) ## <==\n",
    "        # df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        #df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        # df['g_q25'] = df[features_g].quantile(q=.25,axis = 1)\n",
    "        # df['g_q50'] = df[features_g].quantile(q=.5,axis = 1)\n",
    "        # df['g_q75'] = df[features_g].quantile(q=.75,axis = 1)\n",
    "        #df['g_var'] = df[features_g].apply(axis=1,func=stats.variation)\n",
    "        # df['g_mad'] = df[features_g].mad(axis = 1)\n",
    "\n",
    "\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1) ## <==\n",
    "        # df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1) ## <==\n",
    "        # df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        #df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        # df['c_q25'] = df[features_c].quantile(q=.25,axis = 1)\n",
    "        # df['c_q50'] = df[features_c].quantile(q=.5,axis = 1)\n",
    "        # df['c_q75'] = df[features_c].quantile(q=.75,axis = 1)\n",
    "        # df['c_var'] = df[features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['c_mad'] = df[features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1) ## <==\n",
    "        # df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        # df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        # df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        # df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        # df['gc_q25'] = df[features_g + features_c].quantile(q=.25,axis = 1)\n",
    "        # df['gc_q50'] = df[features_g + features_c].quantile(q=.5,axis = 1)\n",
    "        # df['gc_q75'] = df[features_g + features_c].quantile(q=.75,axis = 1)\n",
    "        # df['gc_var'] = df[features_g + features_c].apply(axis=1,func=stats.variation)\n",
    "        # df['gc_mad'] = df[features_g + features_c].mad(axis = 1)\n",
    "\n",
    "\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "train_features,test_features=fe_stats(train_features,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features_gc = train_features[[\"sig_id\"]+GENES+CELLS].copy()\n",
    "# test_features_gc = test_features[[\"sig_id\"]+GENES+CELLS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(21948, 1047)"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "var_thresh = VarianceThreshold(0.8)  #<-- Update\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# def fe_cluster2(train, test, n_clusters = 3, SEED = 42):\n",
    "    \n",
    "\n",
    "#     def create_cluster(train, test, n_clusters = n_clusters):\n",
    "#         train_ = train.copy()\n",
    "#         test_ = test.copy()\n",
    "#         data = pd.concat([train_, test_], axis = 0)\n",
    "#         kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data[[c for c in data.columns if c not in [\"sig_id\",\"cp_type\",\"cp_dose\",\"cp_time\"]]])\n",
    "#         train['cluster'] = kmeans.labels_[:train.shape[0]]\n",
    "#         test['cluster'] = kmeans.labels_[train.shape[0]:]\n",
    "#         train = pd.get_dummies(train, columns = ['cluster'])\n",
    "#         test = pd.get_dummies(test, columns = ['cluster'])\n",
    "#         return train, test\n",
    "    \n",
    "#     train, test = create_cluster(train, test, n_clusters = n_clusters)\n",
    "#     return train, test\n",
    "\n",
    "\n",
    "\n",
    "# train_features,test_features=fe_cluster2(train_features,test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.concat([train1, test1], axis = 0)\n",
    "\n",
    "# distortion = []\n",
    "# for k in range(1,10):\n",
    "#     kmeans = KMeans(n_clusters = k, random_state = 42).fit(data)\n",
    "#     distortion += [kmeans.inertia_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(range(1,10),distortion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = train_features.merge(train_features_gc.loc[:,[col for col in train_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")\n",
    "# test_features = test_features.merge(test_features_gc.loc[:,[col for col in test_features_gc.columns if col not in GENES+CELLS]],on=\"sig_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_features.merge(train_targets_nonscored, on='sig_id')\n",
    "# train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "# test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target_nscored = train[train_targets_nonscored.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test_features.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_nscored_cols = target_nscored.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folds = train.copy()\n",
    "\n",
    "# mskf = MultilabelStratifiedKFold(n_splits=7)\n",
    "\n",
    "# for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target_scored)):\n",
    "#     folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "# folds['kfold'] = folds['kfold'].astype(int)\n",
    "# folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset:\n",
    "    def __init__(self, features, targets_nscored):\n",
    "        self.features = features\n",
    "        self.targets_nscored = targets_nscored\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y_nscored' : torch.tensor(self.targets_nscored[idx, :], dtype=torch.float)           \n",
    "        }\n",
    "        return dct\n",
    "\n",
    "class ValidDataset:\n",
    "    def __init__(self, features, targets_nscored):\n",
    "        self.features = features\n",
    "        self.targets_nscored = targets_nscored\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y_nscored' : torch.tensor(self.targets_nscored[idx, :], dtype=torch.float),          \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y_nscored'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y_nscored'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "    final_loss /= len(dataloader)\n",
    "\n",
    "\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds_nscored = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_nscored = model(inputs)\n",
    "        \n",
    "        preds_nscored.append(outputs_nscored.sigmoid().detach().cpu().numpy())\n",
    "\n",
    "    preds_nscored = np.concatenate(preds_nscored)\n",
    "    \n",
    "    return preds_nscored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, \n",
    "                 num_targets_nscored, \n",
    "                 hidden_sizes,\n",
    "                 dropout_rates):\n",
    "\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_sizes[0]))\n",
    "        self.activation1 = torch.nn.PReLU(num_parameters = hidden_sizes[0], init = 1.0)\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_sizes[0])\n",
    "        self.dropout2 = nn.Dropout(dropout_rates[0])\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_sizes[0], hidden_sizes[1]))\n",
    "        self.activation2 = torch.nn.PReLU(num_parameters = hidden_sizes[1], init = 1.0)\n",
    "\n",
    "\n",
    "        # self.batch_norm2b = nn.BatchNorm1d(hidden_sizes[1])\n",
    "        # self.dropout2b = nn.Dropout(dropout_rates[1])\n",
    "        # self.dense2b = nn.utils.weight_norm(nn.Linear(hidden_sizes[1], hidden_sizes[2]))\n",
    "        # self.activation2b = torch.nn.PReLU(num_parameters = hidden_sizes[2], init = 1.0)\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_sizes[1])\n",
    "        self.dropout3 = nn.Dropout(dropout_rates[1])\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_sizes[1], num_targets_nscored))\n",
    "\n",
    "    def init_bias(self,pos_scored_rate,pos_nscored_rate):\n",
    "        self.dense3.bias.data = nn.Parameter(torch.tensor(pos_scored_rate, dtype=torch.float))\n",
    "        self.dense4.bias.data = nn.Parameter(torch.tensor(pos_nscored_rate, dtype=torch.float))\n",
    "    \n",
    "    def recalibrate_layer(self, layer):\n",
    "        if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "            print ('recalibrate layer.weight_v')\n",
    "            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "        if(torch.isnan(layer.weight).sum() > 0):\n",
    "            print ('recalibrate layer.weight')\n",
    "            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "            layer.weight += 1e-7\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        self.recalibrate_layer(self.dense1)\n",
    "        x = self.activation1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        self.recalibrate_layer(self.dense2)\n",
    "        x = self.activation2(self.dense2(x))\n",
    "\n",
    "        # x = self.batch_norm2b(x)\n",
    "        # x = self.dropout2b(x)\n",
    "        # self.recalibrate_layer(self.dense2b)\n",
    "        # x = self.activation2b(self.dense2b(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        self.recalibrate_layer(self.dense3)\n",
    "        x = self.dense3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 30\n",
    "#EPOCHS = 300 #200\n",
    "PATIENCE=40\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7           \n",
    "EARLY_STOPPING_STEPS = PATIENCE+5\n",
    "EARLY_STOP = False\n",
    "\n",
    "#hidden_size=1500\n",
    "hidden_sizes = [1300,1100]\n",
    "dropout_rates = [0.2619422201258426,0.2619422201258426]\n",
    "#dropout_rate = 0.2619422201258426\n",
    "#dropout_rate = 0.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCATE DRUGS\n",
    "vc = nscored.drug_id.value_counts()\n",
    "vc1 = vc.loc[vc<=18].index.sort_values()\n",
    "vc2 = vc.loc[vc>18].index.sort_values()\n",
    "\n",
    "# STRATIFY DRUGS 18X OR LESS\n",
    "dct1 = {}; dct2 = {}\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = nscored.groupby('drug_id')[targets_nscored].mean().loc[vc1]\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_nscored])):\n",
    "    dd = {k:fold for k in tmp.index[idxV].values}\n",
    "    dct1.update(dd)\n",
    "\n",
    "# STRATIFY DRUGS MORE THAN 18X\n",
    "skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, \n",
    "          random_state=seed)\n",
    "tmp = nscored.loc[nscored.drug_id.isin(vc2)].reset_index(drop=True)\n",
    "for fold,(idxT,idxV) in enumerate( skf.split(tmp,tmp[targets_nscored])):\n",
    "    dd = {k:fold for k in tmp.sig_id[idxV].values}\n",
    "    dct2.update(dd)\n",
    "\n",
    "# ASSIGN FOLDS\n",
    "folds = train.merge(drug,on=\"sig_id\")\n",
    "folds['fold'] = folds.drug_id.map(dct1)\n",
    "folds.loc[folds.fold.isna(),'fold'] =\\\n",
    "    folds.loc[folds.fold.isna(),'sig_id'].map(dct2)\n",
    "folds.fold = folds.fold.astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1048"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "feature_cols = [c for c in process_data(train).columns if c not in (target_nscored_cols)]\n",
    "feature_cols = [c for c in feature_cols if c not in ['fold','sig_id']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=len(feature_cols)\n",
    "num_targets_nscored=len(target_nscored_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(21948, 1448)\n(21948, 1450)\n(3624, 1046)\n(21948, 403)\n(3982, 207)\n"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(folds.shape)\n",
    "print(test.shape)\n",
    "print(target_nscored.shape)\n",
    "print(sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "\n",
    "    \n",
    "    trn_idx = train[train['fold'] != fold].index\n",
    "    val_idx = train[train['fold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['fold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    x_train, y_nscored_train  = train_df[feature_cols].values, train_df[target_nscored_cols].values\n",
    "    x_valid, y_nscored_valid =  valid_df[feature_cols].values, valid_df[target_nscored_cols].values\n",
    "    \n",
    "    train_dataset = TrainDataset(x_train, y_nscored_train)\n",
    "    valid_dataset = ValidDataset(x_valid, y_nscored_valid)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets_nscored=num_targets_nscored,\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates,\n",
    "    )\n",
    "\n",
    "    # model.init_bias(pos_scored_rate,pos_nscored_rate)\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,pct_start=0.1, div_factor=1e4, final_div_factor=1e5,\n",
    "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "\n",
    "\n",
    "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode = \"min\", patience = PATIENCE, min_lr = 1e-6, factor = 0.9)\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "   \n",
    "    oof = np.zeros((len(train), target_nscored.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
    "        #scheduler.step(valid_loss)\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"FOLD{fold}_m1_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets_nscored=num_targets_nscored,\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout_rates=dropout_rates\n",
    "    )\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"FOLD{fold}_m1_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions_nscored = np.zeros((len(test_), target_nscored.iloc[:, 1:].shape[1]))\n",
    "    predictions_nscored = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions_nscored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed):\n",
    "    oof = np.zeros((len(train), len(target_nscored_cols)))\n",
    "    predictions_nscored = np.zeros((len(test), len(target_nscored_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_nscored_ = run_training(fold, seed)\n",
    "        \n",
    "        predictions_nscored += pred_nscored_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions_nscored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "FOLD: 0, EPOCH: 0, train_loss: 0.5416978941533436, valid_loss: 0.027790090590715407\nFOLD: 0, EPOCH: 1, train_loss: 0.011876989916271093, valid_loss: 0.005193247105926275\nFOLD: 0, EPOCH: 2, train_loss: 0.008628546276136118, valid_loss: 0.005190843120217324\nFOLD: 0, EPOCH: 3, train_loss: 0.008594813769315781, valid_loss: 0.007981017380952835\nFOLD: 0, EPOCH: 4, train_loss: 0.008569205667646159, valid_loss: 0.005116049442440271\nFOLD: 0, EPOCH: 5, train_loss: 0.008293163623711486, valid_loss: 0.004582149498164653\nFOLD: 0, EPOCH: 6, train_loss: 0.00826709768513129, valid_loss: 0.00493538349866867\nFOLD: 0, EPOCH: 7, train_loss: 0.008251061194202526, valid_loss: 0.004808509293943644\nFOLD: 0, EPOCH: 8, train_loss: 0.008246377639497827, valid_loss: 0.004856785433366895\nFOLD: 0, EPOCH: 9, train_loss: 0.008255078362262979, valid_loss: 0.004853619318455457\nFOLD: 0, EPOCH: 10, train_loss: 0.008262035620658576, valid_loss: 0.004837706182152033\nFOLD: 0, EPOCH: 11, train_loss: 0.008271684432972451, valid_loss: 0.005048463251441717\nFOLD: 0, EPOCH: 12, train_loss: 0.00828407287318893, valid_loss: 0.004803845891728997\nFOLD: 0, EPOCH: 13, train_loss: 0.008279239226664816, valid_loss: 0.004964429447427392\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 0, EPOCH: 14, train_loss: 0.008280200810887579, valid_loss: 0.004874607818201184\nFOLD: 0, EPOCH: 15, train_loss: 0.008255463701729871, valid_loss: 0.004932753508910537\nFOLD: 0, EPOCH: 16, train_loss: 0.008247094963784932, valid_loss: 0.004967666110023856\nFOLD: 0, EPOCH: 17, train_loss: 0.008236116552281948, valid_loss: 0.004832656960934401\nFOLD: 0, EPOCH: 18, train_loss: 0.008216372425002711, valid_loss: 0.004833611324429512\nFOLD: 0, EPOCH: 19, train_loss: 0.008204076397961297, valid_loss: 0.004930876567959786\nFOLD: 0, EPOCH: 20, train_loss: 0.008208116025467511, valid_loss: 0.004810046581551433\nFOLD: 0, EPOCH: 21, train_loss: 0.00818794737031468, valid_loss: 0.004894826747477055\nFOLD: 0, EPOCH: 22, train_loss: 0.008167397816899884, valid_loss: 0.004844926996156573\nFOLD: 0, EPOCH: 23, train_loss: 0.008159278597378609, valid_loss: 0.0048063610587269065\nFOLD: 0, EPOCH: 24, train_loss: 0.008141063280454297, valid_loss: 0.004816593760624528\nFOLD: 0, EPOCH: 25, train_loss: 0.008112393718745028, valid_loss: 0.004820969793945551\nFOLD: 0, EPOCH: 26, train_loss: 0.008103320283219724, valid_loss: 0.004805603548884391\nFOLD: 0, EPOCH: 27, train_loss: 0.008085365122070119, valid_loss: 0.004787137163802981\nFOLD: 0, EPOCH: 28, train_loss: 0.008067572920196721, valid_loss: 0.00478247381746769\nFOLD: 0, EPOCH: 29, train_loss: 0.008056362546017382, valid_loss: 0.004781956123188138\nFOLD: 1, EPOCH: 0, train_loss: 0.5425295220802024, valid_loss: 0.031040075048804284\nFOLD: 1, EPOCH: 1, train_loss: 0.012223931073787667, valid_loss: 0.00513399638235569\nFOLD: 1, EPOCH: 2, train_loss: 0.00859734391616214, valid_loss: 0.004889863766729832\nFOLD: 1, EPOCH: 3, train_loss: 0.008443754898118121, valid_loss: 0.004887660564854741\nFOLD: 1, EPOCH: 4, train_loss: 0.008407235775953856, valid_loss: 0.004822550257667899\nFOLD: 1, EPOCH: 5, train_loss: 0.00827806606432613, valid_loss: 0.004998422395437956\nFOLD: 1, EPOCH: 6, train_loss: 0.008254441540358828, valid_loss: 0.0048607415053993465\nFOLD: 1, EPOCH: 7, train_loss: 0.008247291032528999, valid_loss: 0.004877146715298295\nFOLD: 1, EPOCH: 8, train_loss: 0.008250934122523078, valid_loss: 0.0050317641999572515\nFOLD: 1, EPOCH: 9, train_loss: 0.008244951816313729, valid_loss: 0.005098326802253723\nFOLD: 1, EPOCH: 10, train_loss: 0.0082688612077816, valid_loss: 0.004840960875153542\nFOLD: 1, EPOCH: 11, train_loss: 0.008270946191940583, valid_loss: 0.004976696735247969\nFOLD: 1, EPOCH: 12, train_loss: 0.008275086624028326, valid_loss: 0.004960013581439853\nFOLD: 1, EPOCH: 13, train_loss: 0.008275999564069266, valid_loss: 0.004872661968693137\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 1, EPOCH: 14, train_loss: 0.008267864103422683, valid_loss: 0.004920927919447422\nFOLD: 1, EPOCH: 15, train_loss: 0.008249567862802825, valid_loss: 0.004890734693035483\nFOLD: 1, EPOCH: 16, train_loss: 0.008238909458804901, valid_loss: 0.004886652724817395\nFOLD: 1, EPOCH: 17, train_loss: 0.0082342658115893, valid_loss: 0.004917505895718932\nFOLD: 1, EPOCH: 18, train_loss: 0.008225841941882153, valid_loss: 0.0049986068345606325\nFOLD: 1, EPOCH: 19, train_loss: 0.008201477292063488, valid_loss: 0.004933186220005155\nFOLD: 1, EPOCH: 20, train_loss: 0.008192816213863034, valid_loss: 0.004927838807925582\nFOLD: 1, EPOCH: 21, train_loss: 0.00817719595443432, valid_loss: 0.004883997058495879\nFOLD: 1, EPOCH: 22, train_loss: 0.008165979783247117, valid_loss: 0.00492451561614871\nFOLD: 1, EPOCH: 23, train_loss: 0.008148852906817076, valid_loss: 0.004897034838795662\nFOLD: 1, EPOCH: 24, train_loss: 0.008130696006626094, valid_loss: 0.004859914416447282\nFOLD: 1, EPOCH: 25, train_loss: 0.008113776957679565, valid_loss: 0.0048831233009696004\nFOLD: 1, EPOCH: 26, train_loss: 0.008098403156614628, valid_loss: 0.004848160455003381\nFOLD: 1, EPOCH: 27, train_loss: 0.008083054441071692, valid_loss: 0.004834905378520489\nFOLD: 1, EPOCH: 28, train_loss: 0.008063184043557263, valid_loss: 0.004828055128455162\nFOLD: 1, EPOCH: 29, train_loss: 0.008057090447468012, valid_loss: 0.004828524738550186\nFOLD: 2, EPOCH: 0, train_loss: 0.5406305048916791, valid_loss: 0.028168813213706016\nFOLD: 2, EPOCH: 1, train_loss: 0.011760887409862433, valid_loss: 0.005281800143420696\nFOLD: 2, EPOCH: 2, train_loss: 0.009276724234852637, valid_loss: 0.004794057840481401\nFOLD: 2, EPOCH: 3, train_loss: 0.0084089398591754, valid_loss: 0.004835469126701355\nFOLD: 2, EPOCH: 4, train_loss: 0.008346070035209728, valid_loss: 0.004879160206764936\nFOLD: 2, EPOCH: 5, train_loss: 0.0082953929857074, valid_loss: 0.005126967895776034\nFOLD: 2, EPOCH: 6, train_loss: 0.00824485219876609, valid_loss: 0.00505311113782227\nFOLD: 2, EPOCH: 7, train_loss: 0.008244450540734909, valid_loss: 0.004910290353000164\nFOLD: 2, EPOCH: 8, train_loss: 0.008231554529629648, valid_loss: 0.005012295246124268\nFOLD: 2, EPOCH: 9, train_loss: 0.008235741608046196, valid_loss: 0.004959897501394152\nFOLD: 2, EPOCH: 10, train_loss: 0.008249678230542387, valid_loss: 0.004917489718645811\nFOLD: 2, EPOCH: 11, train_loss: 0.008244720894868511, valid_loss: 0.005086200293153525\nFOLD: 2, EPOCH: 12, train_loss: 0.008265787606580637, valid_loss: 0.005024088686332107\nFOLD: 2, EPOCH: 13, train_loss: 0.008254986740900454, valid_loss: 0.004935340341180563\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 14, train_loss: 0.008252247979210035, valid_loss: 0.004979079877957701\nFOLD: 2, EPOCH: 15, train_loss: 0.008237890700960683, valid_loss: 0.005000030063092709\nFOLD: 2, EPOCH: 16, train_loss: 0.008226792570329397, valid_loss: 0.004906578902155161\nrecalibrate layer.weight_v\nFOLD: 2, EPOCH: 17, train_loss: 0.008215118860322479, valid_loss: 0.005018720375373959\nFOLD: 2, EPOCH: 18, train_loss: 0.008192873471794097, valid_loss: 0.0049743516277521846\nFOLD: 2, EPOCH: 19, train_loss: 0.008179614030932253, valid_loss: 0.004919137414544821\nFOLD: 2, EPOCH: 20, train_loss: 0.00817163146377818, valid_loss: 0.00495041674003005\nFOLD: 2, EPOCH: 21, train_loss: 0.008148903162470338, valid_loss: 0.004970392659306527\nFOLD: 2, EPOCH: 22, train_loss: 0.008132262001241985, valid_loss: 0.004928171131759882\nFOLD: 2, EPOCH: 23, train_loss: 0.008117338488306347, valid_loss: 0.004949126131832599\nFOLD: 2, EPOCH: 24, train_loss: 0.008099563020231152, valid_loss: 0.0049062847066670655\nFOLD: 2, EPOCH: 25, train_loss: 0.008083720571940413, valid_loss: 0.00490173801779747\nFOLD: 2, EPOCH: 26, train_loss: 0.008063375166415967, valid_loss: 0.00489690407179296\nFOLD: 2, EPOCH: 27, train_loss: 0.008037148754590669, valid_loss: 0.004871007138863206\nFOLD: 2, EPOCH: 28, train_loss: 0.008021430445935679, valid_loss: 0.004869133234024048\nFOLD: 2, EPOCH: 29, train_loss: 0.008023436414077878, valid_loss: 0.004867480145767331\nFOLD: 3, EPOCH: 0, train_loss: 0.5403639358930847, valid_loss: 0.028294947743415833\nFOLD: 3, EPOCH: 1, train_loss: 0.011747787592514438, valid_loss: 0.005248693823814392\nFOLD: 3, EPOCH: 2, train_loss: 0.008620182197971815, valid_loss: 0.005540209002792835\nFOLD: 3, EPOCH: 3, train_loss: 0.008486312098142242, valid_loss: 0.004732160586863756\nFOLD: 3, EPOCH: 4, train_loss: 0.008336499653325802, valid_loss: 0.005057144984602928\nFOLD: 3, EPOCH: 5, train_loss: 0.00829703998783616, valid_loss: 0.004642223287373781\nFOLD: 3, EPOCH: 6, train_loss: 0.008262212245668076, valid_loss: 0.004744772994890809\nFOLD: 3, EPOCH: 7, train_loss: 0.00826110790933476, valid_loss: 0.0046993015613406896\nFOLD: 3, EPOCH: 8, train_loss: 0.008256541131076966, valid_loss: 0.004639667123556137\nFOLD: 3, EPOCH: 9, train_loss: 0.008272609695614804, valid_loss: 0.0047101553250104185\nFOLD: 3, EPOCH: 10, train_loss: 0.008283545545042575, valid_loss: 0.004811470936983824\nFOLD: 3, EPOCH: 11, train_loss: 0.008293392117686418, valid_loss: 0.004825997287407517\nFOLD: 3, EPOCH: 12, train_loss: 0.008300329818307948, valid_loss: 0.004820509515702724\nFOLD: 3, EPOCH: 13, train_loss: 0.008295493127562765, valid_loss: 0.004858137331902981\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 14, train_loss: 0.008297540085884382, valid_loss: 0.004903756817802787\nrecalibrate layer.weight_v\nFOLD: 3, EPOCH: 15, train_loss: 0.008275568177986916, valid_loss: 0.004841970046982169\nFOLD: 3, EPOCH: 16, train_loss: 0.008259792175448063, valid_loss: 0.004793094526976347\nFOLD: 3, EPOCH: 17, train_loss: 0.008256098413903292, valid_loss: 0.004957484509795904\nFOLD: 3, EPOCH: 18, train_loss: 0.00825221933500499, valid_loss: 0.004928525313735008\nFOLD: 3, EPOCH: 19, train_loss: 0.008235005665012972, valid_loss: 0.0048439527582377195\nFOLD: 3, EPOCH: 20, train_loss: 0.008219871654168886, valid_loss: 0.004837835812941194\nFOLD: 3, EPOCH: 21, train_loss: 0.008212653647626744, valid_loss: 0.004898618776351214\nFOLD: 3, EPOCH: 22, train_loss: 0.008203996272542242, valid_loss: 0.004856094084680081\nFOLD: 3, EPOCH: 23, train_loss: 0.008179135579413094, valid_loss: 0.004841576553881169\nFOLD: 3, EPOCH: 24, train_loss: 0.008165141241308176, valid_loss: 0.004788611959666014\nFOLD: 3, EPOCH: 25, train_loss: 0.008149294512105637, valid_loss: 0.004769911020994186\nFOLD: 3, EPOCH: 26, train_loss: 0.008131282746183629, valid_loss: 0.004744424354285002\nFOLD: 3, EPOCH: 27, train_loss: 0.008113859148480658, valid_loss: 0.004731848239898682\nFOLD: 3, EPOCH: 28, train_loss: 0.008105042583758937, valid_loss: 0.004761140616610646\nFOLD: 3, EPOCH: 29, train_loss: 0.008093703145376678, valid_loss: 0.0047364105936139825\nFOLD: 4, EPOCH: 0, train_loss: 0.5407049356632538, valid_loss: 0.032494287860269346\nFOLD: 4, EPOCH: 1, train_loss: 0.011767991406314477, valid_loss: 0.0057211780998234945\nFOLD: 4, EPOCH: 2, train_loss: 0.008657613929637984, valid_loss: 0.005323696318858613\nFOLD: 4, EPOCH: 3, train_loss: 0.008407931608097578, valid_loss: 0.00758989496777455\nFOLD: 4, EPOCH: 4, train_loss: 0.008406425165515896, valid_loss: 0.005315177297840516\nFOLD: 4, EPOCH: 5, train_loss: 0.008255387920684911, valid_loss: 0.0053919497489308315\nFOLD: 4, EPOCH: 6, train_loss: 0.00821656419115292, valid_loss: 0.005236485827481374\nFOLD: 4, EPOCH: 7, train_loss: 0.008201741660643067, valid_loss: 0.005178170171954359\nFOLD: 4, EPOCH: 8, train_loss: 0.008197233744745923, valid_loss: 0.005142706407544513\nFOLD: 4, EPOCH: 9, train_loss: 0.008200355849505679, valid_loss: 0.005387054833893974\nFOLD: 4, EPOCH: 10, train_loss: 0.008201907782160954, valid_loss: 0.005335051071597263\nFOLD: 4, EPOCH: 11, train_loss: 0.00821761752957025, valid_loss: 0.005270223307888955\nFOLD: 4, EPOCH: 12, train_loss: 0.00821882242815116, valid_loss: 0.005236743454588577\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 13, train_loss: 0.008222470620042971, valid_loss: 0.0052021256609198945\nrecalibrate layer.weight_v\nFOLD: 4, EPOCH: 14, train_loss: 0.00821275487416298, valid_loss: 0.005559830654722949\nFOLD: 4, EPOCH: 15, train_loss: 0.008202088285690627, valid_loss: 0.005328648413221042\nFOLD: 4, EPOCH: 16, train_loss: 0.008197102859314229, valid_loss: 0.005275728239212185\nFOLD: 4, EPOCH: 17, train_loss: 0.008184224021042118, valid_loss: 0.005316506537686412\nFOLD: 4, EPOCH: 18, train_loss: 0.008176560642675974, valid_loss: 0.005325047842537363\nFOLD: 4, EPOCH: 19, train_loss: 0.00817315378918539, valid_loss: 0.005324903235305101\nFOLD: 4, EPOCH: 20, train_loss: 0.008157537661719363, valid_loss: 0.005254778603557497\nFOLD: 4, EPOCH: 21, train_loss: 0.008136198566471404, valid_loss: 0.005258544105648373\nFOLD: 4, EPOCH: 22, train_loss: 0.008128075095875239, valid_loss: 0.005307044989118974\nFOLD: 4, EPOCH: 23, train_loss: 0.0081098520989857, valid_loss: 0.005169658853750055\nFOLD: 4, EPOCH: 24, train_loss: 0.008087830898049916, valid_loss: 0.005254146264633164\nFOLD: 4, EPOCH: 25, train_loss: 0.008079288458149578, valid_loss: 0.00522773639143755\nFOLD: 4, EPOCH: 26, train_loss: 0.00804783550808458, valid_loss: 0.005215072102146223\nFOLD: 4, EPOCH: 27, train_loss: 0.008033918465386975, valid_loss: 0.005216107752251749\nFOLD: 4, EPOCH: 28, train_loss: 0.008019240333925228, valid_loss: 0.005202364457848792\nFOLD: 4, EPOCH: 29, train_loss: 0.00801308618581577, valid_loss: 0.0052057659777347\nFOLD: 5, EPOCH: 0, train_loss: 0.5439489563407541, valid_loss: 0.029681644514203073\nFOLD: 5, EPOCH: 1, train_loss: 0.011877903632414179, valid_loss: 0.005371586047112941\nFOLD: 5, EPOCH: 2, train_loss: 0.008597034184883038, valid_loss: 0.005739984940737486\nFOLD: 5, EPOCH: 3, train_loss: 0.008505835996142455, valid_loss: 0.004788176231086254\nFOLD: 5, EPOCH: 4, train_loss: 0.00837709629895533, valid_loss: 0.004733278583735227\nFOLD: 5, EPOCH: 5, train_loss: 0.008313185414996278, valid_loss: 0.004958254089578986\nFOLD: 5, EPOCH: 6, train_loss: 0.008270611837633005, valid_loss: 0.004761496735736728\nFOLD: 5, EPOCH: 7, train_loss: 0.00825962172124256, valid_loss: 0.004970774315297604\nFOLD: 5, EPOCH: 8, train_loss: 0.008259684344766293, valid_loss: 0.004984959289431572\nFOLD: 5, EPOCH: 9, train_loss: 0.008256197704293696, valid_loss: 0.00503695952706039\nFOLD: 5, EPOCH: 10, train_loss: 0.008259290636067285, valid_loss: 0.004926351513713598\nFOLD: 5, EPOCH: 11, train_loss: 0.008263649308078345, valid_loss: 0.004899479495361447\nFOLD: 5, EPOCH: 12, train_loss: 0.00827126186277692, valid_loss: 0.004940948216244578\nFOLD: 5, EPOCH: 13, train_loss: 0.008274670028235434, valid_loss: 0.00501456031575799\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 5, EPOCH: 14, train_loss: 0.008263242488004724, valid_loss: 0.005038473000749946\nFOLD: 5, EPOCH: 15, train_loss: 0.008248569206239618, valid_loss: 0.00498813534155488\nFOLD: 5, EPOCH: 16, train_loss: 0.00823429390965473, valid_loss: 0.004898296128958464\nFOLD: 5, EPOCH: 17, train_loss: 0.008225158300428163, valid_loss: 0.004853014703840018\nFOLD: 5, EPOCH: 18, train_loss: 0.008204533778713876, valid_loss: 0.00488396255299449\nFOLD: 5, EPOCH: 19, train_loss: 0.008190836193559526, valid_loss: 0.004871007455512881\nFOLD: 5, EPOCH: 20, train_loss: 0.008177781727823878, valid_loss: 0.004849761603400111\nFOLD: 5, EPOCH: 21, train_loss: 0.00816768679299018, valid_loss: 0.004917775988578796\nFOLD: 5, EPOCH: 22, train_loss: 0.008151547365574812, valid_loss: 0.0048990412708371876\nFOLD: 5, EPOCH: 23, train_loss: 0.008132393874202658, valid_loss: 0.004847538899630308\nFOLD: 5, EPOCH: 24, train_loss: 0.008115912692583337, valid_loss: 0.0048307126294821505\nFOLD: 5, EPOCH: 25, train_loss: 0.008097108719604356, valid_loss: 0.0048415649589151144\nFOLD: 5, EPOCH: 26, train_loss: 0.008076177399960301, valid_loss: 0.004820057963952422\nFOLD: 5, EPOCH: 27, train_loss: 0.008060451416412786, valid_loss: 0.004818538632243872\nFOLD: 5, EPOCH: 28, train_loss: 0.00803995790707619, valid_loss: 0.004808317311108113\nFOLD: 5, EPOCH: 29, train_loss: 0.008029156738296657, valid_loss: 0.004809346497058869\nFOLD: 6, EPOCH: 0, train_loss: 0.5429933389433387, valid_loss: 0.027859529107809068\nFOLD: 6, EPOCH: 1, train_loss: 0.011720995333197773, valid_loss: 0.005376412402838469\nFOLD: 6, EPOCH: 2, train_loss: 0.00866343590690672, valid_loss: 0.004932644385844469\nFOLD: 6, EPOCH: 3, train_loss: 0.008489012958950737, valid_loss: 0.004965862836688757\nFOLD: 6, EPOCH: 4, train_loss: 0.008441828346500794, valid_loss: 0.004699226589873433\nFOLD: 6, EPOCH: 5, train_loss: 0.008315563214575352, valid_loss: 0.004728196850046515\nFOLD: 6, EPOCH: 6, train_loss: 0.008269368911630848, valid_loss: 0.004837258346378803\nFOLD: 6, EPOCH: 7, train_loss: 0.008250091199566718, valid_loss: 0.0048035942576825615\nFOLD: 6, EPOCH: 8, train_loss: 0.00824623532770645, valid_loss: 0.005025093201547861\nFOLD: 6, EPOCH: 9, train_loss: 0.008245363036411352, valid_loss: 0.004826081143692136\nFOLD: 6, EPOCH: 10, train_loss: 0.008252205903686229, valid_loss: 0.004901078380644321\nFOLD: 6, EPOCH: 11, train_loss: 0.008258709953274249, valid_loss: 0.004936115071177483\nFOLD: 6, EPOCH: 12, train_loss: 0.008267522048281163, valid_loss: 0.0048879551701247696\nFOLD: 6, EPOCH: 13, train_loss: 0.008271244291190792, valid_loss: 0.005077371299266815\nrecalibrate layer.weight_v\nrecalibrate layer.weight_v\nFOLD: 6, EPOCH: 14, train_loss: 0.008267387814287628, valid_loss: 0.004951534550637007\nFOLD: 6, EPOCH: 15, train_loss: 0.008254788353481667, valid_loss: 0.004884509835392237\nFOLD: 6, EPOCH: 16, train_loss: 0.008240383935674113, valid_loss: 0.00488257447257638\nFOLD: 6, EPOCH: 17, train_loss: 0.008226883968300357, valid_loss: 0.004861070346087217\nFOLD: 6, EPOCH: 18, train_loss: 0.008222821089407417, valid_loss: 0.005059397015720606\nFOLD: 6, EPOCH: 19, train_loss: 0.008208367450885018, valid_loss: 0.004942795094102621\nFOLD: 6, EPOCH: 20, train_loss: 0.00819387082580705, valid_loss: 0.004893600381910801\nFOLD: 6, EPOCH: 21, train_loss: 0.00817876022790565, valid_loss: 0.004870489481836557\nFOLD: 6, EPOCH: 22, train_loss: 0.00817238307893783, valid_loss: 0.004887667037546634\nFOLD: 6, EPOCH: 23, train_loss: 0.008151446836588739, valid_loss: 0.004861689116805792\nFOLD: 6, EPOCH: 24, train_loss: 0.008138657740767108, valid_loss: 0.0048688074946403505\nFOLD: 6, EPOCH: 25, train_loss: 0.008121275241632446, valid_loss: 0.004831136371940374\nFOLD: 6, EPOCH: 26, train_loss: 0.008100258477893816, valid_loss: 0.004840311985462904\nFOLD: 6, EPOCH: 27, train_loss: 0.008080980673964535, valid_loss: 0.0048207339271903035\nFOLD: 6, EPOCH: 28, train_loss: 0.008070190449809136, valid_loss: 0.0048244459182024005\nFOLD: 6, EPOCH: 29, train_loss: 0.008066062929526883, valid_loss: 0.004819035008549691\n"
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "#SEED = [0,1,2,3,4,5,6] #<-- Update\n",
    "SEED = [0]\n",
    "oof = np.zeros((len(train), len(target_nscored_cols)))\n",
    "predictions_nscored = np.zeros((len(test), len(target_nscored_cols)))\n",
    "\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_nscored_ = run_k_fold(NFOLDS, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions_nscored += predictions_nscored_ / len(SEED)\n",
    "\n",
    "train[target_nscored_cols] = oof\n",
    "test[target_nscored_cols] = predictions_nscored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV log_loss:  0.0047653734598364535\n"
    }
   ],
   "source": [
    "valid_results = train_targets_nonscored.drop(columns=target_nscored_cols).merge(train[['sig_id']+target_nscored_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "\n",
    "y_true = train_targets_nonscored[target_nscored_cols].values\n",
    "y_pred = valid_results[target_nscored_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_nscored_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i],labels=[0,1])\n",
    "    score += score_ / target_nscored.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV log_loss:  0.004804078075531289\n",
    "#CV log_loss:  0.004795348601078446\n",
    "#CV log_loss:  0.0047653734598364535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_nscored = sample_submission[[\"sig_id\"]].merge(test[train_targets_nonscored.columns], on='sig_id', how='left').fillna(0)\n",
    "sub_nscored.to_csv('sub_nscored.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}