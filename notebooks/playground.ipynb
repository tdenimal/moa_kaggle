{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1602540156760",
   "display_name": "Python 3.7.9 64-bit ('moa_kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/01_raw/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../data/01_raw/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../data/01_raw/train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv('../data/01_raw/test_features.csv')\n",
    "submission = pd.read_csv('../data/01_raw/sample_submission.csv')\n",
    "\n",
    "remove_vehicle = True\n",
    "\n",
    "if remove_vehicle:\n",
    "    train_features = train.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
    "    train_targets_scored = train_targets_scored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
    "    train_targets_nonscored = train_targets_nonscored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n",
    "else:\n",
    "    train_features = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "c_cols = [\"c-\"+str(c) for c in range(0,99+1)]\n",
    "\n",
    "g_cols = [\"g-\"+str(c) for c in range(0,771+1)]\n",
    "\n",
    "\n",
    "\n",
    "full_df_c = pd.concat([train_features[c_cols],test_features[c_cols]]).reset_index(drop=True)\n",
    "full_df_g = pd.concat([train_features[g_cols],test_features[g_cols]]).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c_thresh = 125\n",
    "g_thresh = 500\n",
    "\n",
    "\n",
    "clus_c = AgglomerativeClustering(distance_threshold=c_thresh,\n",
    "                               n_clusters=None,\n",
    "                               affinity='euclidean',\n",
    "                               linkage=\"ward\").fit(full_df_c)\n",
    "\n",
    "\n",
    "clus_g = AgglomerativeClustering(distance_threshold=g_thresh,\n",
    "                               n_clusters=None,\n",
    "                               affinity='euclidean',\n",
    "                               linkage=\"ward\").fit(full_df_g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_features[\"clus_c\"] = clus_c.labels_[train_features.index]\n",
    "train_features[\"clus_g\"] = clus_g.labels_[train_features.index]\n",
    "\n",
    "\n",
    "test_features[\"clus_c\"] = clus_c.labels_[test_features.index+train_features.shape[0]]\n",
    "test_features[\"clus_g\"] = clus_g.labels_[test_features.index+train_features.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'clus_c' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-376290b627ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"NBre of c clusters : {clus_c.n_clusters_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"NBre of g clusters : {clus_g.n_clusters_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clus_c' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"NBre of c clusters : {clus_c.n_clusters_}\")\n",
    "print(f\"NBre of g clusters : {clus_g.n_clusters_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "remove 0 columns\nremove 71 columns\n"
    }
   ],
   "source": [
    "# ratio for each label\n",
    "\n",
    "def get_ratio_labels(df):\n",
    "    columns = list(df.columns)\n",
    "    columns.pop(0)\n",
    "    ratios = []\n",
    "    toremove = []\n",
    "    for c in columns:\n",
    "        counts = df[c].value_counts()\n",
    "        if len(counts) != 1:\n",
    "            ratios.append(counts[0]/counts[1])\n",
    "        else:\n",
    "            toremove.append(c)\n",
    "    print(f\"remove {len(toremove)} columns\")\n",
    "    \n",
    "    for t in toremove:\n",
    "        columns.remove(t)\n",
    "    return columns, np.array(ratios).astype(np.int32)\n",
    "\n",
    "columns, ratios = get_ratio_labels(train_targets_scored)\n",
    "columns_nonscored, ratios_nonscored = get_ratio_labels(train_targets_nonscored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(train, test, col, normalize=True, removed_vehicle=False):\n",
    "    \"\"\"\n",
    "        the first 3 columns represents categories, the others numericals features\n",
    "    \"\"\"\n",
    "    mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n",
    "               \"cp_time\":{48:0, 72:1, 24:2},\n",
    "               \"cp_dose\":{\"D1\":0, \"D2\":1}}\n",
    "    \n",
    "    if removed_vehicle:\n",
    "        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n",
    "        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n",
    "    else:\n",
    "        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n",
    "        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n",
    "    \n",
    "    max_ = 10.\n",
    "    min_ = -10.\n",
    "   \n",
    "    if removed_vehicle:\n",
    "        numerical_tr = train[col[3:]].values\n",
    "        numerical_test = test[col[3:]].values\n",
    "    else:\n",
    "        numerical_tr = train[col[3:]].values\n",
    "        numerical_test = test[col[3:]].values\n",
    "    if normalize:\n",
    "        numerical_tr = (numerical_tr-min_)/(max_ - min_)\n",
    "        numerical_test = (numerical_test-min_)/(max_ - min_)\n",
    "    return categories_tr, categories_test, numerical_tr, numerical_test\n",
    "\n",
    "\n",
    "    \n",
    "col_features = list(train_features.columns)[1:]\n",
    "cat_tr, cat_test, numerical_tr, numerical_test = transform_data(train_features, test_features, col_features, normalize=False, removed_vehicle=remove_vehicle)\n",
    "targets_tr = train_targets_scored[columns].values.astype(np.float32)\n",
    "targets2_tr = train_targets_nonscored[columns_nonscored].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evals(model, X, y, verbose=True):\n",
    "    with torch.no_grad():\n",
    "        y_preds = model.predict(X)\n",
    "        y_preds = torch.clamp(y_preds, 0.0,1.0).detach().numpy()\n",
    "    score = log_loss_multi(y, y_preds)\n",
    "    #print(\"Logloss = \", score)\n",
    "    return y_preds, score\n",
    "\n",
    "\n",
    "def inference_fn(model, X ,verbose=True):\n",
    "    with torch.no_grad():\n",
    "        y_preds = model.predict( X )\n",
    "        y_preds = torch.sigmoid(torch.as_tensor(y_preds)).numpy()\n",
    "    return y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_score(actual, predicted,  eps=1e-15):\n",
    "\n",
    "        \"\"\"\n",
    "        :param predicted:   The predicted probabilities as floats between 0-1\n",
    "        :param actual:      The binary labels. Either 0 or 1.\n",
    "        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n",
    "        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        p1 = actual * np.log(predicted+eps)\n",
    "        p0 = (1-actual) * np.log(1-predicted+eps)\n",
    "        loss = p0 + p1\n",
    "\n",
    "        return -loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_targets(targets):\n",
    "    ### check if targets are all binary in training set\n",
    "    \n",
    "    for i in range(targets.shape[1]):\n",
    "        if len(np.unique(targets[:,i])) != 2:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        try:\n",
    "            results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n",
    "        except:\n",
    "            pass\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABNET\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "from pytorch_tabnet import tab_network\n",
    "from pytorch_tabnet.multiclass_utils import unique_labels\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from pytorch_tabnet.utils import (PredictDataset,\n",
    "                                  create_dataloaders,\n",
    "                                  create_explain_matrix)\n",
    "from sklearn.base import BaseEstimator\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import io\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TabModel(BaseEstimator):\n",
    "    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n",
    "                 lambda_sparse=1e-3, seed=0,\n",
    "                 clip_value=1, verbose=1,\n",
    "                 optimizer_fn=torch.optim.Adam,\n",
    "                 optimizer_params=dict(lr=2e-2),\n",
    "                 scheduler_params=None, scheduler_fn=None,\n",
    "                 mask_type=\"sparsemax\",\n",
    "                 input_dim=None, output_dim=None,\n",
    "                 device_name='auto'):\n",
    "        \"\"\" Class for TabNet model\n",
    "        Parameters\n",
    "        ----------\n",
    "            device_name: str\n",
    "                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.cat_idxs = cat_idxs\n",
    "        self.cat_dims = cat_dims\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.clip_value = clip_value\n",
    "        self.verbose = verbose\n",
    "        self.optimizer_fn = optimizer_fn\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.device_name = device_name\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "        self.mask_type = mask_type\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        #self.batch_size = 1024\n",
    "        self.batch_size = 2048\n",
    "\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        # Defining device\n",
    "        if device_name == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device_name = 'cuda'\n",
    "            else:\n",
    "                device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        print(f\"Device used : {self.device}\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n",
    "                          weights, batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define construct_loaders to use this base class')\n",
    "\n",
    "    def init_network(\n",
    "                     self,\n",
    "                     input_dim,\n",
    "                     output_dim,\n",
    "                     n_d,\n",
    "                     n_a,\n",
    "                     n_steps,\n",
    "                     gamma,\n",
    "                     cat_idxs,\n",
    "                     cat_dims,\n",
    "                     cat_emb_dim,\n",
    "                     n_independent,\n",
    "                     n_shared,\n",
    "                     epsilon,\n",
    "                     virtual_batch_size,\n",
    "                     momentum,\n",
    "                     device_name,\n",
    "                     mask_type,\n",
    "                     ):\n",
    "        self.network = tab_network.TabNet(\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            n_d=n_d,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=cat_emb_dim,\n",
    "            n_independent=n_independent,\n",
    "            n_shared=n_shared,\n",
    "            epsilon=epsilon,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            momentum=momentum,\n",
    "            device_name=device_name,\n",
    "            mask_type=mask_type).to(self.device)\n",
    "\n",
    "        self.reducing_matrix = create_explain_matrix(\n",
    "            self.network.input_dim,\n",
    "            self.network.cat_emb_dim,\n",
    "            self.network.cat_idxs,\n",
    "            self.network.post_embed_dim)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n",
    "            weights=0, max_epochs=100, patience=10, batch_size=1024,\n",
    "            virtual_batch_size=128, num_workers=0, drop_last=False):\n",
    "        \"\"\"Train a neural network stored in self.network\n",
    "        Using train_dataloader for training data and\n",
    "        valid_dataloader for validation.\n",
    "        Parameters\n",
    "        ----------\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            weights : bool or dictionnary\n",
    "                0 for no balancing\n",
    "                1 for automated balancing\n",
    "                dict for custom weights per class\n",
    "            max_epochs : int\n",
    "                Maximum number of epochs during training\n",
    "            patience : int\n",
    "                Number of consecutive non improving epoch before early stopping\n",
    "            batch_size : int\n",
    "                Training batch size\n",
    "            virtual_batch_size : int\n",
    "                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n",
    "            num_workers : int\n",
    "                Number of workers used in torch.utils.data.DataLoader\n",
    "            drop_last : bool\n",
    "                Whether to drop last batch during training\n",
    "        \"\"\"\n",
    "        # update model name\n",
    "\n",
    "        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n",
    "                               weights, max_epochs, patience, batch_size,\n",
    "                               virtual_batch_size, num_workers, drop_last)\n",
    "\n",
    "        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n",
    "                                                                    y_train,\n",
    "                                                                    X_valid,\n",
    "                                                                    y_valid,\n",
    "                                                                    self.updated_weights,\n",
    "                                                                    self.batch_size,\n",
    "                                                                    self.num_workers,\n",
    "                                                                    self.drop_last)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=self.virtual_batch_size,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "\n",
    "        self.optimizer = self.optimizer_fn(self.network.parameters(),\n",
    "                                           **self.optimizer_params)\n",
    "\n",
    "        if self.scheduler_fn:\n",
    "            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "        self.losses_train = []\n",
    "        self.losses_valid = []\n",
    "        self.learning_rates = []\n",
    "        self.metrics_train = []\n",
    "        self.metrics_valid = []\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(\"Will train until validation stopping metric\",\n",
    "                  f\"hasn't improved in {self.patience} rounds.\")\n",
    "            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n",
    "            print('---------------------------------------')\n",
    "            print(msg_epoch)\n",
    "\n",
    "        total_time = 0\n",
    "        while (self.epoch < self.max_epochs and\n",
    "               self.patience_counter < self.patience):\n",
    "            starting_time = time.time()\n",
    "            # updates learning rate history\n",
    "            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n",
    "\n",
    "            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n",
    "\n",
    "            # leaving it here, may be used for callbacks later\n",
    "            self.losses_train.append(fit_metrics['train']['loss_avg'])\n",
    "            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n",
    "            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n",
    "            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n",
    "\n",
    "            stopping_loss = fit_metrics['valid']['stopping_loss']\n",
    "            if stopping_loss < self.best_cost:\n",
    "                self.best_cost = stopping_loss\n",
    "                self.patience_counter = 0\n",
    "                # Saving model\n",
    "                self.best_network = deepcopy(self.network)\n",
    "                has_improved = True\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                has_improved=False\n",
    "            self.epoch += 1\n",
    "            total_time += time.time() - starting_time\n",
    "            if self.verbose > 0:\n",
    "                if self.epoch % self.verbose == 0:\n",
    "                    separator = \"|\"\n",
    "                    msg_epoch = f\"| {self.epoch:<5} | \"\n",
    "                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n",
    "                    msg_epoch += f\" {has_improved}\"\n",
    "                    print(msg_epoch)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            if self.patience_counter == self.patience:\n",
    "                print(f\"Early stopping occured at epoch {self.epoch}\")\n",
    "            print(f\"Training done in {total_time:.3f} seconds.\")\n",
    "            print('---------------------------------------')\n",
    "\n",
    "        self.history = {\"train\": {\"loss\": self.losses_train,\n",
    "                                  \"metric\": self.metrics_train,\n",
    "                                  \"lr\": self.learning_rates},\n",
    "                        \"valid\": {\"loss\": self.losses_valid,\n",
    "                                  \"metric\": self.metrics_valid}}\n",
    "        # load best models post training\n",
    "        self.load_best_model()\n",
    "\n",
    "        # compute feature importance once the best model is defined\n",
    "        self._compute_feature_importances(train_dataloader)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saving model with two distinct files.\n",
    "        \"\"\"\n",
    "        saved_params = {}\n",
    "        for key, val in self.get_params().items():\n",
    "            if isinstance(val, type):\n",
    "                # Don't save torch specific params\n",
    "                continue\n",
    "            else:\n",
    "                saved_params[key] = val\n",
    "\n",
    "        # Create folder\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save models params\n",
    "        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(saved_params, f)\n",
    "\n",
    "        # Save state_dict\n",
    "        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n",
    "        shutil.make_archive(path, 'zip', path)\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Successfully saved model at {path}.zip\")\n",
    "        return f\"{path}.zip\"\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                with zipfile.ZipFile(filepath) as z:\n",
    "                    with z.open(\"model_params.json\") as f:\n",
    "                        loaded_params = json.load(f)\n",
    "                    with z.open(\"network.pt\") as f:\n",
    "                        try:\n",
    "                            saved_state_dict = torch.load(f)\n",
    "                        except io.UnsupportedOperation:\n",
    "                            # In Python <3.7, the returned file object is not seekable (which at least\n",
    "                            # some versions of PyTorch require) - so we'll try buffering it in to a\n",
    "                            # BytesIO instead:\n",
    "                            saved_state_dict = torch.load(io.BytesIO(f.read()))\n",
    "                            \n",
    "            except:\n",
    "                with open(os.path.join(filepath, \"model_params.json\")) as f:\n",
    "                        loaded_params = json.load(f)\n",
    "\n",
    "                saved_state_dict = torch.load(os.path.join(filepath, \"network.pt\"), map_location=\"cpu\")\n",
    " \n",
    "        except KeyError:\n",
    "            raise KeyError(\"Your zip file is missing at least one component\")\n",
    "\n",
    "        #print(loaded_params)\n",
    "        if torch.cuda.is_available():\n",
    "            device_name = 'cuda'\n",
    "        else:\n",
    "            device_name = 'cpu'\n",
    "        loaded_params[\"device_name\"] = device_name\n",
    "        self.__init__(**loaded_params)\n",
    "        \n",
    "        \n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=1024,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "        self.network.load_state_dict(saved_state_dict)\n",
    "        self.network.eval()\n",
    "        return\n",
    "\n",
    "    def fit_epoch(self, train_dataloader, valid_dataloader):\n",
    "        \"\"\"\n",
    "        Evaluates and updates network for one epoch.\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with valid set\n",
    "        \"\"\"\n",
    "        train_metrics = self.train_epoch(train_dataloader)\n",
    "        valid_metrics = self.predict_epoch(valid_dataloader)\n",
    "\n",
    "        fit_metrics = {'train': train_metrics,\n",
    "                       'valid': valid_metrics}\n",
    "\n",
    "        return fit_metrics\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_batch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_batch to use this base class')\n",
    "\n",
    "    def load_best_model(self):\n",
    "        if self.best_network is not None:\n",
    "            self.network = self.best_network\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem or the last class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict to use this base class')\n",
    "\n",
    "    def explain(self, X):\n",
    "        \"\"\"\n",
    "        Return local explanation\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            M_explain: matrix\n",
    "                Importance per sample, per columns.\n",
    "            masks: matrix\n",
    "                Sparse matrix showing attention masks used by network.\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            for key, value in masks.items():\n",
    "                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n",
    "                                            self.reducing_matrix)\n",
    "\n",
    "            if batch_nb == 0:\n",
    "                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                             self.reducing_matrix)\n",
    "                res_masks = masks\n",
    "            else:\n",
    "                res_explain = np.vstack([res_explain,\n",
    "                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                                        self.reducing_matrix)])\n",
    "                for key, value in masks.items():\n",
    "                    res_masks[key] = np.vstack([res_masks[key], value])\n",
    "        return res_explain, res_masks\n",
    "\n",
    "    def _compute_feature_importances(self, loader):\n",
    "        self.network.eval()\n",
    "        feature_importances_ = np.zeros((self.network.post_embed_dim))\n",
    "        for data, targets in loader:\n",
    "            data = data.to(self.device).float()\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n",
    "\n",
    "        feature_importances_ = csc_matrix.dot(feature_importances_,\n",
    "                                              self.reducing_matrix)\n",
    "        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n",
    "        \n",
    "        \n",
    "class TabNetRegressor(TabModel):\n",
    "\n",
    "    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n",
    "                          batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if isinstance(weights, int):\n",
    "            if weights == 1:\n",
    "                raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "        if isinstance(weights, dict):\n",
    "            raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "\n",
    "        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n",
    "                                                                y_train,\n",
    "                                                                X_valid,\n",
    "                                                                y_valid,\n",
    "                                                                weights,\n",
    "                                                                batch_size,\n",
    "                                                                num_workers,\n",
    "                                                                drop_last)\n",
    "        return train_dataloader, valid_dataloader\n",
    "\n",
    "    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n",
    "                          weights, max_epochs, patience,\n",
    "                          batch_size, virtual_batch_size, num_workers, drop_last):\n",
    "\n",
    "        if loss_fn is None:\n",
    "            self.loss_fn = torch.nn.functional.mse_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n",
    "        self.input_dim = X_train.shape[1]\n",
    "\n",
    "        if len(y_train.shape) == 1:\n",
    "            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n",
    "                                if doing single regression.\"\"\")\n",
    "        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n",
    "        self.output_dim = y_train.shape[1]\n",
    "\n",
    "        self.updated_weights = weights\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        # Initialize counters and histories.\n",
    "        self.patience_counter = 0\n",
    "        self.epoch = 0\n",
    "        self.best_cost = np.inf\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.train()\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in train_loader:\n",
    "            batch_outs = self.train_batch(data, targets)\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n",
    "        total_loss = total_loss / len(train_loader)\n",
    "        epoch_metrics = {'loss_avg': total_loss,\n",
    "                         'stopping_loss': total_loss,\n",
    "                         }\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "        return epoch_metrics\n",
    "\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        self.network.train()\n",
    "        data = data.to(self.device).float()\n",
    "\n",
    "        targets = targets.to(self.device).float()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "\n",
    "        loss = self.loss_fn(output, targets)\n",
    "        \n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "\n",
    "        loss.backward()\n",
    "        if self.clip_value:\n",
    "            clip_grad_norm_(self.network.parameters(), self.clip_value)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        y_preds = []\n",
    "        ys = []\n",
    "        self.network.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        for data, targets in loader:\n",
    "            batch_outs = self.predict_batch(data, targets)\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n",
    "            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "\n",
    "        y_preds = np.vstack(y_preds)\n",
    "        ys = np.vstack(ys)\n",
    "\n",
    "        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "\n",
    "        total_loss = total_loss / len(loader)\n",
    "        epoch_metrics = {'total_loss': total_loss,\n",
    "                         'stopping_loss': stopping_loss}\n",
    "\n",
    "        return epoch_metrics\n",
    "\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        data = data.to(self.device).float()\n",
    "        targets = targets.to(self.device).float()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "       \n",
    "        loss = self.loss_fn(output, targets)\n",
    "        #print(self.loss_fn, loss)\n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "        #print(loss)\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        results = []\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            output, M_loss = self.network(data)\n",
    "            predictions = output.cpu().detach().numpy()\n",
    "            results.append(predictions)\n",
    "        res = np.vstack(results)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.num_class = targets_tr.shape[1]\n",
    "        self.verbose=False\n",
    "        #\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.SPLITS = 10\n",
    "        self.EPOCHS = 300\n",
    "        self.num_ensembling = 1\n",
    "        self.seed = 0\n",
    "        # Parameters model\n",
    "        self.cat_emb_dim=[1] * cat_tr.shape[1] #to choose\n",
    "        self.cats_idx = list(range(cat_tr.shape[1]))\n",
    "        self.cat_dims = [len(np.unique(cat_tr[:, i])) for i in range(cat_tr.shape[1])]\n",
    "        self.num_numericals= numerical_tr.shape[1]\n",
    "        # save\n",
    "        self.save_name = \"../data/tabnet-weights-public/tabnet-raw-public-step1/tabnet_raw_step1\"\n",
    "        \n",
    "        self.strategy = \"KFOLD\" # \n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate([cat_test, numerical_test ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "00 |   392.2      False\nEarly stopping occured at epoch 205\nTraining done in 392.224 seconds.\n---------------------------------------\nSuccessfully saved model at ../data/tabnet-weights-public/tabnet-raw-public-step1/tabnet_raw_step1_fold7_0.zip\nvalidation fold 7 : 0.016654937377185968\nFOLDS :  8\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  0.39232 |   0.05622 |   2.2        True\n| 2     |  0.02991 |   0.02736 |   4.7        True\n| 3     |  0.02325 |   0.02164 |   6.5        True\n| 4     |  0.02160 |   0.02103 |   8.2        True\n| 5     |  0.02108 |   0.02073 |   10.3       True\n| 6     |  0.02082 |   0.02055 |   12.6       True\n| 7     |  0.02070 |   0.02035 |   15.2       True\n| 8     |  0.02043 |   0.02024 |   17.3       True\n| 9     |  0.02022 |   0.02011 |   19.1       True\n| 10    |  0.02004 |   0.02134 |   21.0       False\n| 11    |  0.01970 |   0.01966 |   23.1       True\n| 12    |  0.01935 |   0.02035 |   24.9       False\n| 13    |  0.01899 |   0.01911 |   26.7       True\n| 14    |  0.01884 |   0.01894 |   28.5       True\n| 15    |  0.01853 |   0.01890 |   30.9       True\n| 16    |  0.01831 |   0.01860 |   33.2       True\n| 17    |  0.01811 |   0.01835 |   35.7       True\n| 18    |  0.01795 |   0.01827 |   37.8       True\n| 19    |  0.01788 |   0.01818 |   39.8       True\n| 20    |  0.01776 |   0.02055 |   41.8       False\n| 21    |  0.01748 |   0.01827 |   44.0       False\n| 22    |  0.01731 |   0.01787 |   46.3       True\n| 23    |  0.01722 |   0.01839 |   48.2       False\n| 24    |  0.01725 |   0.01800 |   50.0       False\n| 25    |  0.01716 |   0.01791 |   52.0       False\n| 26    |  0.01701 |   0.01762 |   54.0       True\n| 27    |  0.01685 |   0.01931 |   56.2       False\n| 28    |  0.01674 |   0.01774 |   58.1       False\n| 29    |  0.01676 |   0.01829 |   60.1       False\n| 30    |  0.01661 |   0.01770 |   62.1       False\n| 31    |  0.01663 |   0.01770 |   64.0       False\n| 32    |  0.01657 |   0.01761 |   65.9       True\n| 33    |  0.01651 |   0.01957 |   67.8       False\n| 34    |  0.01649 |   0.01747 |   69.7       True\n| 35    |  0.01640 |   0.01763 |   71.6       False\n| 36    |  0.01629 |   0.01741 |   73.4       True\n| 37    |  0.01633 |   0.01737 |   75.3       True\n| 38    |  0.01634 |   0.01902 |   77.2       False\n| 39    |  0.01624 |   0.01824 |   79.1       False\n| 40    |  0.01637 |   0.01763 |   80.9       False\n| 41    |  0.01627 |   0.01741 |   82.7       False\n| 42    |  0.01618 |   0.01727 |   84.5       True\n| 43    |  0.01626 |   0.01753 |   86.4       False\n| 44    |  0.01609 |   0.01739 |   88.3       False\n| 45    |  0.01611 |   0.01743 |   90.2       False\n| 46    |  0.01608 |   0.01780 |   92.1       False\n| 47    |  0.01617 |   0.01709 |   93.9       True\n| 48    |  0.01599 |   0.01726 |   95.9       False\n| 49    |  0.01598 |   0.01730 |   97.8       False\n| 50    |  0.01607 |   0.01727 |   99.7       False\n| 51    |  0.01610 |   0.01727 |   101.5      False\n| 52    |  0.01611 |   0.01739 |   103.4      False\n| 53    |  0.01608 |   0.01719 |   105.2      False\n| 54    |  0.01600 |   0.01707 |   107.1      True\n| 55    |  0.01601 |   0.01723 |   109.0      False\n| 56    |  0.01586 |   0.01697 |   110.8      True\n| 57    |  0.01592 |   0.01728 |   112.7      False\n| 58    |  0.01595 |   0.01698 |   114.5      False\n| 59    |  0.01589 |   0.01719 |   116.4      False\n| 60    |  0.01589 |   0.01696 |   118.3      True\n| 61    |  0.01582 |   0.01714 |   120.2      False\n| 62    |  0.01591 |   0.01715 |   122.0      False\n| 63    |  0.01604 |   0.01741 |   123.8      False\n| 64    |  0.01589 |   0.01684 |   125.7      True\n| 65    |  0.01581 |   0.01704 |   127.5      False\n| 66    |  0.01574 |   0.01721 |   129.4      False\n| 67    |  0.01585 |   0.01744 |   131.3      False\n| 68    |  0.01570 |   0.01689 |   133.2      False\n| 69    |  0.01575 |   0.01687 |   135.0      False\n| 70    |  0.01577 |   0.01690 |   136.9      False\n| 71    |  0.01592 |   0.01699 |   139.3      False\n| 72    |  0.01591 |   0.01716 |   141.2      False\n| 73    |  0.01585 |   0.01733 |   143.1      False\n| 74    |  0.01585 |   0.01711 |   145.0      False\n| 75    |  0.01579 |   0.01688 |   146.8      False\n| 76    |  0.01575 |   0.01704 |   148.7      False\n| 77    |  0.01585 |   0.01713 |   150.6      False\n| 78    |  0.01584 |   0.01726 |   152.4      False\n| 79    |  0.01581 |   0.01692 |   154.3      False\n| 80    |  0.01576 |   0.01752 |   156.2      False\n| 81    |  0.01594 |   0.01769 |   158.0      False\n| 82    |  0.01586 |   0.01698 |   159.9      False\n| 83    |  0.01581 |   0.01732 |   161.8      False\n| 84    |  0.01571 |   0.01700 |   163.7      False\n| 85    |  0.01574 |   0.01704 |   165.5      False\n| 86    |  0.01575 |   0.01717 |   167.4      False\n| 87    |  0.01577 |   0.01696 |   169.3      False\n| 88    |  0.01578 |   0.01702 |   171.2      False\n| 89    |  0.01560 |   0.01686 |   173.0      False\n| 90    |  0.01557 |   0.01703 |   174.9      False\n| 91    |  0.01597 |   0.01725 |   176.7      False\n| 92    |  0.01595 |   0.01701 |   178.6      False\n| 93    |  0.01572 |   0.01712 |   180.5      False\n| 94    |  0.01584 |   0.01695 |   182.4      False\n| 95    |  0.01574 |   0.01719 |   184.2      False\n| 96    |  0.01568 |   0.01697 |   186.1      False\n| 97    |  0.01579 |   0.01739 |   188.0      False\n| 98    |  0.01580 |   0.01694 |   189.9      False\n| 99    |  0.01576 |   0.01697 |   191.7      False\n| 100   |  0.01565 |   0.01787 |   193.8      False\n| 101   |  0.01566 |   0.01697 |   196.0      False\n| 102   |  0.01563 |   0.01700 |   198.0      False\n| 103   |  0.01567 |   0.01689 |   199.9      False\n| 104   |  0.01553 |   0.01693 |   201.9      False\n| 105   |  0.01555 |   0.01702 |   203.7      False\n| 106   |  0.01568 |   0.01680 |   206.6      True\n| 107   |  0.01549 |   0.01705 |   209.0      False\n| 108   |  0.01544 |   0.01715 |   211.6      False\n| 109   |  0.01552 |   0.01702 |   213.8      False\n| 110   |  0.01572 |   0.01697 |   215.8      False\n| 111   |  0.01560 |   0.01723 |   217.8      False\n| 112   |  0.01573 |   0.01711 |   220.0      False\n| 113   |  0.01570 |   0.01692 |   222.0      False\n| 114   |  0.01565 |   0.01687 |   224.0      False\n| 115   |  0.01558 |   0.01690 |   226.0      False\n| 116   |  0.01558 |   0.01686 |   228.0      False\n| 117   |  0.01566 |   0.01733 |   230.0      False\n| 118   |  0.01570 |   0.01677 |   232.1      True\n| 119   |  0.01548 |   0.01695 |   234.1      False\n| 120   |  0.01559 |   0.01716 |   236.1      False\n| 121   |  0.01548 |   0.01743 |   238.1      False\n| 122   |  0.01553 |   0.01718 |   240.1      False\n| 123   |  0.01563 |   0.01712 |   242.1      False\n| 124   |  0.01551 |   0.01679 |   244.1      False\n| 125   |  0.01554 |   0.01685 |   246.1      False\n| 126   |  0.01559 |   0.01671 |   248.4      True\n| 127   |  0.01557 |   0.01684 |   251.1      False\n| 128   |  0.01548 |   0.01692 |   253.2      False\n| 129   |  0.01535 |   0.01702 |   255.3      False\n| 130   |  0.01547 |   0.01707 |   257.3      False\n| 131   |  0.01546 |   0.01718 |   259.4      False\n| 132   |  0.01561 |   0.01880 |   261.4      False\n| 133   |  0.01599 |   0.01698 |   263.4      False\n| 134   |  0.01569 |   0.01690 |   265.7      False\n| 135   |  0.01558 |   0.01704 |   267.8      False\n| 136   |  0.01557 |   0.01709 |   269.7      False\n| 137   |  0.01566 |   0.01710 |   272.0      False\n| 138   |  0.01559 |   0.01683 |   275.1      False\n| 139   |  0.01547 |   0.01681 |   278.1      False\n| 140   |  0.01557 |   0.01689 |   281.0      False\n| 141   |  0.01559 |   0.01704 |   283.2      False\n| 142   |  0.01560 |   0.01688 |   285.4      False\n| 143   |  0.01559 |   0.01685 |   287.5      False\n| 144   |  0.01552 |   0.01695 |   289.6      False\n| 145   |  0.01548 |   0.01759 |   291.7      False\n| 146   |  0.01552 |   0.01706 |   294.1      False\n| 147   |  0.01550 |   0.01693 |   296.4      False\n| 148   |  0.01554 |   0.01705 |   299.1      False\n| 149   |  0.01550 |   0.01705 |   302.2      False\n| 150   |  0.01541 |   0.01684 |   304.0      False\n| 151   |  0.01535 |   0.01706 |   305.9      False\n| 152   |  0.01538 |   0.01687 |   307.6      False\n| 153   |  0.01525 |   0.01714 |   309.5      False\n| 154   |  0.01530 |   0.01697 |   311.8      False\n| 155   |  0.01528 |   0.01675 |   315.0      False\n| 156   |  0.01530 |   0.01715 |   317.0      False\n| 157   |  0.01540 |   0.01688 |   318.9      False\n| 158   |  0.01534 |   0.01692 |   320.7      False\n| 159   |  0.01534 |   0.01733 |   322.6      False\n| 160   |  0.01536 |   0.01672 |   324.4      False\n| 161   |  0.01525 |   0.01690 |   326.3      False\n| 162   |  0.01524 |   0.01702 |   328.1      False\n| 163   |  0.01533 |   0.01692 |   329.9      False\n| 164   |  0.01522 |   0.01693 |   331.7      False\n| 165   |  0.01537 |   0.01695 |   333.5      False\n| 166   |  0.01533 |   0.01689 |   335.2      False\n| 167   |  0.01525 |   0.01679 |   337.1      False\n| 168   |  0.01528 |   0.01700 |   338.9      False\n| 169   |  0.01537 |   0.01711 |   340.7      False\n| 170   |  0.01537 |   0.01696 |   342.5      False\n| 171   |  0.01528 |   0.01677 |   344.3      False\n| 172   |  0.01538 |   0.01690 |   346.3      False\n| 173   |  0.01532 |   0.01704 |   348.3      False\n| 174   |  0.01529 |   0.01686 |   350.1      False\n| 175   |  0.01533 |   0.01700 |   351.9      False\n| 176   |  0.01534 |   0.01707 |   353.6      False\nEarly stopping occured at epoch 176\nTraining done in 353.643 seconds.\n---------------------------------------\nSuccessfully saved model at ../data/tabnet-weights-public/tabnet-raw-public-step1/tabnet_raw_step1_fold8_0.zip\nvalidation fold 8 : 0.016710873267407646\nFOLDS :  9\nDevice used : cuda\nWill train until validation stopping metric hasn't improved in 50 rounds.\n---------------------------------------\n| EPOCH |  train  |   valid  | total time (s)\n| 1     |  0.39166 |   0.05223 |   1.8        True\n| 2     |  0.03008 |   0.02719 |   3.6        True\n| 3     |  0.02340 |   0.02134 |   5.4        True\n| 4     |  0.02137 |   0.02092 |   7.2        True\n| 5     |  0.02098 |   0.02067 |   8.9        True\n| 6     |  0.02076 |   0.02048 |   10.7       True\n| 7     |  0.02053 |   0.02033 |   12.5       True\n| 8     |  0.02038 |   0.02016 |   14.3       True\n| 9     |  0.02021 |   0.02006 |   16.1       True\n| 10    |  0.01998 |   0.01978 |   17.9       True\n| 11    |  0.01974 |   0.02104 |   19.7       False\n| 12    |  0.01946 |   0.02109 |   21.5       False\n| 13    |  0.01911 |   0.01922 |   23.3       True\n| 14    |  0.01883 |   0.01875 |   25.3       True\n| 15    |  0.01848 |   0.01994 |   27.5       False\n| 16    |  0.01831 |   0.01851 |   29.4       True\n| 17    |  0.01807 |   0.01850 |   31.1       True\n| 18    |  0.01791 |   0.01830 |   32.9       True\n| 19    |  0.01777 |   0.01938 |   34.7       False\n| 20    |  0.01756 |   0.01798 |   36.5       True\n| 21    |  0.01742 |   0.01821 |   38.3       False\n| 22    |  0.01737 |   0.01992 |   40.1       False\n| 23    |  0.01727 |   0.01946 |   41.9       False\n| 24    |  0.01722 |   0.01994 |   43.6       False\n| 25    |  0.01723 |   0.01925 |   45.4       False\n| 26    |  0.01717 |   0.01792 |   47.2       True\n| 27    |  0.01705 |   0.01769 |   49.0       True\n| 28    |  0.01691 |   0.01754 |   51.1       True\n| 29    |  0.01692 |   0.01748 |   52.9       True\n| 30    |  0.01686 |   0.01764 |   55.0       False\n| 31    |  0.01680 |   0.01768 |   57.4       False\n| 32    |  0.01663 |   0.01758 |   59.6       False\n| 33    |  0.01675 |   0.01740 |   61.5       True\n| 34    |  0.01668 |   0.01973 |   63.2       False\n| 35    |  0.01665 |   0.01745 |   65.3       False\n| 36    |  0.01667 |   0.01760 |   68.1       False\n| 37    |  0.01652 |   0.01754 |   70.1       False\n| 38    |  0.01654 |   0.01737 |   72.0       True\n| 39    |  0.01658 |   0.01953 |   73.9       False\n| 40    |  0.01658 |   0.01771 |   75.7       False\n| 41    |  0.01638 |   0.01725 |   77.5       True\n| 42    |  0.01637 |   0.01958 |   79.2       False\n| 43    |  0.01641 |   0.01832 |   81.0       False\n| 44    |  0.01628 |   0.01722 |   82.9       True\n| 45    |  0.01623 |   0.01716 |   84.7       True\n| 46    |  0.01625 |   0.01728 |   86.4       False\n| 47    |  0.01623 |   0.01711 |   88.2       True\n| 48    |  0.01625 |   0.01842 |   90.0       False\n| 49    |  0.01625 |   0.01924 |   91.7       False\n| 50    |  0.01622 |   0.01702 |   93.5       True\n| 51    |  0.01615 |   0.01836 |   95.3       False\n| 52    |  0.01616 |   0.01918 |   97.2       False\n| 53    |  0.01614 |   0.01784 |   99.0       False\n| 54    |  0.01614 |   0.01684 |   100.8      True\n| 55    |  0.01609 |   0.01699 |   102.6      False\n| 56    |  0.01605 |   0.01714 |   104.4      False\n| 57    |  0.01610 |   0.01712 |   106.2      False\n| 58    |  0.01604 |   0.01757 |   108.0      False\n| 59    |  0.01600 |   0.01691 |   109.8      False\n| 60    |  0.01592 |   0.01703 |   111.5      False\n| 61    |  0.01593 |   0.01750 |   113.3      False\n| 62    |  0.01592 |   0.01698 |   115.1      False\n| 63    |  0.01605 |   0.01702 |   116.8      False\n| 64    |  0.01600 |   0.01682 |   118.6      True\n| 65    |  0.01594 |   0.01772 |   120.4      False\n| 66    |  0.01610 |   0.01733 |   122.1      False\n| 67    |  0.01590 |   0.01684 |   123.9      False\n| 68    |  0.01577 |   0.01709 |   125.7      False\n| 69    |  0.01581 |   0.01685 |   127.6      False\n| 70    |  0.01583 |   0.01760 |   129.4      False\n| 71    |  0.01618 |   0.01704 |   131.1      False\n| 72    |  0.01592 |   0.01678 |   132.9      True\n| 73    |  0.01586 |   0.01684 |   134.6      False\n| 74    |  0.01593 |   0.01727 |   136.4      False\n| 75    |  0.01603 |   0.01697 |   138.2      False\n| 76    |  0.01597 |   0.01684 |   140.0      False\n| 77    |  0.01582 |   0.01689 |   141.8      False\n| 78    |  0.01593 |   0.01710 |   143.6      False\n| 79    |  0.01589 |   0.01680 |   145.3      False\n| 80    |  0.01590 |   0.01708 |   147.1      False\n| 81    |  0.01577 |   0.01679 |   148.9      False\n| 82    |  0.01577 |   0.01677 |   150.6      True\n| 83    |  0.01576 |   0.01729 |   152.4      False\n| 84    |  0.01598 |   0.01721 |   154.1      False\n| 85    |  0.01597 |   0.01688 |   155.9      False\n| 86    |  0.01571 |   0.01760 |   157.7      False\n| 87    |  0.01566 |   0.01780 |   159.4      False\n| 88    |  0.01587 |   0.01707 |   161.2      False\n| 89    |  0.01587 |   0.01869 |   163.0      False\n| 90    |  0.01589 |   0.01714 |   164.8      False\n| 91    |  0.01583 |   0.01695 |   166.5      False\n| 92    |  0.01576 |   0.01676 |   168.4      True\n| 93    |  0.01567 |   0.01690 |   170.2      False\n| 94    |  0.01563 |   0.01688 |   172.0      False\n| 95    |  0.01580 |   0.01719 |   173.7      False\n| 96    |  0.01586 |   0.01721 |   175.4      False\n| 97    |  0.01576 |   0.01685 |   177.2      False\n| 98    |  0.01573 |   0.01689 |   179.0      False\n| 99    |  0.01585 |   0.01713 |   180.8      False\n| 100   |  0.01581 |   0.01715 |   182.5      False\n| 101   |  0.01576 |   0.01684 |   184.3      False\n| 102   |  0.01571 |   0.01694 |   186.1      False\n| 103   |  0.01563 |   0.01682 |   187.9      False\n| 104   |  0.01558 |   0.01729 |   189.8      False\n| 105   |  0.01555 |   0.01760 |   191.6      False\n| 106   |  0.01558 |   0.01818 |   193.4      False\n| 107   |  0.01562 |   0.01724 |   195.2      False\n| 108   |  0.01577 |   0.01710 |   196.9      False\n| 109   |  0.01564 |   0.01775 |   198.6      False\n| 110   |  0.01550 |   0.01686 |   200.4      False\n| 111   |  0.01559 |   0.01702 |   202.2      False\n| 112   |  0.01557 |   0.01682 |   204.0      False\n| 113   |  0.01549 |   0.01697 |   205.8      False\n| 114   |  0.01551 |   0.01717 |   207.5      False\n| 115   |  0.01569 |   0.01696 |   209.2      False\n| 116   |  0.01558 |   0.01672 |   211.1      True\n| 117   |  0.01557 |   0.01685 |   212.9      False\n| 118   |  0.01563 |   0.01689 |   214.7      False\n| 119   |  0.01557 |   0.01702 |   216.4      False\n| 120   |  0.01550 |   0.01686 |   218.1      False\n| 121   |  0.01557 |   0.01711 |   219.9      False\n| 122   |  0.01573 |   0.01709 |   221.6      False\n| 123   |  0.01567 |   0.01675 |   223.5      False\n| 124   |  0.01554 |   0.01686 |   225.3      False\n| 125   |  0.01551 |   0.01679 |   227.1      False\n| 126   |  0.01566 |   0.01687 |   228.8      False\n| 127   |  0.01568 |   0.01685 |   230.6      False\n| 128   |  0.01562 |   0.01686 |   232.4      False\n| 129   |  0.01554 |   0.01688 |   234.2      False\n| 130   |  0.01553 |   0.01687 |   236.0      False\n| 131   |  0.01549 |   0.01923 |   237.9      False\n| 132   |  0.01616 |   0.01729 |   239.7      False\n| 133   |  0.01581 |   0.01692 |   241.5      False\n| 134   |  0.01565 |   0.01697 |   243.3      False\n| 135   |  0.01568 |   0.01698 |   245.1      False\n| 136   |  0.01558 |   0.01687 |   246.8      False\n| 137   |  0.01552 |   0.01664 |   248.7      True\n| 138   |  0.01555 |   0.01664 |   250.4      False\n| 139   |  0.01549 |   0.01691 |   252.2      False\n| 140   |  0.01555 |   0.01688 |   254.0      False\n| 141   |  0.01568 |   0.01678 |   255.8      False\n| 142   |  0.01551 |   0.01682 |   257.5      False\n| 143   |  0.01562 |   0.01696 |   259.2      False\n| 144   |  0.01563 |   0.01680 |   261.0      False\n| 145   |  0.01559 |   0.01686 |   262.7      False\n| 146   |  0.01565 |   0.01697 |   264.5      False\n| 147   |  0.01559 |   0.01687 |   266.3      False\n| 148   |  0.01548 |   0.01687 |   268.0      False\n| 149   |  0.01552 |   0.01695 |   269.7      False\n| 150   |  0.01560 |   0.01723 |   271.5      False\n| 151   |  0.01556 |   0.01692 |   273.2      False\n| 152   |  0.01539 |   0.01678 |   275.0      False\n| 153   |  0.01538 |   0.01667 |   276.8      False\n| 154   |  0.01548 |   0.01689 |   278.5      False\n| 155   |  0.01540 |   0.01674 |   280.3      False\n| 156   |  0.01528 |   0.01724 |   282.1      False\n| 157   |  0.01551 |   0.01688 |   283.9      False\n| 158   |  0.01546 |   0.01699 |   285.7      False\n| 159   |  0.01536 |   0.01700 |   287.4      False\n| 160   |  0.01541 |   0.01684 |   289.2      False\n| 161   |  0.01533 |   0.01692 |   291.0      False\n| 162   |  0.01534 |   0.01685 |   292.8      False\n| 163   |  0.01540 |   0.01702 |   294.5      False\n| 164   |  0.01541 |   0.01700 |   296.3      False\n| 165   |  0.01546 |   0.01691 |   298.1      False\n| 166   |  0.01541 |   0.01684 |   299.8      False\n| 167   |  0.01527 |   0.01683 |   301.6      False\n| 168   |  0.01523 |   0.01688 |   303.4      False\n| 169   |  0.01537 |   0.01691 |   305.2      False\n| 170   |  0.01532 |   0.01684 |   307.0      False\n| 171   |  0.01531 |   0.01700 |   308.9      False\n| 172   |  0.01545 |   0.01717 |   310.6      False\n| 173   |  0.01548 |   0.01776 |   312.4      False\n| 174   |  0.01539 |   0.01684 |   314.2      False\n| 175   |  0.01527 |   0.01691 |   315.9      False\n| 176   |  0.01528 |   0.01718 |   317.6      False\n| 177   |  0.01538 |   0.01696 |   319.4      False\n| 178   |  0.01554 |   0.01696 |   321.1      False\n| 179   |  0.01561 |   0.01680 |   322.9      False\n| 180   |  0.01541 |   0.01690 |   324.6      False\n| 181   |  0.01536 |   0.01670 |   326.4      False\n| 182   |  0.01543 |   0.01687 |   328.1      False\n| 183   |  0.01545 |   0.01682 |   329.9      False\n| 184   |  0.01541 |   0.01675 |   331.7      False\n| 185   |  0.01535 |   0.01682 |   333.5      False\n| 186   |  0.01538 |   0.01690 |   335.2      False\n| 187   |  0.01530 |   0.01701 |   337.0      False\nEarly stopping occured at epoch 187\nTraining done in 337.003 seconds.\n---------------------------------------\nSuccessfully saved model at ../data/tabnet-weights-public/tabnet-raw-public-step1/tabnet_raw_step1_fold9_0.zip\nvalidation fold 9 : 0.016636595410550268\n"
    }
   ],
   "source": [
    "if cfg.strategy == \"KFOLD\":\n",
    "    oof_preds_all = []\n",
    "    oof_targets_all = []\n",
    "    scores_all =  []\n",
    "    scores_auc_all= []\n",
    "    for seed in range(cfg.num_ensembling):\n",
    "        print(\"## SEED : \", seed)\n",
    "        mskf = MultilabelStratifiedKFold(n_splits=cfg.SPLITS, random_state=cfg.seed+seed, shuffle=True)\n",
    "        oof_preds = []\n",
    "        oof_targets = []\n",
    "        scores = []\n",
    "        scores_auc = []\n",
    "        for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n",
    "            print(\"FOLDS : \", j)\n",
    "\n",
    "            ## model\n",
    "            X_train, y_train = torch.as_tensor(np.concatenate([cat_tr[train_idx], numerical_tr[train_idx] ], axis=1)), torch.as_tensor(targets_tr[train_idx])\n",
    "            X_val, y_val = torch.as_tensor(np.concatenate([cat_tr[val_idx], numerical_tr[val_idx] ], axis=1)), torch.as_tensor(targets_tr[val_idx])\n",
    "            model = TabNetRegressor(n_d=24, \n",
    "                                    n_a=24, \n",
    "                                    n_steps=1, \n",
    "                                    gamma=1.3, \n",
    "                                    lambda_sparse=0, \n",
    "                                    cat_dims=cfg.cat_dims, \n",
    "                                    cat_emb_dim=cfg.cat_emb_dim, \n",
    "                                    cat_idxs=cfg.cats_idx, \n",
    "                                    optimizer_fn=torch.optim.Adam,\n",
    "                                    optimizer_params=dict(lr=2e-2, weight_decay=1e-5), \n",
    "                                    mask_type='entmax', \n",
    "                                    device_name=cfg.device, \n",
    "                                    scheduler_params=dict(milestones=[ 100,150], gamma=0.9), \n",
    "                                    scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n",
    "            #'sparsemax'\n",
    "            \n",
    "            model.fit(X_train=X_train, y_train=y_train, X_valid=X_val, y_valid=y_val,max_epochs=cfg.EPOCHS, patience=50, batch_size=1024, virtual_batch_size=128,\n",
    "                    num_workers=0, drop_last=False, loss_fn=torch.nn.functional.binary_cross_entropy_with_logits)\n",
    "            model.load_best_model()\n",
    "            preds = model.predict(X_val)\n",
    "            preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n",
    "            score = log_loss_multi(y_val, preds)\n",
    "            name = cfg.save_name + f\"_fold{j}_{seed}\"\n",
    "            model.save_model(name)\n",
    "            ## save oof to compute the CV later\n",
    "            oof_preds.append(preds)\n",
    "            oof_targets.append(y_val)\n",
    "            scores.append(score)\n",
    "            scores_auc.append(auc_multi(y_val,preds))\n",
    "            print(f\"validation fold {j} : {score}\")\n",
    "        oof_preds_all.append(np.concatenate(oof_preds))\n",
    "        oof_targets_all.append(np.concatenate(oof_targets))\n",
    "        scores_all.append(np.array(scores))\n",
    "        scores_auc_all.append(np.array(scores_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CV score fold :  0.016605770493997713\nauc mean :  0.7277670023750323\n"
    }
   ],
   "source": [
    "if cfg.strategy == \"KFOLD\":\n",
    "\n",
    "    for i in range(cfg.num_ensembling): \n",
    "        print(\"CV score fold : \", log_loss_multi(oof_targets_all[i], oof_preds_all[i]))\n",
    "        print(\"auc mean : \", sum(scores_auc_all[i])/len(scores_auc_all[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[array([0.01662209, 0.01650294, 0.01655404, 0.01632828, 0.0165854 ,\n        0.01658792, 0.01641699, 0.01684334, 0.01672764, 0.01621404,\n        0.01643073, 0.01640905, 0.01652504, 0.01638695, 0.01685884,\n        0.01631663, 0.01627308, 0.01694572, 0.01660748, 0.01636035])]"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "#BASELINE\n",
    "#CV score fold :  0.016605770493997713\n",
    "#auc mean :  0.7277670023750323\n",
    "\n",
    "\n",
    "# CV score fold :  0.01664568302707602\n",
    "# auc mean :  0.7273466060209528\n",
    "\n",
    "# CV score fold :  0.016619097094731947\n",
    "# auc mean :  0.7273243153239334\n",
    "\n",
    "# CV score fold :  0.016632541307153234\n",
    "# auc mean :  0.7262181535818258\n",
    "\n",
    "scores_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[array([0.66427642, 0.70730322, 0.68564941, 0.68923364, 0.68644887,\n        0.69182598, 0.68833095, 0.69018028, 0.66921278, 0.66401097,\n        0.67932986, 0.70317714, 0.67107204, 0.68059878, 0.67500852,\n        0.68648776, 0.68608135, 0.66362488, 0.69904479, 0.67801958])]"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "scores_auc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "TabNetRegressor(cat_dims=[3, 2], cat_emb_dim=[1, 1], cat_idxs=[0, 1],\n                device_name='cuda', input_dim=874, lambda_sparse=0,\n                mask_type='entmax', n_a=24, n_d=24, n_steps=1,\n                optimizer_params={'lr': 0.02, 'weight_decay': 1e-05},\n                output_dim=206,\n                scheduler_fn=<class 'torch.optim.lr_scheduler.MultiStepLR'>,\n                scheduler_params={'gamma': 0.9, 'milestones': [100, 150]})\n"
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "## SEED :  0\nFOLDS :  0\nDevice used : cuda\n"
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/tabnet-weights-public/tabnet-raw-public-step1/tabnet_raw_step1_fold0_0/model_params.json'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9608fd852575>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_params.json\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/moa_kaggle/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/tabnet-weights-public/tabnet-raw-public-step1/tabnet_raw_step1_fold0_0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6b08947a329f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"_fold{j}_{seed}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;31m# preds on val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-9608fd852575>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_params.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m                         \u001b[0mloaded_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/tabnet-weights-public/tabnet-raw-public-step1/tabnet_raw_step1_fold0_0/model_params.json'"
     ]
    }
   ],
   "source": [
    "if cfg.strategy == \"KFOLD\":\n",
    "    oof_preds_all = []\n",
    "    oof_targets_all = []\n",
    "    scores_all =  []\n",
    "    scores_auc_all= []\n",
    "    preds_test = []\n",
    "    for seed in range(cfg.num_ensembling):\n",
    "        print(\"## SEED : \", seed)\n",
    "        mskf = MultilabelStratifiedKFold(n_splits=cfg.SPLITS, random_state=cfg.seed+seed, shuffle=True)\n",
    "        oof_preds = []\n",
    "        oof_targets = []\n",
    "        scores = []\n",
    "        scores_auc = []\n",
    "        p = []\n",
    "        for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n",
    "            print(\"FOLDS : \", j)\n",
    "\n",
    "            ## model\n",
    "            X_train, y_train = torch.as_tensor(np.concatenate([cat_tr[train_idx], numerical_tr[train_idx] ], axis=1)), torch.as_tensor(targets_tr[train_idx])\n",
    "            X_val, y_val = torch.as_tensor(np.concatenate([cat_tr[val_idx], numerical_tr[val_idx] ], axis=1)), torch.as_tensor(targets_tr[val_idx])\n",
    "            model = TabNetRegressor(n_d=24, \n",
    "                                    n_a=24, \n",
    "                                    n_steps=1, \n",
    "                                    gamma=1.3, \n",
    "                                    lambda_sparse=0, \n",
    "                                    cat_dims=cfg.cat_dims, \n",
    "                                    cat_emb_dim=cfg.cat_emb_dim, \n",
    "                                    cat_idxs=cfg.cats_idx, \n",
    "                                    optimizer_fn=torch.optim.Adam,\n",
    "                                    optimizer_params=dict(lr=2e-2), \n",
    "                                    mask_type='entmax', \n",
    "                                    device_name=cfg.device, \n",
    "                                    scheduler_params=dict(milestones=[ 50,100,150], gamma=0.9), \n",
    "                                    scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n",
    "            #'sparsemax'\n",
    "            \n",
    "            name = cfg.save_name + f\"_fold{j}_{seed}\"\n",
    "            model.load_model(name)\n",
    "            # preds on val\n",
    "            preds = model.predict(X_val)\n",
    "            preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n",
    "            score = log_loss_multi(y_val, preds)\n",
    "            \n",
    "            # preds on test\n",
    "            temp = model.predict(X_test)\n",
    "            p.append(torch.sigmoid(torch.as_tensor(temp)).detach().cpu().numpy())\n",
    "            ## save oof to compute the CV later\n",
    "            oof_preds.append(preds)\n",
    "            oof_targets.append(y_val)\n",
    "            scores.append(score)\n",
    "            scores_auc.append(auc_multi(y_val,preds))\n",
    "            print(f\"validation fold {j} : {score}\")\n",
    "        p = np.stack(p)\n",
    "        preds_test.append(p)\n",
    "        oof_preds_all.append(np.concatenate(oof_preds))\n",
    "        oof_targets_all.append(np.concatenate(oof_targets))\n",
    "        scores_all.append(np.array(scores))\n",
    "        scores_auc_all.append(np.array(scores_auc))\n",
    "    preds_test = np.stack(preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}